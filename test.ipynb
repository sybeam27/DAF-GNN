{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\fair-gnn\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "import torch_sparse\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.typing import Adj, OptTensor\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "\n",
    "# import warnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(y_true, y_pred, tau=0.9):\n",
    "    error = y_true - y_pred\n",
    "    return torch.mean(torch.max(tau * error, (tau - 1) * error))\n",
    "\n",
    "class MYPlainGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)  # 단일 회귀 출력\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        out = self.fc(x).squeeze(-1)  # [N] 형태\n",
    "        return out\n",
    "\n",
    "class MYFairGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, lambda_fair=1.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.lambda_fair = lambda_fair\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        preds = self.fc(x).squeeze(-1)  # [N]\n",
    "        return preds\n",
    "\n",
    "    def fairness_loss(self, preds, sensitive):\n",
    "        group0 = preds[sensitive == 0]\n",
    "        group1 = preds[sensitive == 1]\n",
    "        if len(group0) == 0 or len(group1) == 0:\n",
    "            return torch.tensor(0.0, device=preds.device)\n",
    "        return torch.abs(group0.mean() - group1.mean())\n",
    "\n",
    "    def total_loss(self, preds, y_true, sensitive, tau=0.9):\n",
    "        # 기본 quantile loss\n",
    "        error = y_true - preds\n",
    "        quantile_loss = torch.mean(torch.max(tau * error, (tau - 1) * error))\n",
    "\n",
    "        # 공정성 정규화\n",
    "        fair_loss = self.fairness_loss(preds, sensitive)\n",
    "\n",
    "        return quantile_loss + self.lambda_fair * fair_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FairGNN\n",
    "class GCN_Body(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, dropout):\n",
    "        super(GCN_Body, self).__init__()\n",
    "        self.gc1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.gc2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.gc1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.gc2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.body = GCN_Body(in_channels, hidden_channels, dropout)\n",
    "        self.fc = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.body(x, edge_index)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "class GAT_body(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 in_dim,\n",
    "                 num_hidden,\n",
    "                 heads,\n",
    "                 feat_drop,\n",
    "                 attn_drop,\n",
    "                 negative_slope,\n",
    "                 residual):\n",
    "        super(GAT_body, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        self.activation = F.elu\n",
    "        self.gat_layers.append(GATConv(\n",
    "            in_dim, num_hidden, heads[0],\n",
    "            feat_drop, attn_drop, negative_slope, False, self.activation))\n",
    "        \n",
    "        # hidden layers\n",
    "        for l in range(1, num_layers):\n",
    "            self.gat_layers.append(GATConv(\n",
    "                num_hidden * heads[l-1], num_hidden, heads[l],\n",
    "                feat_drop, attn_drop, negative_slope, residual, self.activation))\n",
    "            \n",
    "        # output projection\n",
    "        self.gat_layers.append(GATConv(\n",
    "            num_hidden * heads[-2], num_hidden, heads[-1],\n",
    "            feat_drop, attn_drop, negative_slope, residual, None))\n",
    "        \n",
    "    def forward(self, g, inputs):\n",
    "        h = inputs\n",
    "        for l in range(self.num_layers):\n",
    "            h = self.gat_layers[l](g, h).flatten(1)\n",
    "        \n",
    "        # output projection\n",
    "        logits = self.gat_layers[-1](g, h).mean(1)\n",
    "\n",
    "        return logits\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 in_dim,\n",
    "                 num_hidden,\n",
    "                 num_classes,\n",
    "                 heads,\n",
    "                 feat_drop,\n",
    "                 attn_drop,\n",
    "                 negative_slope,\n",
    "                 residual):\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        self.body = GAT_body(num_layers, in_dim, num_hidden, heads, feat_drop, attn_drop, negative_slope, residual)\n",
    "        self.fc = nn.Linear(num_hidden,num_classes)\n",
    "    \n",
    "    def forward(self, g, inputs):\n",
    "\n",
    "        logits = self.body(g,inputs)\n",
    "        logits = self.fc(logits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "def get_model(model, nfeat, num_hidden=64, dropout=0.5, num_heads=1, num_layers=1, num_out_heads=1, attn_drop=0.0, negative_slope=0.2, residual=False):\n",
    "    if model == \"GCN\":\n",
    "        model = GCN_Body(nfeat, num_hidden, dropout)\n",
    "    elif model == \"GAT\":\n",
    "        heads = ([num_heads] * num_layers) + [num_out_heads]\n",
    "        model = GAT_body(num_layers, nfeat, num_hidden, heads,\n",
    "                         dropout, attn_drop, negative_slope, residual)\n",
    "    else:\n",
    "        raise ValueError(\"Model not implemented\")\n",
    "    return model\n",
    "\n",
    "class FairGNN_Q_bf(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=64, model='GCN', dropout=0.5, hidden = 128, lr=0.001, weight=1e-5, tau=0.9):\n",
    "        super(FairGNN_Q, self).__init__()\n",
    "        \n",
    "        nfeat = in_dim\n",
    "        nhid = hidden_dim\n",
    "        self.estimator = GCN(nfeat, hidden, 1, dropout)\n",
    "        self.GNN = get_model(model, nfeat)\n",
    "        self.classifier = nn.Linear(nhid, 1)\n",
    "        self.adv = nn.Linear(nhid, 1)\n",
    "\n",
    "        G_params = list(self.GNN.parameters()) + list(self.classifier.parameters()) + list(self.estimator.parameters())\n",
    "        self.optimizer_G = torch.optim.Adam(G_params, lr=lr, weight_decay=weight)\n",
    "        self.optimizer_A = torch.optim.Adam(self.adv.parameters(), lr=lr, weight_decay=weight)\n",
    "\n",
    "        self.tau = tau  # Quantile 설정\n",
    "        self.G_loss = 0\n",
    "        self.A_loss = 0\n",
    "\n",
    "        self.alpha = 4\n",
    "        self.beta = 0.01\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        s = self.estimator(g, x)\n",
    "        z = self.GNN(g, x)\n",
    "        y = self.classifier(z)\n",
    "        return y, s\n",
    "\n",
    "    def optimize(self, g, x, labels, idx_train, sens, idx_sens_train, tau):\n",
    "        self.train()\n",
    "\n",
    "        ### update GNN and Classifier\n",
    "        self.adv.requires_grad_(False)\n",
    "        self.optimizer_G.zero_grad()\n",
    "\n",
    "        s = self.estimator(g, x)\n",
    "        h = self.GNN(g, x)\n",
    "        y = self.classifier(h)\n",
    "\n",
    "        s_g = self.adv(h)\n",
    "\n",
    "        s_score = s.detach()\n",
    "        s_score[idx_sens_train] = sens[idx_sens_train].unsqueeze(1).float()\n",
    "\n",
    "        error = labels[idx_train].unsqueeze(1).float() - y[idx_train]\n",
    "        self.cls_loss = torch.mean(torch.max(tau * error, (tau - 1) * error))\n",
    "        \n",
    "        self.adv_loss = F.mse_loss(s_g, s_score)  # adversary는 MSE\n",
    "\n",
    "        # 공정성 페널티\n",
    "        cov = torch.abs(torch.mean((s_score - torch.mean(s_score)) * (y - torch.mean(y))))\n",
    "\n",
    "        self.G_loss = self.cls_loss + self.alpha * cov - self.beta * self.adv_loss\n",
    "\n",
    "        self.G_loss.backward()\n",
    "        self.optimizer_G.step()\n",
    "\n",
    "        ### update Adversary\n",
    "        self.adv.requires_grad_(True)\n",
    "        self.optimizer_A.zero_grad()\n",
    "        s_g = self.adv(h.detach())\n",
    "        self.A_loss = F.mse_loss(s_g, s_score)\n",
    "\n",
    "        self.A_loss.backward()\n",
    "        self.optimizer_A.step()\n",
    "\n",
    "class FairGNN_Q(nn.Module):\n",
    "    def __init__(self, in_dim, \n",
    "                 hidden_dim=64, dropout=0.5, hidden=128, lr=0.001, weight=1e-5, tau=0.9):\n",
    "        super(FairGNN_Q, self).__init__()\n",
    "        self.estimator = GCN(in_dim, hidden, 1, dropout)\n",
    "        self.GNN = GCN(in_dim, hidden_dim, hidden_dim, dropout)\n",
    "        self.classifier = nn.Linear(hidden_dim, 1)\n",
    "        self.adv = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        G_params = list(self.GNN.parameters()) + list(self.classifier.parameters()) + list(self.estimator.parameters())\n",
    "        self.optimizer_G = torch.optim.Adam(G_params, lr=lr, weight_decay=weight)\n",
    "        self.optimizer_A = torch.optim.Adam(self.adv.parameters(), lr=lr, weight_decay=weight)\n",
    "\n",
    "        self.tau = tau\n",
    "        self.G_loss = 0\n",
    "        self.A_loss = 0\n",
    "\n",
    "        self.alpha = 4\n",
    "        self.beta = 0.01\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        s = self.estimator(x, edge_index)\n",
    "        z = self.GNN(x, edge_index)\n",
    "        y = self.classifier(z)\n",
    "        return y, s\n",
    "\n",
    "    def optimize(self, data, tau):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        labels = data.y\n",
    "        idx_train = data.idx_train\n",
    "        sens = data.sensitive_attr\n",
    "        idx_sens_train = data.idx_sens_train\n",
    "\n",
    "        self.train()\n",
    "        self.adv.requires_grad_(False)\n",
    "        self.optimizer_G.zero_grad()\n",
    "\n",
    "        s = self.estimator(x, edge_index)\n",
    "        h = self.GNN(x, edge_index)\n",
    "        y = self.classifier(h)\n",
    "\n",
    "        s_g = self.adv(h)\n",
    "\n",
    "        s_score = s.detach()\n",
    "        s_score[idx_sens_train] = sens[idx_sens_train].unsqueeze(1).float()\n",
    "\n",
    "        error = labels[idx_train].unsqueeze(1).float() - y[idx_train]\n",
    "        self.cls_loss = torch.mean(torch.max(tau * error, (tau - 1) * error))\n",
    "\n",
    "        self.adv_loss = F.mse_loss(s_g, s_score)\n",
    "        cov = torch.abs(torch.mean((s_score - s_score.mean()) * (y - y.mean())))\n",
    "\n",
    "        self.G_loss = self.cls_loss + self.alpha * cov - self.beta * self.adv_loss\n",
    "        self.G_loss.backward()\n",
    "        self.optimizer_G.step()\n",
    "\n",
    "        self.adv.requires_grad_(True)\n",
    "        self.optimizer_A.zero_grad()\n",
    "        s_g = self.adv(h.detach())\n",
    "        self.A_loss = F.mse_loss(s_g, s_score)\n",
    "        self.A_loss.backward()\n",
    "        self.optimizer_A.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FMP\n",
    "def get_sen(sens, idx_sens_train):\n",
    "    sens_zeros = torch.zeros_like(sens)\n",
    "    sens_1 = sens \n",
    "    sens_0 = (1 - sens) \n",
    "    \n",
    "    sens_1[idx_sens_train] = sens_1[idx_sens_train] / len(sens_1[idx_sens_train])\n",
    "    sens_0[idx_sens_train] = sens_0[idx_sens_train] / len(sens_0[idx_sens_train])\n",
    "\n",
    "    sens_zeros[idx_sens_train] = sens_1[idx_sens_train] - sens_0[idx_sens_train]\n",
    "    sen_mat = torch.unsqueeze(sens_zeros, dim=0)\n",
    "    return sen_mat\n",
    "\n",
    "def check_sen(edge_index, sen):\n",
    "    nnz = edge_index.nnz()\n",
    "    deg = torch.eye(edge_index.sizes()[0]).cuda()\n",
    "    adj = edge_index.to_dense()\n",
    "    lap = (sen.t() @ sen).to_dense()\n",
    "    lap2 = deg - adj\n",
    "    diff = torch.sum(torch.abs(lap2-lap)) / nnz\n",
    "    assert diff < 0.000001, f'error: {diff} need to make sure L=B^TB'\n",
    "\n",
    "class FMP_bf(GraphConv):\n",
    "    r\"\"\"Fair message passing layer\n",
    "    \"\"\"\n",
    "    _cached_sen = Optional[SparseTensor]\n",
    "\n",
    "    def __init__(self, \n",
    "                 in_feats: int,\n",
    "                 out_feats: int,\n",
    "                 K: int, \n",
    "                 lambda1: float = None,\n",
    "                 lambda2: float = None,\n",
    "                 L2: bool = True,\n",
    "                 dropout: float = 0.,\n",
    "                 cached: bool = False):\n",
    "\n",
    "        super(FMP, self).__init__(in_feats, out_feats)\n",
    "\n",
    "        self.K = K\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.L2 = L2\n",
    "        self.dropout = dropout\n",
    "        self.cached = cached\n",
    "        self._cached_sen = None  ## sensitive matrix\n",
    "        self.propa = GraphConv(in_feats, in_feats, weight=False, bias=False, activation=None)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self._cached_sen = None\n",
    "\n",
    "    def forward(self, x: Tensor, \n",
    "                g, \n",
    "                idx_sens_train,\n",
    "                edge_weight: OptTensor = None, \n",
    "                sens=None) -> Tensor:\n",
    "        \n",
    "        if g.device != features.device:\n",
    "            g = g.to(features.device)\n",
    "\n",
    "        if self.K <= 0: return x\n",
    "\n",
    "        cache = self._cached_sen\n",
    "        if cache is None:\n",
    "            sen_mat = get_sen(sens=sens, idx_sens_train=idx_sens_train)               ## compute sensitive matrix\n",
    "\n",
    "            if self.cached:\n",
    "                self._cached_sen = sen_mat\n",
    "                self.init_z = torch.zeros((sen_mat.size()[0], x.size()[-1]))\n",
    "        else:\n",
    "            sen_mat = self._cached_sen # N,\n",
    "\n",
    "        hh = x\n",
    "        x = self.emp_forward(g, x=x, hh=hh, sen=sen_mat, K=self.K)\n",
    "        return x\n",
    "\n",
    "    def emp_forward(self, g, x, hh, K, sen):\n",
    "        lambda1 = self.lambda1\n",
    "        lambda2 = self.lambda2\n",
    "\n",
    "        gamma = 1/(1+lambda2)\n",
    "        beta = 1/(2*gamma)\n",
    "\n",
    "        for _ in range(K):\n",
    "\n",
    "            if lambda2 > 0:\n",
    "                y = gamma * hh + (1-gamma) * self.propa(g, feat=x)\n",
    "            else:\n",
    "                y = gamma * hh + (1-gamma) * x # y = x - gamma * (x - hh)\n",
    "\n",
    "            if lambda1 > 0:\n",
    "                z = sen @ F.softmax(y, dim=1) / (gamma * sen @ sen.t())\n",
    "                \n",
    "                x_bar0 = sen.t() @ z\n",
    "                x_bar1 = F.softmax(x_bar0, dim=1) ## node * feature\n",
    "\n",
    "                correct = x_bar0 * x_bar1 \n",
    "\n",
    "                coeff = torch.sum(x_bar0 * x_bar1, 1, keepdim=True)\n",
    "                correct = correct - coeff * x_bar1\n",
    "\n",
    "                x_bar = y - gamma * correct\n",
    "                z_bar  = z + beta * (sen @ F.softmax(x_bar, dim=1))\n",
    "                \n",
    "                if self.L2:\n",
    "                    z  = self.L2_projection(z_bar, lambda_=lambda1, beta=beta)\n",
    "                else:\n",
    "                    z  = self.L1_projection(z_bar, lambda_=lambda1)\n",
    "                \n",
    "                x_bar0 = sen.t() @ z\n",
    "                x_bar1 = F.softmax(x_bar0, dim=1) ## node * feature\n",
    "                \n",
    "                correct = x_bar0 * x_bar1 \n",
    "                coeff = torch.sum(x_bar0 * x_bar1, 1, keepdim=True)\n",
    "                correct = correct - coeff * x_bar1\n",
    "                x = y - gamma * correct\n",
    "            else:\n",
    "                x = y # z=0\n",
    "\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "    def L1_projection(self, x: Tensor, lambda_):\n",
    "        return torch.clamp(x, min=-lambda_, max=lambda_)\n",
    "    \n",
    "    def L2_projection(self, x: Tensor, lambda_, beta):\n",
    "        coeff = (2*lambda_) / (2*lambda_ + beta)\n",
    "        return coeff * x\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_weight: Tensor) -> Tensor:\n",
    "        return edge_weight.view(-1, 1) * x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(K={}, lambda1={}, lambda2={}, L2={})'.format(\n",
    "            self.__class__.__name__, self.K, self.lambda1, self.lambda2, self.L2)\n",
    "\n",
    "class FMP_Q_bf(torch.nn.Module):\n",
    "    def __init__(self, input_size, size, prop, num_layer):\n",
    "        super(FMP_Q, self).__init__()\n",
    "\n",
    "        self.hidden = nn.ModuleList()\n",
    "        for _ in range(num_layer - 2):\n",
    "            self.hidden.append(nn.Linear(size, size))\n",
    "\n",
    "        self.first = nn.Linear(input_size, size)\n",
    "        self.last = nn.Linear(size, 1) \n",
    "\n",
    "        self.prop = prop\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.first.reset_parameters()\n",
    "        self.last.reset_parameters()\n",
    "        self.prop.reset_parameters()\n",
    "        for layer in self.hidden:\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    def forward(self, features, g, sens, idx_sens_train):\n",
    "        x = features\n",
    "        \n",
    "        out = F.relu(self.first(x))\n",
    "        for layer in self.hidden:\n",
    "            out = F.relu(layer(out))\n",
    "        \n",
    "        x = self.last(out).squeeze(-1) \n",
    "        x = self.prop(x, sens=sens, g=g, idx_sens_train=idx_sens_train)\n",
    "        return x\n",
    "\n",
    "# FMP는 group-average correction 수준에 머물러 있어서, distribution-aware 메시지 패싱으로 발전시키려면 몇 가지 방향에서 개선\n",
    "# 각 그룹(예: 남/여, 지역0/1)의 평균값만 계산해서 예측이 한쪽 그룹으로 치우치지 않게 고쳐줌. 근데, 평균값만 보면 그 그룹 안에서 어떤 일이 벌어지는지 모름.\n",
    "class FMP(torch.nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, K, lambda1, lambda2, dropout=0.0, cached=False, L2=True):\n",
    "        super(FMP, self).__init__()\n",
    "        self.K = K   # \n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.L2 = L2\n",
    "        self.dropout = dropout\n",
    "        self.cached = cached\n",
    "        self._cached_sen = None\n",
    "        # self.propa = GCNConv(in_feats, in_feats, add_self_loops=False, normalize=True)\n",
    "        \n",
    "        self.propa = GCNConv(out_feats, out_feats, add_self_loops=False, normalize=True)  # 그래프 컨볼루션 레이어: self-loop 미포함, 메세지 전파시 degree로 normalization\n",
    "\n",
    "    def forward(self, x, edge_index, idx_sens_train, sens):\n",
    "        if self.K <= 0:\n",
    "            return x\n",
    "\n",
    "        if self._cached_sen is None:\n",
    "            sen_mat = get_sen(sens=sens, idx_sens_train=idx_sens_train)  # 민감속성 차이 계산\n",
    "            if self.cached:\n",
    "                self._cached_sen = sen_mat\n",
    "        else:\n",
    "            sen_mat = self._cached_sen\n",
    "\n",
    "        hh = x  # 원래 입력 저장: residual 연결용\n",
    "\n",
    "        for _ in range(self.K): # 메세지 패싱 K 반복\n",
    "            if self.lambda2 > 0:\n",
    "                y = (1 / (1 + self.lambda2)) * hh + (self.lambda2 / (1 + self.lambda2)) * self.propa(x, edge_index)  # propagated 정보 + 원래 입력 비율 섞기\n",
    "            else:\n",
    "                y = hh   # 원래 입력\n",
    "\n",
    "            if self.lambda1 > 0:   # 민감속성 보정 사용 여부\n",
    "                # softmax 출력으로 민감속성별 representation 계산 \n",
    "                # sen_mat: 민감속성 정보를 나타낸 행렬 \n",
    "                # 그룹별 평균 softmax 특징 계산 \n",
    "                z = sen_mat @ F.softmax(y, dim=1) / (sen_mat @ sen_mat.t() + 1e-8)\n",
    "                # 그룹 평균 다시 노드로 확산\n",
    "                x_bar0 = sen_mat.t() @ z\n",
    "                x_bar1 = F.softmax(x_bar0, dim=1)\n",
    "                \n",
    "                # 그룹간 차이 계산\n",
    "                # 각 노드의 그룹 평균과 자기 softmax 값 차이 계산 → global mean 빼서 보정값 추출\n",
    "                correct = x_bar0 * x_bar1\n",
    "                coeff = torch.sum(x_bar0 * x_bar1, dim=1, keepdim=True)\n",
    "                correct = correct - coeff * x_bar1\n",
    "\n",
    "                # 민감속성 보정 적용: 불공정성 줄임\n",
    "                x = y - (1 / (1 + self.lambda2)) * correct\n",
    "            else:\n",
    "                x = y\n",
    "\n",
    "            # dropout으로 overffiting 방지: 네트워크가 너무 특정 노드에 과적합되지 않도록 일부 뉴런을 무작위로 꺼주는 정규화 작업\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        return x\n",
    "\n",
    "class FMP_Q(torch.nn.Module):\n",
    "    def __init__(self, input_size, size, prop, num_layer):\n",
    "        super().__init__()\n",
    "        self.first = nn.Linear(input_size, size)\n",
    "        self.hidden = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layer - 2)])\n",
    "        self.last = nn.Linear(size, 1)\n",
    "        self.prop = prop\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        sens = data.sensitive_attr\n",
    "\n",
    "        out = F.relu(self.first(x))\n",
    "        for layer in self.hidden:\n",
    "            out = F.relu(layer(out))\n",
    "\n",
    "        # x = self.last(x).squeeze(-1)\n",
    "        # x = self.prop(x, edge_index=edge_index, sens=sens, idx_sens_train=data.idx_sens_train)\n",
    "\n",
    "        out = self.prop(out, edge_index=edge_index, sens=sens, idx_sens_train=data.idx_sens_train)\n",
    "\n",
    "        x = self.last(out).squeeze(-1)  # shape: [N]\n",
    "        return x\n",
    "\n",
    "def fmp_model(data, num_layers=5, lambda1=3, lambda2=3, L2=True, num_hidden=64, num_gnn_layer=2):    \n",
    "    prop = FMP(\n",
    "        # in_feats=data.num_features,\n",
    "        # out_feats=data.num_features,\n",
    "        in_feats=num_hidden,  \n",
    "        out_feats=num_hidden,\n",
    "        K=num_layers,    # 메세지 패싱 반복 횟수\n",
    "        lambda1=lambda1, # 민감속성 보정 정도\n",
    "        lambda2=lambda2, # 메세지 패싱과 residual(원래 입력) mixing 비율: oversmoothing 방지\n",
    "        L2=L2,           # L2 norm 프로젝션 여부\n",
    "        cached=False\n",
    "    )\n",
    "\n",
    "    model = FMP_Q(\n",
    "        input_size=data.num_features,\n",
    "        size=num_hidden,\n",
    "        prop=prop,\n",
    "        num_layer=num_gnn_layer\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our\n",
    "class DAFGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, device, lambda_fair=1.0, gamma=1.0, alpha=1.0, use_projection=True, use_fairness=True, use_lambda=True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.lambda_fair = lambda_fair\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.use_projection = use_projection\n",
    "        self.use_fairness = use_fairness\n",
    "        self.use_lambda = use_lambda\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, x, edge_index, return_hidden=False):\n",
    "        h = F.relu(self.conv1(x, edge_index))\n",
    "        h = F.relu(self.conv2(h, edge_index))\n",
    "        \n",
    "        preds = self.fc(h).squeeze(-1)\n",
    "\n",
    "        if return_hidden:\n",
    "            return preds, h\n",
    "        else:\n",
    "            return preds\n",
    "        \n",
    "    def distribution_aware_fairness_loss(self, preds, sensitive, var_gap=True):\n",
    "        g0 = preds[sensitive == 0]\n",
    "        g1 = preds[sensitive == 1]\n",
    "\n",
    "        if len(g0) == 0 or len(g1) == 0:\n",
    "            return torch.tensor(0.0, device=preds.device)\n",
    "\n",
    "        mean_gap = torch.abs(g0.mean() - g1.mean())\n",
    "        gap = mean_gap\n",
    "        \n",
    "        if var_gap:\n",
    "            var_gap = torch.abs(g0.var(unbiased=False) - g1.var(unbiased=False))\n",
    "            gap = mean_gap + self.alpha * var_gap\n",
    "        return gap\n",
    "\n",
    "    def compute_bias_score(self, hidden, sensitive):\n",
    "        g0 = hidden[sensitive == 0]\n",
    "        g1 = hidden[sensitive == 1]\n",
    "        bias = torch.norm(g0.mean(dim=0) - g1.mean(dim=0), p=2)\n",
    "        return bias\n",
    "\n",
    "    def adaptive_projection(self, h, bias_score):\n",
    "        h_norm = h.norm(p=2, dim=1, keepdim=True) + 1e-6\n",
    "        adaptive_lambda = self.lambda_fair * (1 + self.gamma * bias_score)\n",
    "        scale = torch.min(torch.ones_like(h_norm), adaptive_lambda.unsqueeze(-1) / h_norm)\n",
    "        return h * scale\n",
    "\n",
    "    def hierarchical_lambda(self, epoch, max_lambda=1.0, beta=0.05):\n",
    "        device = next(self.parameters()).device  # 모델 파라미터의 디바이스 확인\n",
    "        return max_lambda * (1 - torch.exp(-beta * torch.tensor(epoch, dtype=torch.float32, device=device)))\n",
    "\n",
    "    def total_loss(self, preds, hidden, sensitive, y_true, epoch, tau=0.9):\n",
    "        \n",
    "        loss_qr = quantile_loss(y_true, preds, tau)\n",
    "        \n",
    "        # Projection\n",
    "        if self.use_projection:\n",
    "            bias_score = self.compute_bias_score(hidden, sensitive)\n",
    "            hidden = self.adaptive_projection(hidden, bias_score)\n",
    "        \n",
    "        # Fairness\n",
    "        if self.use_fairness:\n",
    "            fair_loss = self.distribution_aware_fairness_loss(preds, sensitive)  \n",
    "        else:\n",
    "            fair_loss =  torch.tensor(0.0, device=preds.device)\n",
    "\n",
    "        # if self.use_lambda:\n",
    "        #     lambda_fair = self.hierarchical_lambda(epoch, max_lambda=self.lambda_fair)\n",
    "        # else:\n",
    "        #     lambda_fair = self.lambda_fair\n",
    "\n",
    "        return loss_qr + lambda_fair * fair_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분위수 예측은 단순히 \"평균값\"을 맞추는 문제가 아님\n",
    "# 데이터 분포의 전체 모양을 예측하는 문제예요.\n",
    "# 그래서 평균만 잘 맞춰도 되는 일반 회귀와 달리, 퍼짐, 꼬리, 극단값을 잘 잡아야 함\n",
    "\n",
    "# 현재 FMP 같은 fairness-aware GNN은: 그룹별 평균 맞추기, softmax 평균 기반 편향 제거\n",
    "# 근데 분위수 예측에서는: 그룹별로 “평균은 같지만, 분포 꼬리나 퍼짐이 완전 다를 수 있음”\n",
    "# 특히 lower/upper quantile 쪽에서 bias가 많이 발생할 수 있음\n",
    "# 그룹 A: 예측값 [0.2, 0.3, 0.4] → 분위수 안정적\n",
    "# 그룹 B: 예측값 [0.1, 0.9, 0.9] → 분위수 불안정 (overshoot, undershoot)\n",
    "# 결론: 평균 기반 보정만으로는 분위수 예측의 공정성이나 신뢰성을 확보하기 어려움\n",
    "\n",
    "# distribution-aware 메시지 패싱이 왜 필요할까?\n",
    "# 분위수 task에서는 평균, 분산, 꼬리, 범위 같은 분포 정보 전체를 이웃과 공유해야 함\n",
    "# 이웃 정보 섞을 때 “나랑 비슷한 분포”에서만 강하게 섞어야 함\n",
    "# 그룹 간 분위수 차이를 직접 비교·보정해야 함\n",
    "# 예시) 0.9 분위수에서 그룹별 gap이 크게 벌어지면, 메시지 패싱 단계에서 upper quantile 쪽 정보를 집중적으로 보정\n",
    "\n",
    "# 설계 아이디어\n",
    "# 핵심 목표\n",
    "# 분위수 예측에서 단순히 평균 예측 편향만 줄이는 것이 아니라, 분위수 예측의 공정성을 보장하고, 그룹 간 분포 형태(분산, 꼬리, 범위) 차이도 최소화한다. 메시지 패싱 단계에서 이웃 노드 정보도 𝜏 별로 신중히 전달\n",
    "# 주요 설계\n",
    "# 1. 분위수 메세지 패싱\n",
    "# 2. 분위수 보정량(correction) 설계\n",
    "# 3. 메세지 패싱 weight 조정\n",
    "# 4. fairness loss 다층화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "def get_sen_da(sens, idx_sens_train):\n",
    "    num_nodes = sens.size(0)\n",
    "    group_0_mask = (sens == 0)\n",
    "    group_1_mask = (sens == 1)\n",
    "\n",
    "    # idx_sens_train에서 그룹별 마스크\n",
    "    train_group_0_mask = group_0_mask.clone()\n",
    "    train_group_1_mask = group_1_mask.clone()\n",
    "    train_group_0_mask[~idx_sens_train] = False\n",
    "    train_group_1_mask[~idx_sens_train] = False\n",
    "\n",
    "    group_0 = group_0_mask.float()\n",
    "    group_1 = group_1_mask.float()\n",
    "\n",
    "    sum_group_0 = train_group_0_mask.float().sum()\n",
    "    sum_group_1 = train_group_1_mask.float().sum()\n",
    "    if sum_group_0 == 0 or sum_group_1 == 0:\n",
    "        raise ValueError(f\"get_sen_da: One of the groups has no samples in idx_sens_train! group_0 sum: {sum_group_0}, group_1 sum: {sum_group_1}\")\n",
    "\n",
    "    group_0[idx_sens_train] /= sum_group_0\n",
    "    group_1[idx_sens_train] /= sum_group_1\n",
    "\n",
    "    sen_mat = torch.stack([group_0, group_1], dim=0)\n",
    "    return sen_mat\n",
    "\n",
    "class DAFMP(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, K, dropout=0.0, cached=False):\n",
    "        super(DAFMP, self).__init__()\n",
    "        self.K = K\n",
    "        self.dropout = dropout\n",
    "        self.cached = cached\n",
    "        self._cached_sen = None\n",
    "        self.propa = GATConv(in_feats, out_feats, heads=1)\n",
    "\n",
    "    def forward(self, x, edge_index, idx_sens_train, sens, tau, mean_baseline=1.0, var_baseline=1.0, range_baseline=1.0):\n",
    "        if self.K <= 0:\n",
    "            return x, None, None, None\n",
    "\n",
    "        if self._cached_sen is None:\n",
    "            sen_mat = get_sen_da(sens=sens, idx_sens_train=idx_sens_train)\n",
    "            if self.cached:\n",
    "                self._cached_sen = sen_mat\n",
    "        else:\n",
    "            sen_mat = self._cached_sen\n",
    "\n",
    "        for _ in range(self.K):\n",
    "            y = self.propa(x, edge_index)\n",
    "\n",
    "            group_counts = sen_mat.sum(dim=1, keepdim=True)\n",
    "            z_mean = sen_mat @ y / (sen_mat @ torch.ones_like(y))\n",
    "            z_var = (sen_mat @ (y ** 2)) / (group_counts + 1e-8) - z_mean ** 2\n",
    "\n",
    "            group0_mask = sen_mat[0].bool()\n",
    "            group1_mask = sen_mat[1].bool()\n",
    "            y_group0 = y[group0_mask]\n",
    "            y_group1 = y[group1_mask]\n",
    "\n",
    "            if y_group0.size(0) == 0 or y_group1.size(0) == 0:\n",
    "                raise ValueError(\"One of the groups has no samples! Check your sens and idx_sens_train.\")\n",
    "\n",
    "            z_min0, _ = torch.min(y_group0, dim=0, keepdim=True)\n",
    "            z_max0, _ = torch.max(y_group0, dim=0, keepdim=True)\n",
    "            z_min1, _ = torch.min(y_group1, dim=0, keepdim=True)\n",
    "            z_max1, _ = torch.max(y_group1, dim=0, keepdim=True)\n",
    "\n",
    "            z_range0 = z_max0 - z_min0\n",
    "            z_range1 = z_max1 - z_min1\n",
    "\n",
    "            mean_gap = torch.abs(z_mean[0] - z_mean[1])\n",
    "            var_gap = torch.abs(z_var[0] - z_var[1])\n",
    "            range_gap = torch.abs(z_range0 - z_range1)\n",
    "\n",
    "            mean_gap_norm = mean_gap / (mean_baseline + 1e-8)\n",
    "            var_gap_norm = var_gap / (var_baseline + 1e-8)\n",
    "            range_gap_norm = range_gap / (range_baseline + 1e-8)\n",
    "\n",
    "            tau_scalar = tau.mean() if isinstance(tau, torch.Tensor) and tau.numel() > 1 else tau\n",
    "\n",
    "            correction = tau_scalar * (mean_gap_norm + var_gap_norm + range_gap_norm)\n",
    "\n",
    "            x = y - correction\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        return x, mean_gap, var_gap, range_gap\n",
    "\n",
    "class DAFGNN_2(nn.Module):\n",
    "    def __init__(self, input_size, size, prop, num_layer):\n",
    "        super().__init__()\n",
    "        self.first = nn.Linear(input_size, size)\n",
    "        self.hidden = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layer - 2)])\n",
    "        self.last = nn.Linear(size, 1)\n",
    "        self.prop = prop\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        sens = data.sensitive_attr\n",
    "        tau = data.quantile_tau\n",
    "\n",
    "        out = F.relu(self.first(x))\n",
    "        for layer in self.hidden:\n",
    "            out = F.relu(layer(out))\n",
    "\n",
    "        # DAFMP 호출 후 tuple 언팩\n",
    "        out, mean_gap, var_gap, range_gap = self.prop(\n",
    "            out, edge_index=edge_index, sens=sens, idx_sens_train=data.idx_sens_train, tau=tau\n",
    "        )\n",
    "\n",
    "        x = self.last(out).squeeze(-1)\n",
    "        return x, mean_gap, var_gap, range_gap\n",
    "\n",
    "def daf_model(data, num_layers=5, num_hidden=64, num_gnn_layer=2):\n",
    "    prop = DAFMP(\n",
    "        in_feats=num_hidden,\n",
    "        out_feats=num_hidden,\n",
    "        K=num_layers,\n",
    "        cached=False\n",
    "    )\n",
    "\n",
    "    model = DAFGNN_2(\n",
    "        input_size=data.num_features,\n",
    "        size=num_hidden,\n",
    "        prop=prop,\n",
    "        num_layer=num_gnn_layer\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def daf_loss(y_true, y_pred, tau, mean_gap, var_gap, range_gap, lambda_fair, mean_baseline=1.0, var_baseline=1.0, range_baseline=1.0):\n",
    "    diff = y_true - y_pred\n",
    "    qr_loss = torch.mean(torch.max(tau * diff, (tau - 1) * diff))\n",
    "\n",
    "    mean_gap_norm = mean_gap / (mean_baseline + 1e-8)\n",
    "    var_gap_norm = var_gap / (var_baseline + 1e-8)\n",
    "    range_gap_norm = range_gap / (range_baseline + 1e-8)\n",
    "\n",
    "    # Reduce to scalar\n",
    "    fairness_loss = lambda_fair * (mean_gap_norm.mean() + var_gap_norm.mean() + range_gap_norm.mean())\n",
    "\n",
    "    return qr_loss + fairness_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAFMP(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, K, dropout=0.0, cached=False, lambda_fair=1.0, gamma=1.0, use_projection=True):\n",
    "        super(DAFMP, self).__init__()\n",
    "        self.K = K\n",
    "        self.dropout = dropout\n",
    "        self.cached = cached\n",
    "        self.lambda_fair = lambda_fair\n",
    "        self.gamma = gamma\n",
    "        self.use_projection = use_projection\n",
    "        self._cached_sen = None\n",
    "\n",
    "        self.propa = GATConv(out_feats + 1, out_feats, heads=1)\n",
    "\n",
    "    def compute_bias_score(self, hidden, sensitive):\n",
    "        g0 = hidden[sensitive == 0]\n",
    "        g1 = hidden[sensitive == 1]\n",
    "        if len(g0) == 0 or len(g1) == 0:\n",
    "            return torch.tensor(0.0, device=hidden.device)\n",
    "        return torch.norm(g0.mean(dim=0) - g1.mean(dim=0), p=2)\n",
    "\n",
    "    def adaptive_projection(self, h, bias_score):\n",
    "        h_norm = h.norm(p=2, dim=1, keepdim=True) + 1e-6\n",
    "        adaptive_lambda = self.lambda_fair * (1 + self.gamma * bias_score)\n",
    "        scale = torch.min(torch.ones_like(h_norm), adaptive_lambda.unsqueeze(-1) / h_norm)\n",
    "        return h * scale\n",
    "\n",
    "    def forward(self, x, edge_index, idx_sens_train, sens, tau, mean_baseline=1.0, var_baseline=1.0, range_baseline=1.0):\n",
    "        if self.K <= 0:\n",
    "            return x, 0, 0, 0\n",
    "\n",
    "        if self._cached_sen is None:\n",
    "            sen_mat = get_sen(sens=sens, idx_sens_train=idx_sens_train)\n",
    "            if self.cached:\n",
    "                self._cached_sen = sen_mat\n",
    "        else:\n",
    "            sen_mat = self._cached_sen\n",
    "\n",
    "        tau_expand = tau.unsqueeze(-1).repeat(1, x.size(1))\n",
    "        x = torch.cat([x, tau_expand], dim=1)\n",
    "\n",
    "        for _ in range(self.K):\n",
    "            y = self.propa(x, edge_index)\n",
    "\n",
    "            z_mean = sen_mat @ y / (sen_mat @ sen_mat.t() + 1e-8)\n",
    "            z_var = sen_mat @ (y ** 2) / (sen_mat @ sen_mat.t() + 1e-8) - z_mean ** 2\n",
    "            z_min, _ = torch.min(y, dim=0, keepdim=True)\n",
    "            z_max, _ = torch.max(y, dim=0, keepdim=True)\n",
    "            z_range = z_max - z_min\n",
    "\n",
    "            mean_gap = torch.abs(z_mean[0] - z_mean[1])\n",
    "            var_gap = torch.abs(z_var[0] - z_var[1])\n",
    "            range_gap = torch.abs(z_range[0] - z_range[1])\n",
    "\n",
    "            mean_gap_norm = mean_gap / (mean_baseline + 1e-8)\n",
    "            var_gap_norm = var_gap / (var_baseline + 1e-8)\n",
    "            range_gap_norm = range_gap / (range_baseline + 1e-8)\n",
    "\n",
    "            correction = mean_gap_norm + var_gap_norm + range_gap_norm\n",
    "            x = y - correction\n",
    "\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        if self.use_projection:\n",
    "            bias_score = self.compute_bias_score(x, sens)\n",
    "            x = self.adaptive_projection(x, bias_score)\n",
    "\n",
    "        return x, mean_gap, var_gap, range_gap\n",
    "\n",
    "class DAFGNN_2(nn.Module):\n",
    "    def __init__(self, input_size, size, prop, num_layer):\n",
    "        super().__init__()\n",
    "        self.first = nn.Linear(input_size, size)\n",
    "        self.hidden = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layer - 2)])\n",
    "        self.last = nn.Linear(size, 1)\n",
    "        self.prop = prop\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        sens = data.sensitive_attr\n",
    "        tau = data.quantile_tau\n",
    "\n",
    "        out = F.relu(self.first(x))\n",
    "        for layer in self.hidden:\n",
    "            out = F.relu(layer(out))\n",
    "\n",
    "        out, mean_gap, var_gap, range_gap = self.prop(out, edge_index=edge_index, sens=sens,\n",
    "                                                      idx_sens_train=data.idx_sens_train, tau=tau)\n",
    "        x = self.last(out).squeeze(-1)\n",
    "        return x, mean_gap, var_gap, range_gap\n",
    "\n",
    "def daf_loss(y_true, y_pred, tau, mean_gap, var_gap, range_gap, lambda_fair,\n",
    "             mean_baseline=1.0, var_baseline=1.0, range_baseline=1.0):\n",
    "    diff = y_true - y_pred\n",
    "    qr_loss = torch.mean(torch.max(tau * diff, (tau - 1) * diff))\n",
    "\n",
    "    mean_gap_norm = mean_gap / (mean_baseline + 1e-8)\n",
    "    var_gap_norm = var_gap / (var_baseline + 1e-8)\n",
    "    range_gap_norm = range_gap / (range_baseline + 1e-8)\n",
    "\n",
    "    fairness_loss = lambda_fair * (mean_gap_norm + var_gap_norm + range_gap_norm)\n",
    "\n",
    "    return qr_loss + fairness_loss\n",
    "\n",
    "def daf_model(data, num_layers=5, num_hidden=64, num_gnn_layer=2, lambda_fair=1.0, gamma=1.0, use_projection=True):\n",
    "    prop = DAFMP(\n",
    "        in_feats=num_hidden,\n",
    "        out_feats=num_hidden,\n",
    "        K=num_layers,\n",
    "        cached=False,\n",
    "        lambda_fair=lambda_fair,\n",
    "        gamma=gamma,\n",
    "        use_projection=use_projection\n",
    "    )\n",
    "\n",
    "    model = DAFGNN_2(\n",
    "        input_size=data.num_features,\n",
    "        size=num_hidden,\n",
    "        prop=prop,\n",
    "        num_layer=num_gnn_layer\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드 \n",
    "def feature_norm(features):\n",
    "    min_values = features.min(axis=0)[0]\n",
    "    max_values = features.max(axis=0)[0]\n",
    "    return 2*(features - min_values).div(max_values-min_values) - 1\n",
    "\n",
    "def load_dataset(dataset,sens_attr,predict_attr, path=\"./dataset/pokec/\", \n",
    "               label_number=1000, sens_number=500, test_idx=False, seed=1127):\n",
    "    \"\"\"Load data\"\"\"\n",
    "    print('Loading {} dataset from {}'.format(dataset,path))\n",
    "\n",
    "    idx_features_labels = pd.read_csv(os.path.join(path,\"{}.csv\".format(dataset)))\n",
    "    header = list(idx_features_labels.columns)\n",
    "    header.remove(\"user_id\")\n",
    "\n",
    "    header.remove(sens_attr)\n",
    "    header.remove(predict_attr)\n",
    "\n",
    "    features = sp.csr_matrix(idx_features_labels[header], dtype=np.float32)\n",
    "    labels = idx_features_labels[predict_attr].values\n",
    "    \n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[\"user_id\"], dtype=int)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "\n",
    "    # edges_unordered = np.genfromtxt(os.path.join(path,\"{}_relationship.txt\".format(dataset)), dtype=int)\n",
    "    edges_unordered = np.genfromtxt(os.path.join(path, f\"{dataset}_relationship.txt\"), dtype=np.int64)\n",
    "\n",
    "    # edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "    #                  dtype=int).reshape(edges_unordered.shape)\n",
    "    mapped_edges = []\n",
    "    dropped = 0\n",
    "    for u, v in edges_unordered:\n",
    "        u_mapped = idx_map.get(u)\n",
    "        v_mapped = idx_map.get(v)\n",
    "        if u_mapped is not None and v_mapped is not None:\n",
    "            mapped_edges.append((u_mapped, v_mapped))\n",
    "        else:\n",
    "            dropped += 1\n",
    "\n",
    "    print(f\"[INFO] 유효하지 않은 user_id로 인해 제거된 edge 수: {dropped}\")\n",
    "    edges = np.array(mapped_edges, dtype=int)\n",
    "\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "    \n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    # features = normalize(features)\n",
    "    adj = adj + sp.eye(adj.shape[0])\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(labels)\n",
    "    # adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    random.seed(seed)\n",
    "    label_idx = np.where(labels>=0)[0]\n",
    "    random.shuffle(label_idx)\n",
    "\n",
    "    idx_train = label_idx[:min(int(0.5 * len(label_idx)),label_number)]\n",
    "    idx_val = label_idx[int(0.5 * len(label_idx)):int(0.75 * len(label_idx))]\n",
    "    if test_idx:\n",
    "        idx_test = label_idx[label_number:]\n",
    "        idx_val = idx_test\n",
    "    else:\n",
    "        idx_test = label_idx[int(0.75 * len(label_idx)):]\n",
    "\n",
    "    sens = idx_features_labels[sens_attr].values\n",
    "\n",
    "    sens_idx = set(np.where(sens >= 0)[0])\n",
    "    idx_test = np.asarray(list(sens_idx & set(idx_test)))\n",
    "    sens = torch.FloatTensor(sens)\n",
    "    idx_sens_train = list(sens_idx - set(idx_val) - set(idx_test))\n",
    "    random.seed(seed)\n",
    "    random.shuffle(idx_sens_train)\n",
    "    idx_sens_train = torch.LongTensor(idx_sens_train[:sens_number])\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    \n",
    "    # random.shuffle(sens_idx)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test, sens,idx_sens_train\n",
    "\n",
    "def is_symmetric(m):\n",
    "    '''\n",
    "    Judge whether the matrix is symmetric or not.\n",
    "    :param m: Adjacency matrix(Array)\n",
    "    '''\n",
    "    res = np.int64(np.triu(m).T == np.tril(m))\n",
    "    # if np.where(res==0)[0] != []:\n",
    "    if np.where(res == 0)[0].size > 0:\n",
    "        raise ValueError(\"The matrix is not symmetric!\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "def symetric_normalize(m, half: bool):\n",
    "    '''\n",
    "    Symmetrically normalization for adjacency matrix\n",
    "    :param m: (Array) Adjacency matrix\n",
    "    :param half: (bool) whether m is triu or full\n",
    "    :return: (Array) An symmetric adjacency matrix\n",
    "    '''\n",
    "    if not half:\n",
    "        is_symmetric(m)\n",
    "    else:\n",
    "        m = m + m.T - np.diag(np.diagonal(m))\n",
    "\n",
    "    hat_m = m + np.eye(m.shape[0])\n",
    "    D = np.sum(hat_m, axis=1)\n",
    "    D = np.diag(D)\n",
    "\n",
    "    # D = np.power(D, -0.5)\n",
    "    with np.errstate(divide='ignore'):\n",
    "        D = np.power(D, -0.5)\n",
    "        D[np.isinf(D)] = 0\n",
    "\n",
    "    D[np.isinf(D)] = 0\n",
    "    sn_m = np.matmul(np.matmul(D, hat_m), D)\n",
    "    return sn_m\n",
    "\n",
    "def sp2sptensor(m):\n",
    "    # sparse_m = sp.coo_matrix(m).astype(np.float)\n",
    "    sparse_m = sp.coo_matrix(m).astype(np.float64)\n",
    "    # indices = torch.from_numpy(np.vstack((sparse_m.row, sparse_m.col)).astype(int))\n",
    "    indices = torch.from_numpy(np.vstack((sparse_m.row, sparse_m.col))).long()\n",
    "    values = torch.from_numpy(sparse_m.data)\n",
    "    shape = torch.Size(sparse_m.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "class German:\n",
    "    def __init__(self, path):\n",
    "        if not os.path.exists(path):\n",
    "            raise ValueError(\"Data path doesn't exist!\")\n",
    "        else:\n",
    "            self.data_path = path\n",
    "        self.raw_data = self._node_process()\n",
    "        self.A_tensor, self.A = self._edge_process()\n",
    "        self.senIdx, self.sen_vals, self.trainIdxtensor, self.valIdxTensor, self.testIdxTensor, self.features, self.labels = self._split_data()\n",
    "\n",
    "    def _node_process(self):\n",
    "        filenames = os.listdir(self.data_path)\n",
    "\n",
    "        for file in filenames:\n",
    "            if os.path.splitext(file)[1] != '.csv':\n",
    "                continue\n",
    "            else:\n",
    "                df_data = pd.read_csv(os.path.join(self.data_path, file))\n",
    "\n",
    "                # modify str feature\n",
    "                # df_data['GoodCustomer'].replace(-1, 0, inplace=True)\n",
    "                df_data['GoodCustomer'] = df_data['GoodCustomer'].replace(-1, 0).astype(int)\n",
    "\n",
    "                # df_data['Gender'].replace('Male', 1, inplace=True)\n",
    "                # df_data['Gender'].replace('Female', 0, inplace=True)\n",
    "                gender_map = {'Female': 0, 'Male': 1}\n",
    "                df_data['Gender'] = df_data['Gender'].map(gender_map).astype(int)\n",
    "\n",
    "                purposeList = list(df_data['PurposeOfLoan'])\n",
    "                random.shuffle(purposeList)\n",
    "                purposeDict = {}\n",
    "                index = 0\n",
    "                for pur in purposeList:\n",
    "                    if purposeDict.get(pur, None) is None:\n",
    "                        purposeDict[pur] = index\n",
    "                        index += 1\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                for key in purposeDict.keys():\n",
    "                    # df_data['PurposeOfLoan'].replace(key, purposeDict[key], inplace=True)\n",
    "                    df_data['PurposeOfLoan'] = df_data['PurposeOfLoan'].map(purposeDict).fillna(-1).astype(int)\n",
    "                    \n",
    "                return df_data\n",
    "\n",
    "    def _edge_process(self):\n",
    "        filenames = os.listdir(self.data_path)\n",
    "\n",
    "        for file in filenames:\n",
    "            if os.path.splitext(file)[1] != '.txt':\n",
    "                continue\n",
    "            else:\n",
    "                edges = np.loadtxt(os.path.join(self.data_path, file)).astype(int)\n",
    "\n",
    "                # Adjacency\n",
    "                num_dim = len(self.raw_data)\n",
    "                A = np.zeros((num_dim, num_dim))\n",
    "                for i in range(len(edges)):\n",
    "                    A[edges[i][0]][edges[i][1]] = 1\n",
    "                    A[edges[i][1]][edges[i][0]] = 1\n",
    "                sym_norm_A = symetric_normalize(A, half=False)\n",
    "                syn_norm_A_tensor = sp2sptensor(sym_norm_A)\n",
    "                return syn_norm_A_tensor, A\n",
    "\n",
    "    def _split_data(self):\n",
    "        pos_data = self.raw_data[self.raw_data['GoodCustomer']==1]\n",
    "        pos_index = list(pos_data.index)\n",
    "        neg_data = self.raw_data[self.raw_data['GoodCustomer']==0]\n",
    "        neg_index = list(neg_data.index)\n",
    "\n",
    "        # shuffle the index\n",
    "        random.seed(20)\n",
    "        random.shuffle(pos_index)\n",
    "        random.shuffle(neg_index)\n",
    "\n",
    "        # split the data\n",
    "        train_pos_idx = pos_index[:int(0.5*len(pos_index))]\n",
    "        train_neg_idx = neg_index[:int(0.5*len(neg_index))]\n",
    "        val_pos_idx = pos_index[int(0.5*len(pos_index)): int(0.75*len(pos_index))]\n",
    "        val_neg_idx = neg_index[int(0.5*len(neg_index)): int(0.75*len(neg_index))]\n",
    "        test_pos_idx = pos_index[int(0.75*len(pos_index)):]\n",
    "        test_neg_idx = neg_index[int(0.75*len(neg_index)):]\n",
    "\n",
    "        trainIdx = train_pos_idx + train_neg_idx\n",
    "        random.shuffle(trainIdx)\n",
    "        valIdx = val_pos_idx + val_neg_idx\n",
    "        random.shuffle(valIdx)\n",
    "        testIdx = test_pos_idx + test_neg_idx\n",
    "        random.shuffle(testIdx)\n",
    "\n",
    "        assert len(trainIdx)+len(valIdx)+len(testIdx) == len(self.raw_data), \"Missing data or leaking data!\"\n",
    "\n",
    "        feature_cols = list(self.raw_data.columns)\n",
    "        feature_cols.remove('GoodCustomer')\n",
    "        sen_idx = feature_cols.index('Gender')\n",
    "        sen_vals = self.raw_data['Gender'].values.astype(int)\n",
    "        feature_data = self.raw_data[feature_cols]\n",
    "        labels = self.raw_data['GoodCustomer']\n",
    "\n",
    "        # transform to tensor\n",
    "        trainIdxTensor = torch.LongTensor(trainIdx)\n",
    "        valIdxTensor = torch.LongTensor(valIdx)\n",
    "        testIdxTensor = torch.LongTensor(testIdx)\n",
    "        featuredata = torch.FloatTensor(np.array(feature_data))\n",
    "        labels = torch.LongTensor(np.array(labels))\n",
    "\n",
    "        return sen_idx, sen_vals, trainIdxTensor, valIdxTensor, testIdxTensor, featuredata, labels\n",
    "\n",
    "    def get_index(self):\n",
    "        return [self.trainIdxtensor, self.valIdxTensor, self.testIdxTensor]\n",
    "\n",
    "    def get_raw_data(self):\n",
    "        return [self.features, self.A_tensor, self.labels]\n",
    "\n",
    "    def generate_counterfactual_perturbation(self, data):\n",
    "        '''\n",
    "        Generate counterfactual data by flipping sensitive attribute.\n",
    "        :param data: Tensor\n",
    "        :return: counterfactual data\n",
    "        '''\n",
    "        feature_data = copy.deepcopy(data)\n",
    "        feature_data[:, self.senIdx] = 1 - feature_data[:, self.senIdx]\n",
    "        return feature_data\n",
    "\n",
    "    def generate_node_perturbation(self, prob: float, sen: bool = False):\n",
    "        '''\n",
    "        Perturbing node attributes, except for sensitive attributes.\n",
    "        :param prob: portion of perturbed data\n",
    "        :return: perturbed data\n",
    "        '''\n",
    "        feature_data = copy.deepcopy(self.features)\n",
    "        r = np.random.binomial(n=1, p=prob, size=feature_data.numpy().shape)\n",
    "        for i in range(len(feature_data)):\n",
    "            r[i][self.senIdx] = 0\n",
    "        noise = np.multiply(r, np.random.normal(0., 1., r.shape))\n",
    "        noise_tensor = torch.FloatTensor(noise)\n",
    "        x_hat = feature_data + noise_tensor\n",
    "\n",
    "        if sen:\n",
    "            x_hat = self.generate_counterfactual_perturbation(x_hat)\n",
    "        return x_hat\n",
    "\n",
    "    def generate_struc_perturbation(self, drop_prob: float, tensor: bool = True):\n",
    "        A = copy.deepcopy(self.A)\n",
    "        half_A = np.triu(A)\n",
    "        row, col = np.nonzero(half_A)\n",
    "        idx_perturb = np.random.binomial(n=1, p=1-drop_prob, size=row.shape)\n",
    "        broken_edges = np.where(idx_perturb==0)[0]\n",
    "        for idx in broken_edges:\n",
    "            half_A[row[idx]][col[idx]] = 0\n",
    "        new_A = symetric_normalize(half_A, half=True)\n",
    "        if tensor:\n",
    "            new_A = sp2sptensor(new_A)\n",
    "        return new_A\n",
    "\n",
    "def load_dataset_unified(dataset, sens_attr, predict_attr, path=\"./dataset/\", \n",
    "                         label_number=1000, sens_number=500, test_idx=False, seed=1127):\n",
    "    if dataset.lower() != 'german':\n",
    "        full_path = path\n",
    "        # full_path = os.path.join(path, \"pokec\")\n",
    "        return load_dataset(dataset, sens_attr, predict_attr, full_path, label_number, sens_number, test_idx, seed)\n",
    "\n",
    "    elif dataset.lower() == 'german':\n",
    "        full_path = path\n",
    "        # full_path = os.path.join(path, \"german\")\n",
    "        german_data = German(full_path)\n",
    "\n",
    "        adj = german_data.A_tensor\n",
    "        features = german_data.features\n",
    "        labels = german_data.labels\n",
    "        sens = torch.FloatTensor(german_data.sen_vals)\n",
    "        idx_train, idx_val, idx_test = german_data.get_index()\n",
    "\n",
    "        # 민감 속성 학습용 인덱스 (val/test 제외)\n",
    "        all_idx = set(range(len(sens)))\n",
    "        sens_idx = set(np.where(sens.numpy() >= 0)[0])\n",
    "        idx_sens_train = list(sens_idx - set(idx_val.numpy()) - set(idx_test.numpy()))\n",
    "        random.seed(seed)\n",
    "        random.shuffle(idx_sens_train)\n",
    "        idx_sens_train = torch.LongTensor(idx_sens_train[:sens_number])\n",
    "\n",
    "        return adj, features, labels, idx_train, idx_val, idx_test, sens, idx_sens_train\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset: {dataset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 함수\n",
    "def evaluate(model, data, idx_eval, tau=0.9):\n",
    "    model.eval()\n",
    "    if isinstance(model, DAFGNN):\n",
    "        preds = model(data.x, data.edge_index, return_hidden=False)\n",
    "    elif isinstance(model, DAFGNN_2):\n",
    "        preds = model(data)\n",
    "    elif isinstance(model, MYFairGNN):\n",
    "        preds = model(data.x, data.edge_index)\n",
    "    elif isinstance(model, MYPlainGNN):\n",
    "        preds = model(data.x, data.edge_index)\n",
    "    elif isinstance(model, FairGNN_Q):\n",
    "        preds, _ = model(data)\n",
    "    elif isinstance(model, FMP_Q):\n",
    "        preds = model(data)\n",
    "        \n",
    "    preds_eval = preds[idx_eval]\n",
    "    sens_eval = data.sensitive_attr[idx_eval]\n",
    "    \n",
    "    g0 = preds_eval[sens_eval == 0]\n",
    "    g1 = preds_eval[sens_eval == 1]\n",
    "    \n",
    "    if len(g0) == 0 or len(g1) == 0:\n",
    "        fair_loss = None\n",
    "    else:\n",
    "        fair_loss = torch.abs(g0.mean() - g1.mean()).item()\n",
    "        \n",
    "    qr_loss = quantile_loss(data.y[idx_eval], preds_eval, tau).item()\n",
    "    return qr_loss, fair_loss\n",
    "\n",
    "def evaluate_metrics(model, data, idx_eval, tau=0.9):\n",
    "    model.eval()\n",
    "\n",
    "    if isinstance(model, DAFGNN):\n",
    "        preds, _ = model(data.x, data.edge_index, return_hidden=True)\n",
    "    elif isinstance(model, MYFairGNN):\n",
    "        preds = model(data.x, data.edge_index)\n",
    "    elif isinstance(model, MYPlainGNN):\n",
    "        preds = model(data.x, data.edge_index)\n",
    "    elif isinstance(model, FairGNN_Q):\n",
    "        preds, _ = model(data)\n",
    "    elif isinstance(model, FMP_Q):\n",
    "        preds = model(data)\n",
    "    elif isinstance(model, DAFGNN_2):\n",
    "        preds = model(data)\n",
    "\n",
    "    preds = preds[idx_eval].cpu()\n",
    "    y_true = data.y[idx_eval].cpu()\n",
    "    sensitive = data.sensitive_attr[idx_eval].cpu()\n",
    "\n",
    "    error = y_true - preds\n",
    "    qr_loss = torch.mean(torch.max(tau * error, (tau - 1) * error)).item()\n",
    "    mae = torch.mean(torch.abs(error)).item()\n",
    "    rmse = torch.sqrt(torch.mean(error ** 2)).item()\n",
    "    r2 = 1 - torch.sum(error ** 2) / torch.sum((y_true - y_true.mean()) ** 2)\n",
    "\n",
    "    group0 = preds[sensitive == 0]\n",
    "    group1 = preds[sensitive == 1]\n",
    "    fair_gap = torch.abs(group0.mean() - group1.mean()).item()\n",
    "\n",
    "    results = {\n",
    "        'QR Loss': qr_loss,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2.item(),\n",
    "        'Fairness Gap': fair_gap\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "def our_groupwise_evaluation(model, data, idx_eval, tau=0.9):\n",
    "    model.eval()\n",
    "\n",
    "    preds, _ = model(data.x, data.edge_index, return_hidden=True)\n",
    "\n",
    "    preds = preds[idx_eval]\n",
    "    y_true = data.y[idx_eval]\n",
    "    sensitive = data.sensitive_attr[idx_eval]\n",
    "\n",
    "    # 전체 QR loss\n",
    "    overall_qr = quantile_loss(y_true, preds, tau).item()\n",
    "\n",
    "    # Group별 나누기\n",
    "    preds_g0 = preds[sensitive == 0]\n",
    "    preds_g1 = preds[sensitive == 1]\n",
    "    y_true_g0 = y_true[sensitive == 0]\n",
    "    y_true_g1 = y_true[sensitive == 1]\n",
    "\n",
    "    # Group별 QR loss\n",
    "    qr_loss_g0 = quantile_loss(y_true_g0, preds_g0, tau).item()\n",
    "    qr_loss_g1 = quantile_loss(y_true_g1, preds_g1, tau).item()\n",
    "\n",
    "    # Group별 예측 평균 (Fairness 측정용)\n",
    "    mean_pred_g0 = preds_g0.mean().item()\n",
    "    mean_pred_g1 = preds_g1.mean().item()\n",
    "    fairness_gap = abs(mean_pred_g0 - mean_pred_g1)\n",
    "\n",
    "    results = {\n",
    "        'Overall QR Loss': overall_qr,\n",
    "        'Group 0 QR Loss': qr_loss_g0,\n",
    "        'Group 1 QR Loss': qr_loss_g1,\n",
    "        'Fairness Gap (mean)': fairness_gap,\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "def regression_evaluation_metrics(model, data, idx_eval, tau=0.9):\n",
    "    model.eval()\n",
    "\n",
    "    # 예측\n",
    "    if isinstance(model, DAFGNN):\n",
    "        preds, _ = model(data.x, data.edge_index, return_hidden=True)\n",
    "    elif isinstance(model, (MYFairGNN, MYPlainGNN)):\n",
    "        preds = model(data.x, data.edge_index)\n",
    "    elif isinstance(model, FairGNN_Q):\n",
    "        preds, _ = model(data)\n",
    "    elif isinstance(model, FMP_Q):\n",
    "        preds = model(data)\n",
    "    elif isinstance(model, DAFGNN_2):\n",
    "        preds_tuple = model(data)\n",
    "        if isinstance(preds_tuple, tuple):\n",
    "            preds = preds_tuple[0]\n",
    "        else:\n",
    "            preds = preds_tuple\n",
    "\n",
    "    # Tensor → numpy\n",
    "    y_pred = preds[idx_eval].detach().cpu().numpy().flatten()\n",
    "    y_true = data.y[idx_eval].detach().cpu().numpy().flatten()\n",
    "    sensitive_attr = data.sensitive_attr[idx_eval].detach().cpu().numpy().flatten()\n",
    "\n",
    "    # 기본 오류 계산\n",
    "    error = y_true - y_pred\n",
    "    abs_error = np.abs(error)\n",
    "    sq_error = error ** 2\n",
    "\n",
    "    # MAPE 관련 계산 시 y_true가 너무 작으면 제거\n",
    "    mask = np.abs(y_true) > 1e-4\n",
    "    mape_safe = abs_error[mask] / (np.abs(y_true[mask]) + 1e-8)\n",
    "    mape = np.mean(mape_safe)\n",
    "    mape_var = np.var(mape_safe)\n",
    "\n",
    "    # QR Loss\n",
    "    error_torch = torch.tensor(error)\n",
    "    qr_loss = torch.mean(torch.max(tau * error_torch, (tau - 1) * error_torch)).item()\n",
    "\n",
    "    # 정확도\n",
    "    mae = np.mean(abs_error)\n",
    "    rmse = np.sqrt(np.mean(sq_error))\n",
    "    mae_var = np.var(abs_error)\n",
    "    r2 = 1 - np.sum(sq_error) / np.sum((y_true - y_true.mean()) ** 2)\n",
    "\n",
    "    # 그룹별\n",
    "    df = pd.DataFrame({\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'group': sensitive_attr\n",
    "    })\n",
    "\n",
    "    group_metrics = {}\n",
    "    for group_id in np.unique(sensitive_attr):\n",
    "        group = df[df['group'] == group_id]\n",
    "        g_m = np.mean(group['y_pred'])\n",
    "        g_err = np.abs(group['y_true'] - group['y_pred'])\n",
    "        g_sq = (group['y_true'] - group['y_pred']) ** 2\n",
    "\n",
    "        group_metrics[f'mean_group_{int(group_id)}'] = g_m\n",
    "        group_metrics[f'mae_group_{int(group_id)}'] = g_err.mean()\n",
    "        group_metrics[f'rmse_group_{int(group_id)}'] = np.sqrt(g_sq.mean())\n",
    "\n",
    "    mean_gap = abs(group_metrics.get('mean_group_0', 0) - group_metrics.get('mean_group_1', 0))\n",
    "    mae_gap = abs(group_metrics.get('mae_group_0', 0) - group_metrics.get('mae_group_1', 0))\n",
    "    rmse_gap = abs(group_metrics.get('rmse_group_0', 0) - group_metrics.get('rmse_group_1', 0))\n",
    "\n",
    "    # 민감속성과 오차의 상관관계\n",
    "    corr_error = np.corrcoef(abs_error, sensitive_attr)[0, 1]    \n",
    "\n",
    "    return {\n",
    "        'QR Loss': qr_loss,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'MAE-Var': mae_var,\n",
    "        'MAPE-Var': mape_var,\n",
    "        'Mean Gap': mean_gap,\n",
    "        'MAE Gap': mae_gap,\n",
    "        'RMSE Gap': rmse_gap,\n",
    "        'Error-Sensitive Corr': corr_error,\n",
    "        **group_metrics\n",
    "    }\n",
    "\n",
    "def generate_latex_rows(df, metric_names, end_column=' \\\\\\\\'):\n",
    "    latex_lines = []\n",
    "    for idx, row in df.iterrows():\n",
    "        model = row['Model']\n",
    "        line = f\"& {model}\"\n",
    "        for metric in metric_names:\n",
    "            mean = row[f\"{metric} Mean\"]\n",
    "            std = row[f\"{metric} Std\"]\n",
    "            line += f\" & ${mean:.2f} \\\\pm {std:.2f}$\"\n",
    "        line += f\" {end_column} \\\\\\\\\"\n",
    "        latex_lines.append(line)\n",
    "    return latex_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "def train_model(model_class, data, device, tau=0.9, lambda_fair=1.0, hidden_dim=64, n_epochs=500, lr=0.01, weight=5e-4):\n",
    "    in_dim = data.x.size(1)\n",
    "    if model_class.__name__ == 'DAFGNN':\n",
    "        model = model_class(in_dim, hidden_dim, device, lambda_fair).to(device)\n",
    "    elif model_class.__name__ == 'MYFairGNN':\n",
    "        model = model_class(in_dim, hidden_dim, lambda_fair).to(device)\n",
    "    elif model_class.__name__ == 'MYPlainGNN':\n",
    "        model = model_class(in_dim, hidden_dim).to(device)\n",
    "    elif model_class.__name__ == 'FairGNN_Q':\n",
    "        model = model_class(in_dim, hidden_dim, tau).to(device)\n",
    "    elif model_class.__name__ == 'FMP_Q':\n",
    "        model = fmp_model(data).to(device)\n",
    "    elif model_class.__name__ == 'DAFGNN_2':\n",
    "        model = daf_model(data).to(device)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight)\n",
    "\n",
    "    data = data.to(device)\n",
    "    data.sensitive_attr = data.sensitive_attr.to(device)\n",
    "    \n",
    "    print('-' * 100)\n",
    "    n_epochs = int(n_epochs)\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if isinstance(model, DAFGNN):\n",
    "            preds, hidden = model(data.x, data.edge_index, return_hidden=True)\n",
    "            loss = model.total_loss(preds[data.idx_train], hidden[data.idx_train], data.sensitive_attr[data.idx_train], data.y[data.idx_train], epoch, tau)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        elif isinstance(model, MYFairGNN):\n",
    "            preds = model(data.x, data.edge_index)\n",
    "            loss = model.total_loss(preds[data.idx_train], data.y[data.idx_train], data.sensitive_attr[data.idx_train], tau)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        elif isinstance(model, MYPlainGNN):\n",
    "            preds = model(data.x, data.edge_index)\n",
    "            loss = quantile_loss(data.y[data.idx_train], preds[data.idx_train], tau)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        elif isinstance(model, FairGNN_Q):\n",
    "            model.optimize(data, tau)\n",
    "            loss = model.G_loss \n",
    "        elif isinstance(model, FMP_Q):\n",
    "            preds = model(data)\n",
    "            loss = quantile_loss(data.y[data.idx_train], preds[data.idx_train], tau)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        elif isinstance(model, DAFGNN_2):\n",
    "            preds, mean_gap, var_gap, range_gap = model(data)\n",
    "            loss = daf_loss(data.y[data.idx_train], preds[data.idx_train], tau, mean_gap, var_gap, range_gap, lambda_fair)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if epoch % 100 == 0 or epoch == 1:\n",
    "            print(f\"[Epoch {epoch}] Loss: {loss.item():.4f}\")\n",
    "            \n",
    "    return model\n",
    "\n",
    "def train_one_epoch(model, data, optimizer, epoch, tau=0.9):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if isinstance(model, DAFGNN):\n",
    "        preds, hidden = model(data.x, data.edge_index, return_hidden=True)\n",
    "        loss = model.total_loss(preds[data.idx_train], hidden[data.idx_train], data.sensitive_attr[data.idx_train], data.y[data.idx_train], epoch, tau)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    elif isinstance(model, MYFairGNN):\n",
    "        preds = model(data.x, data.edge_index)\n",
    "        loss = model.total_loss(preds[data.idx_train], data.y[data.idx_train], data.sensitive_attr[data.idx_train], tau)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    elif isinstance(model, MYPlainGNN):\n",
    "        preds = model(data.x, data.edge_index)\n",
    "        loss = quantile_loss(data.y[data.idx_train], preds[data.idx_train], tau)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    elif isinstance(model, FairGNN_Q):\n",
    "        model.optimize(data, tau)\n",
    "        loss = model.G_loss \n",
    "    elif isinstance(model, FMP_Q):\n",
    "        preds = model(data)\n",
    "        loss = quantile_loss(data.y[data.idx_train], preds[data.idx_train], tau)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    elif isinstance(model, DAFGNN_2):\n",
    "        preds, mean_gap, var_gap, range_gap = model(data)\n",
    "        loss = daf_loss(data.y[data.idx_train], preds[data.idx_train], tau, mean_gap, var_gap, range_gap, lambda_fair)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "    return loss.item()\n",
    "\n",
    "def run_experiment(model_class, data, device, tau=0.9, lambda_fair=1.0, hidden_dim=64, n_epochs=500, lr=0.01, weight=5e-4):\n",
    "    print(device)\n",
    "    in_dim = data.x.size(1)\n",
    "    \n",
    "    if model_class.__name__ == 'DAFGNN':\n",
    "        model = model_class(in_dim, hidden_dim, device, lambda_fair).to(device)\n",
    "    elif model_class.__name__ == 'MYFairGNN':\n",
    "        model = model_class(in_dim, hidden_dim, lambda_fair).to(device)\n",
    "    elif model_class.__name__ == 'MYPlainGNN':\n",
    "        model = model_class(in_dim, hidden_dim).to(device)\n",
    "    elif model_class.__name__ == 'FairGNN_Q':\n",
    "        model = model_class(in_dim, hidden_dim).to(device)\n",
    "    elif model_class.__name__ == 'FMP_Q':\n",
    "        model = fmp_model(data).to(device)\n",
    "    elif model_class.__name__ == 'DAFGNN_2':\n",
    "        model = daf_loss(data).to(device)\n",
    "\n",
    "    data = data.to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight)\n",
    "\n",
    "    best_val_qr = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    print('-' * 100)\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_one_epoch(model, data, optimizer, epoch, tau=tau)\n",
    "        val_qr, val_fair = evaluate(model, data, data.idx_val, tau=tau)\n",
    "        \n",
    "        if val_qr < best_val_qr:\n",
    "            best_val_qr = val_qr\n",
    "            best_model = model.state_dict()\n",
    "\n",
    "        if epoch % 100 == 0 or epoch == n_epochs:\n",
    "            print(f\"[{model_class.__name__}] Epoch {epoch}: Val QR Loss={val_qr:.4f}, Fairness Gap={val_fair:.4f}\")\n",
    "            \n",
    "    model.load_state_dict(best_model)\n",
    "    test_qr, test_fair = evaluate(model, data, data.idx_test, tau=tau)\n",
    "    return test_qr, test_fair\n",
    "\n",
    "def multi_run_experiment(model_class, data, device, n_runs=5, tau=0.9, lambda_fair=1.0):\n",
    "    qr_losses = []\n",
    "    fair_gaps = []\n",
    "    \n",
    "    print('-' * 100)\n",
    "    for run in range(n_runs):\n",
    "        qr, fair = run_experiment(model_class, data, device, tau, lambda_fair)\n",
    "        qr_losses.append(qr)\n",
    "        fair_gaps.append(fair)\n",
    "        \n",
    "        print(f\"Run {run+1}/{n_runs} completed: QR Loss={qr:.4f}, Fairness Gap={fair:.4f}\")\n",
    "\n",
    "    return np.array(qr_losses), np.array(fair_gaps)\n",
    "\n",
    "def multiple_runs_with_full_metrics(model_class, data, device, n_runs=5, hidden_dim=64, n_epochs=500, tau=0.9, lambda_fair=1.0, reg=True):\n",
    "    all_metrics = []\n",
    "    \n",
    "    print('-' * 100)\n",
    "    for run in range(n_runs):\n",
    "        print(f\"\\n [{model_class.__name__}] --- Run {run+1}/{n_runs} ---\")\n",
    "        model = train_model(model_class, data, device, tau, lambda_fair, hidden_dim, n_epochs)\n",
    "\n",
    "        if reg:\n",
    "            metrics = regression_evaluation_metrics(model, data, data.idx_test, tau)\n",
    "        else:\n",
    "            metrics = evaluate_metrics(model, data, data.idx_test, tau)\n",
    "        all_metrics.append(metrics)\n",
    "        \n",
    "    metrics_mean = {k: np.mean([m[k] for m in all_metrics if m[k] is not None]) for k in all_metrics[0]}\n",
    "    metrics_std = {k: np.std([m[k] for m in all_metrics if m[k] is not None]) for k in all_metrics[0]}\n",
    "\n",
    "    results = {}\n",
    "    for k in metrics_mean:\n",
    "        results[f\"{k} Mean\"] = metrics_mean[k]\n",
    "        results[f\"{k} Std\"] = metrics_std[k]\n",
    "\n",
    "    return results, all_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation 실험\n",
    "def ablation_train(model_class, data, device, hidden_dim=64, n_epochs=500, tau=0.9, lambda_fair=1.0, gamma=1.0, alpha=1.0, lr=0.01,\n",
    "                   use_projection=True, use_fairness=True, use_lambda=True):\n",
    "    \n",
    "    model = model_class(data.x.size(1), hidden_dim, device, lambda_fair=lambda_fair, \n",
    "                        gamma=gamma, alpha=alpha, use_projection=use_projection, use_fairness=use_fairness, use_lambda=use_lambda\n",
    "                        ).to(device)\n",
    "    data = data.to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        preds, hidden = model(data.x, data.edge_index, return_hidden=True)\n",
    "        loss = model.total_loss(preds[data.idx_train], hidden[data.idx_train], data.sensitive_attr[data.idx_train], data.y[data.idx_train], epoch, tau)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 100 == 0 or epoch == 1:\n",
    "            print(f\"[Epoch {epoch}] Loss: {loss.item():.4f}\")\n",
    "    return model\n",
    "\n",
    "def ablation_evaluate(model, data, idx_eval, tau=0.9):\n",
    "    model.eval()\n",
    "    preds = model(data.x, data.edge_index, return_hidden=False)\n",
    "    preds = preds[idx_eval]\n",
    "    y_true = data.y[idx_eval]\n",
    "    sensitive = data.sensitive_attr[idx_eval]\n",
    "    \n",
    "    qr_loss = quantile_loss(y_true, preds, tau).item()\n",
    "    g0 = preds[sensitive == 0]\n",
    "    g1 = preds[sensitive == 1]\n",
    "    fair_gap = abs(g0.mean() - g1.mean()).item()\n",
    "    return qr_loss, fair_gap\n",
    "\n",
    "def run_ablation_experiments(data, device, lambda_fair, n_runs=5):\n",
    "    results = []\n",
    "    \n",
    "    settings = [\n",
    "        (\"Full\", True, True, True),\n",
    "        (\"w/o Projection\", False, True, True),\n",
    "        (\"w/o Fairness\", True, False, True),\n",
    "        (\"w/o Lambda\", True, True, False),\n",
    "        (\"w/o Projection, Fairness\", False, False, True),\n",
    "        (\"w/o Projection, Lambda\", False, True, False),\n",
    "        (\"w/o Fairness, Lambda\", True, False, False),\n",
    "        (\"w/o All\", False, False, False),\n",
    "    ]\n",
    "\n",
    "\n",
    "    for setting_name, use_proj, use_fair, use_lmbd in settings:\n",
    "        qr_losses = []\n",
    "        fair_gaps = []\n",
    "\n",
    "        for run in range(n_runs):\n",
    "            print(f\"\\n[{setting_name}] Run {run+1}/{n_runs}\")\n",
    "            \n",
    "            model = ablation_train(DAFGNN, data, device, lambda_fair=lambda_fair,\n",
    "                                   use_projection=use_proj, use_fairness=use_fair, use_lambda=use_lmbd)\n",
    "            qr, fair = ablation_evaluate(model, data, data.idx_test)\n",
    "            qr_losses.append(qr)\n",
    "            fair_gaps.append(fair)\n",
    "\n",
    "        results.append({\n",
    "            \"Setting\": setting_name,\n",
    "            \"QR Loss Mean\": np.mean(qr_losses),\n",
    "            \"QR Loss Std\": np.std(qr_losses),\n",
    "            \"Fairness Gap Mean\": np.mean(fair_gaps),\n",
    "            \"Fairness Gap Std\": np.std(fair_gaps)\n",
    "        })\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 선택\n",
    "dataset = 'region_job' # Pokec_z \n",
    "# dataset = 'region_job_2' # Pokec_n\n",
    "# dataset = 'nba' # NBA\n",
    "# dataset = 'german' # German\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋 확인\n",
    "\n",
    "# completion_percentage: 프로필을 얼마나 채웠는가 (%). 자발적 행동의 간접 지표.\n",
    "pokec_predict_attr = 'completion_percentage'\n",
    "pokec_sens_attr = 'region' # gender\n",
    "pokec_label_number, pokec_sens_number, pokec_test_idx, pokec_path = 500, 200, False, \"./dataset/pokec\"\n",
    "\n",
    "# PIE (Player Impact Estimate): 통합 퍼포먼스 지표, 퍼포먼스에 대한 차별 여부 확인 가능\n",
    "# MPG (Minutes Per Game): 출전 시간 → 팀의 코치 결정, 제도적 편향 가능성\n",
    "nba_predict_attr = 'PIE'\n",
    "nba_sens_attr = 'country'  # AGE, palyer_height, player_weight\n",
    "nba_label_number, nba_sens_number, nba_test_idx, nba_path = 100, 50, True, \"./dataset/NBA\"\n",
    "\n",
    "# German\n",
    "german_predict_attr = 'LoanAmount' # LoanAmount(대출 금액), LoanRateAsPercentOfIncome(소득대비상환비율), YearsAtCurrentHome(거주연수)\n",
    "german_sens_attr = 'Gender' # Gender, ForeignWorker, Single(독신여부), HasTelephone(전화기보유여부), OwnsHouse(주택소유여부), Unemployed(실직상태여부) Age(고령자로 나눠서 가능) 등등등..\n",
    "german_path = './dataset/NIFTY'\n",
    "\n",
    "\n",
    "if dataset == 'german':\n",
    "    df, col, group = pd.read_csv(f'{german_path}/{dataset}.csv'), german_predict_attr, german_sens_attr\n",
    "elif dataset == 'nba':\n",
    "    df, col, group = pd.read_csv(f'{nba_path}/{dataset}.csv'), nba_predict_attr, nba_sens_attr\n",
    "else:\n",
    "    df, col, group = pd.read_csv(f'{pokec_path}/{dataset}.csv'), pokec_predict_attr, pokec_sens_attr\n",
    "\n",
    "if dataset == 'nba':\n",
    "    dn = 'NBA'\n",
    "elif dataset == 'german':\n",
    "    dn = 'German'\n",
    "else:\n",
    "    dn = 'Pokec_z' if dataset == 'region_job' else 'Pokec_n'\n",
    "\n",
    "# print(df.columns)\n",
    "# print(df.describe())\n",
    "\n",
    "# print(df[col].describe())\n",
    "# print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 분포 시각화\n",
    "\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(12, 3))\n",
    "# sns.kdeplot(data=df, x=col, ax=axes[0], fill=True, color='darkorchid')\n",
    "# axes[0].set_title(f\"KDE of '{col}' of {dn} dataset\", size=11)\n",
    "# axes[0].set_xlabel(col, size=10)\n",
    "# axes[0].set_ylabel('Density', size=10)\n",
    "# axes[0].grid(True)\n",
    "# sns.kdeplot(data=df, x=col, hue=group, ax=axes[1], fill=True, common_norm=False, alpha=0.5)\n",
    "# axes[1].set_title(f\"KDE of '{col}' by {group} of {dn} dataset\", size=11)\n",
    "# axes[1].set_xlabel(col, size=10)\n",
    "# axes[1].set_ylabel('Density', size=10)\n",
    "# axes[1].grid(True)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.kdeplot(data=df, x=col, fill=True, color='darkorchid', alpha=0.3, linewidth=1, label='Overall')\n",
    "for value in sorted(df[group].unique()):\n",
    "    subset = df[df[group] == value]\n",
    "    sns.kdeplot(data=subset, x=col, fill=True, alpha=0.4, label=f\"{group} = {value}\")\n",
    "\n",
    "plt.title(f\"KDE of '{col}' from {dn} dataset\", size=11)\n",
    "plt.xlabel(col, size=10)\n",
    "plt.ylabel('Density', size=10)\n",
    "plt.legend(title=group)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading region_job dataset from ./dataset/pokec\n",
      "[INFO] 유효하지 않은 user_id로 인해 제거된 edge 수: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12904\\2187334796.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  edge_index = torch.tensor([adj_coo.row, adj_coo.col], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 로드\n",
    "seed = 1127\n",
    "tau = 0.9\n",
    "\n",
    "if dataset == 'nba':\n",
    "    predict_attr, sens_attr, label_number, sens_number, test_idx, path = nba_predict_attr, nba_sens_attr, nba_label_number, nba_sens_number, nba_test_idx, nba_path\n",
    "elif dataset == 'german':\n",
    "    predict_attr, sens_attr, path = german_predict_attr, german_sens_attr, german_path\n",
    "else:\n",
    "    predict_attr, sens_attr, label_number, sens_number, test_idx, path = pokec_predict_attr, pokec_sens_attr, pokec_label_number, pokec_sens_number, pokec_test_idx, pokec_path\n",
    "\n",
    "adj, features, labels, idx_train, idx_val, idx_test, sens, idx_sens_train = load_dataset_unified(dataset, sens_attr, predict_attr, path, label_number, sens_number, test_idx, seed=seed)\n",
    "\n",
    "if dataset == 'nba':\n",
    "    features = feature_norm(features)\n",
    "\n",
    "if isinstance(adj, torch.Tensor):  # torch.sparse_coo_tensor\n",
    "    src, dst = adj._indices()\n",
    "    g = dgl.graph((src, dst))\n",
    "    edge_index = adj._indices()\n",
    "else:  # scipy sparse matrix\n",
    "    g = dgl.from_scipy(adj)\n",
    "    adj_coo = adj.tocoo()\n",
    "    edge_index = torch.tensor([adj_coo.row, adj_coo.col], dtype=torch.long)\n",
    "\n",
    "# 3. PyG-style 데이터 객체\n",
    "data = Data(x=features, edge_index=edge_index, y=labels.float(), sensitive_attr=sens, quantile_tau=torch.full((features.shape[0],), tau))\n",
    "data.idx_train = idx_train\n",
    "data.idx_val = idx_val\n",
    "data.idx_test = idx_test\n",
    "data.idx_sens_train = idx_sens_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실험"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실험 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 실험 설정\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "print('Device:', device)\n",
    "\n",
    "tau = 0.9\n",
    "lambda_fair = 1.0\n",
    "hidden_dim = 64\n",
    "n_epochs = 500\n",
    "n_runs = 5\n",
    "lr = 0.01\n",
    "weight = 5e-4\n",
    "\n",
    "# data = data.to(device)\n",
    "# data.sensitive_attr = data.sensitive_attr.to(device)\n",
    "\n",
    "# DGL 그래프는 GPU로 올리지 않음\n",
    "# g = g.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 간단 비교 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading region_job dataset from ./dataset/pokec\n",
      "[INFO] 유효하지 않은 user_id로 인해 제거된 edge 수: 0\n",
      "cuda:0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[FairGNN_Q] Epoch 100: Val QR Loss=10.6781, Fairness Gap=3.0136\n",
      "[FairGNN_Q] Epoch 200: Val QR Loss=9.7702, Fairness Gap=2.2615\n",
      "[FairGNN_Q] Epoch 300: Val QR Loss=9.6836, Fairness Gap=2.0447\n",
      "[FairGNN_Q] Epoch 400: Val QR Loss=9.5166, Fairness Gap=1.4875\n",
      "[FairGNN_Q] Epoch 500: Val QR Loss=8.7837, Fairness Gap=1.4418\n",
      "cuda:0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[FMP_Q] Epoch 100: Val QR Loss=3.7564, Fairness Gap=3.4993\n",
      "[FMP_Q] Epoch 200: Val QR Loss=1.3917, Fairness Gap=2.3070\n",
      "[FMP_Q] Epoch 300: Val QR Loss=0.9408, Fairness Gap=1.9572\n",
      "[FMP_Q] Epoch 400: Val QR Loss=0.9054, Fairness Gap=1.8058\n",
      "[FMP_Q] Epoch 500: Val QR Loss=0.9205, Fairness Gap=1.6989\n",
      "cuda:0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[MYPlainGNN] Epoch 100: Val QR Loss=4.2641, Fairness Gap=3.5565\n",
      "[MYPlainGNN] Epoch 200: Val QR Loss=4.6408, Fairness Gap=2.2113\n",
      "[MYPlainGNN] Epoch 300: Val QR Loss=4.5776, Fairness Gap=1.8656\n",
      "[MYPlainGNN] Epoch 400: Val QR Loss=4.4689, Fairness Gap=1.5798\n",
      "[MYPlainGNN] Epoch 500: Val QR Loss=4.7023, Fairness Gap=1.1371\n",
      "cuda:0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[MYFairGNN] Epoch 100: Val QR Loss=4.4511, Fairness Gap=1.9805\n",
      "[MYFairGNN] Epoch 200: Val QR Loss=4.1003, Fairness Gap=0.7988\n",
      "[MYFairGNN] Epoch 300: Val QR Loss=3.9111, Fairness Gap=0.0383\n",
      "[MYFairGNN] Epoch 400: Val QR Loss=3.7462, Fairness Gap=0.0026\n",
      "[MYFairGNN] Epoch 500: Val QR Loss=3.6495, Fairness Gap=0.2466\n",
      "cuda:0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[DAFGNN] Epoch 100: Val QR Loss=4.7802, Fairness Gap=2.4515\n",
      "[DAFGNN] Epoch 200: Val QR Loss=4.6024, Fairness Gap=2.0548\n",
      "[DAFGNN] Epoch 300: Val QR Loss=4.5835, Fairness Gap=1.8069\n",
      "[DAFGNN] Epoch 400: Val QR Loss=4.0932, Fairness Gap=1.8862\n",
      "[DAFGNN] Epoch 500: Val QR Loss=4.1269, Fairness Gap=1.5162\n",
      "Loading region_job_2 dataset from ./dataset/pokec\n",
      "[INFO] 유효하지 않은 user_id로 인해 제거된 edge 수: 0\n",
      "cuda:0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[FairGNN_Q] Epoch 100: Val QR Loss=7.9475, Fairness Gap=6.5185\n",
      "[FairGNN_Q] Epoch 200: Val QR Loss=8.8899, Fairness Gap=8.0241\n",
      "[FairGNN_Q] Epoch 300: Val QR Loss=8.9268, Fairness Gap=6.8904\n",
      "[FairGNN_Q] Epoch 400: Val QR Loss=9.1001, Fairness Gap=4.4050\n",
      "[FairGNN_Q] Epoch 500: Val QR Loss=8.9209, Fairness Gap=2.7201\n",
      "cuda:0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[FMP_Q] Epoch 100: Val QR Loss=3.5104, Fairness Gap=6.3583\n",
      "[FMP_Q] Epoch 200: Val QR Loss=1.6254, Fairness Gap=4.6426\n",
      "[FMP_Q] Epoch 300: Val QR Loss=1.0733, Fairness Gap=4.0223\n",
      "[FMP_Q] Epoch 400: Val QR Loss=1.1234, Fairness Gap=3.6369\n",
      "[FMP_Q] Epoch 500: Val QR Loss=0.9487, Fairness Gap=3.7311\n",
      "cuda:0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[MYPlainGNN] Epoch 100: Val QR Loss=4.0981, Fairness Gap=6.7517\n",
      "[MYPlainGNN] Epoch 200: Val QR Loss=4.2275, Fairness Gap=4.2232\n",
      "[MYPlainGNN] Epoch 300: Val QR Loss=4.9862, Fairness Gap=3.3703\n",
      "[MYPlainGNN] Epoch 400: Val QR Loss=5.1913, Fairness Gap=3.1549\n",
      "[MYPlainGNN] Epoch 500: Val QR Loss=5.0365, Fairness Gap=3.3506\n",
      "cuda:0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[MYFairGNN] Epoch 100: Val QR Loss=4.3334, Fairness Gap=0.9299\n",
      "[MYFairGNN] Epoch 200: Val QR Loss=4.0854, Fairness Gap=0.7496\n",
      "[MYFairGNN] Epoch 300: Val QR Loss=3.9268, Fairness Gap=0.4934\n",
      "[MYFairGNN] Epoch 400: Val QR Loss=3.8106, Fairness Gap=1.0820\n",
      "[MYFairGNN] Epoch 500: Val QR Loss=3.4714, Fairness Gap=0.6862\n",
      "cuda:0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[DAFGNN] Epoch 100: Val QR Loss=5.2838, Fairness Gap=3.2883\n",
      "[DAFGNN] Epoch 200: Val QR Loss=5.3702, Fairness Gap=0.3789\n",
      "[DAFGNN] Epoch 300: Val QR Loss=4.2579, Fairness Gap=0.2198\n",
      "[DAFGNN] Epoch 400: Val QR Loss=3.6126, Fairness Gap=0.4734\n",
      "[DAFGNN] Epoch 500: Val QR Loss=3.2297, Fairness Gap=0.5784\n",
      "Loading nba dataset from ./dataset/NBA\n",
      "[INFO] 유효하지 않은 user_id로 인해 제거된 edge 수: 1641\n",
      "cuda:0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[FairGNN_Q] Epoch 100: Val QR Loss=0.9900, Fairness Gap=0.2258\n",
      "[FairGNN_Q] Epoch 200: Val QR Loss=0.9873, Fairness Gap=0.3479\n",
      "[FairGNN_Q] Epoch 300: Val QR Loss=0.9912, Fairness Gap=0.1737\n",
      "[FairGNN_Q] Epoch 400: Val QR Loss=0.8760, Fairness Gap=0.0583\n",
      "[FairGNN_Q] Epoch 500: Val QR Loss=1.0720, Fairness Gap=0.1317\n",
      "cuda:0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[FMP_Q] Epoch 100: Val QR Loss=0.5439, Fairness Gap=0.1453\n",
      "[FMP_Q] Epoch 200: Val QR Loss=0.3578, Fairness Gap=0.3364\n",
      "[FMP_Q] Epoch 300: Val QR Loss=0.5054, Fairness Gap=0.1605\n",
      "[FMP_Q] Epoch 400: Val QR Loss=0.3755, Fairness Gap=0.1138\n",
      "[FMP_Q] Epoch 500: Val QR Loss=2.3384, Fairness Gap=0.0743\n",
      "cuda:0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[MYPlainGNN] Epoch 100: Val QR Loss=0.6873, Fairness Gap=0.3694\n",
      "[MYPlainGNN] Epoch 200: Val QR Loss=0.5741, Fairness Gap=0.1261\n",
      "[MYPlainGNN] Epoch 300: Val QR Loss=0.5419, Fairness Gap=0.0941\n",
      "[MYPlainGNN] Epoch 400: Val QR Loss=0.5679, Fairness Gap=0.0065\n",
      "[MYPlainGNN] Epoch 500: Val QR Loss=0.5725, Fairness Gap=0.2826\n",
      "cuda:0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[MYFairGNN] Epoch 100: Val QR Loss=0.7329, Fairness Gap=0.3845\n",
      "[MYFairGNN] Epoch 200: Val QR Loss=0.7403, Fairness Gap=0.2957\n",
      "[MYFairGNN] Epoch 300: Val QR Loss=0.6259, Fairness Gap=0.1572\n",
      "[MYFairGNN] Epoch 400: Val QR Loss=0.6099, Fairness Gap=0.1063\n",
      "[MYFairGNN] Epoch 500: Val QR Loss=0.5557, Fairness Gap=0.0537\n",
      "cuda:0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[DAFGNN] Epoch 100: Val QR Loss=0.7165, Fairness Gap=0.3742\n",
      "[DAFGNN] Epoch 200: Val QR Loss=0.6874, Fairness Gap=0.1105\n",
      "[DAFGNN] Epoch 300: Val QR Loss=0.6793, Fairness Gap=0.0580\n",
      "[DAFGNN] Epoch 400: Val QR Loss=0.6781, Fairness Gap=0.0469\n",
      "[DAFGNN] Epoch 500: Val QR Loss=0.6759, Fairness Gap=0.0536\n",
      "cuda:0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[FairGNN_Q] Epoch 100: Val QR Loss=6.0852, Fairness Gap=8.0202\n",
      "[FairGNN_Q] Epoch 200: Val QR Loss=365.8364, Fairness Gap=52.4543\n",
      "[FairGNN_Q] Epoch 300: Val QR Loss=14.2543, Fairness Gap=18.5766\n",
      "[FairGNN_Q] Epoch 400: Val QR Loss=23.7088, Fairness Gap=30.7992\n",
      "[FairGNN_Q] Epoch 500: Val QR Loss=13.4147, Fairness Gap=17.4714\n",
      "cuda:0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[FMP_Q] Epoch 100: Val QR Loss=0.1389, Fairness Gap=0.1990\n",
      "[FMP_Q] Epoch 200: Val QR Loss=0.1389, Fairness Gap=0.1998\n",
      "[FMP_Q] Epoch 300: Val QR Loss=0.1314, Fairness Gap=0.0130\n",
      "[FMP_Q] Epoch 400: Val QR Loss=0.0911, Fairness Gap=0.1366\n",
      "[FMP_Q] Epoch 500: Val QR Loss=0.0515, Fairness Gap=0.0394\n",
      "cuda:0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[MYPlainGNN] Epoch 100: Val QR Loss=0.2351, Fairness Gap=0.4145\n",
      "[MYPlainGNN] Epoch 200: Val QR Loss=0.0653, Fairness Gap=0.1025\n",
      "[MYPlainGNN] Epoch 300: Val QR Loss=0.1769, Fairness Gap=0.2424\n",
      "[MYPlainGNN] Epoch 400: Val QR Loss=0.1683, Fairness Gap=0.0162\n",
      "[MYPlainGNN] Epoch 500: Val QR Loss=0.1216, Fairness Gap=0.1575\n",
      "cuda:0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[MYFairGNN] Epoch 100: Val QR Loss=0.2365, Fairness Gap=0.2845\n",
      "[MYFairGNN] Epoch 200: Val QR Loss=0.1063, Fairness Gap=0.0502\n",
      "[MYFairGNN] Epoch 300: Val QR Loss=0.0670, Fairness Gap=0.0016\n",
      "[MYFairGNN] Epoch 400: Val QR Loss=0.1050, Fairness Gap=0.0593\n",
      "[MYFairGNN] Epoch 500: Val QR Loss=0.0395, Fairness Gap=0.0289\n",
      "cuda:0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[DAFGNN] Epoch 100: Val QR Loss=0.0645, Fairness Gap=0.0026\n",
      "[DAFGNN] Epoch 200: Val QR Loss=0.0574, Fairness Gap=0.0607\n",
      "[DAFGNN] Epoch 300: Val QR Loss=0.1016, Fairness Gap=0.1086\n",
      "[DAFGNN] Epoch 400: Val QR Loss=0.2024, Fairness Gap=0.1084\n",
      "[DAFGNN] Epoch 500: Val QR Loss=0.1136, Fairness Gap=0.0691\n"
     ]
    }
   ],
   "source": [
    "# 전체 모델에 대해서 수행\n",
    "all_dataset_results = {}\n",
    "\n",
    "for dataset in ['region_job', 'region_job_2', 'nba', 'german']:\n",
    "    seed = 1127\n",
    "    if dataset == 'nba':\n",
    "        predict_attr, sens_attr, label_number, sens_number, test_idx, path = nba_predict_attr, nba_sens_attr, nba_label_number, nba_sens_number, nba_test_idx, nba_path\n",
    "    elif dataset == 'german':\n",
    "        predict_attr, sens_attr, path = german_predict_attr, german_sens_attr, german_path\n",
    "    else:\n",
    "        predict_attr, sens_attr, label_number, sens_number, test_idx, path = pokec_predict_attr, pokec_sens_attr, pokec_label_number, pokec_sens_number, pokec_test_idx, pokec_path\n",
    "\n",
    "    adj, features, labels, idx_train, idx_val, idx_test, sens, idx_sens_train = load_dataset_unified(dataset, sens_attr, predict_attr, path, label_number, sens_number, test_idx, seed=seed)\n",
    "\n",
    "    if dataset == 'nba':\n",
    "        features = feature_norm(features)\n",
    "\n",
    "    if isinstance(adj, torch.Tensor):  # torch.sparse_coo_tensor\n",
    "        src, dst = adj._indices()\n",
    "        g = dgl.graph((src, dst))\n",
    "        edge_index = adj._indices()\n",
    "    else:  # scipy sparse matrix\n",
    "        g = dgl.from_scipy(adj)\n",
    "        adj_coo = adj.tocoo()\n",
    "        edge_index = torch.tensor([adj_coo.row, adj_coo.col], dtype=torch.long)\n",
    "\n",
    "    data = Data(x=features, edge_index=edge_index, y=labels.float(), sensitive_attr=sens, quantile_tau=torch.full((features.shape[0],), tau))\n",
    "    data.idx_train = idx_train\n",
    "    data.idx_val = idx_val\n",
    "    data.idx_test = idx_test\n",
    "    data.idx_sens_train = idx_sens_train\n",
    "\n",
    "    results = []\n",
    "\n",
    "    fair_qr, fair_fair = run_experiment(FairGNN_Q, data, device, tau, lambda_fair, hidden_dim, n_epochs, lr, weight)\n",
    "    results.append({'Model': 'FairGNN', 'QR Loss': fair_qr, 'Fairness Gap': fair_fair,})\n",
    "\n",
    "    fmp_qr, fmp_fair = run_experiment(FMP_Q, data, device, tau, lambda_fair, hidden_dim, n_epochs, lr, weight)\n",
    "    results.append({'Model': 'FMP', 'QR Loss': fmp_qr, 'Fairness Gap': fmp_fair,})\n",
    "\n",
    "    plain_qr, plain_fair = run_experiment(MYPlainGNN, data, device, tau, lambda_fair, hidden_dim, n_epochs, lr, weight)\n",
    "    results.append({'Model': 'B-GNN', 'QR Loss': plain_qr, 'Fairness Gap': plain_fair,})\n",
    "\n",
    "    myfair_qr, myfair_fair = run_experiment(MYFairGNN, data, device, tau, lambda_fair, hidden_dim, n_epochs, lr, weight)\n",
    "    results.append({'Model': 'F-GNN', 'QR Loss': myfair_qr, 'Fairness Gap': myfair_fair,})\n",
    "\n",
    "    dist_qr, dist_fair = run_experiment(DAFGNN, data, device, tau, lambda_fair, hidden_dim, n_epochs, lr, weight)\n",
    "    results.append({'Model': 'DAF-GNN', 'QR Loss': dist_qr, 'Fairness Gap': dist_fair,})\n",
    "\n",
    "    # 결과 입력\n",
    "    all_dataset_results[dataset] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region_job\n",
      "     Model  QR Loss  Fairness Gap\n",
      "0  FairGNN     8.82          1.68\n",
      "1      FMP     0.94          1.88\n",
      "2    B-GNN     4.69          1.31\n",
      "3    F-GNN     3.64          0.26\n",
      "4  DAF-GNN     4.16          1.58\n",
      "----------------------------------------------------------------------------------------------------\n",
      "region_job_2\n",
      "     Model  QR Loss  Fairness Gap\n",
      "0  FairGNN     8.90          2.82\n",
      "1      FMP     0.97          3.57\n",
      "2    B-GNN     5.11          3.23\n",
      "3    F-GNN     3.49          0.54\n",
      "4  DAF-GNN     3.21          0.48\n",
      "----------------------------------------------------------------------------------------------------\n",
      "nba\n",
      "     Model  QR Loss  Fairness Gap\n",
      "0  FairGNN     1.07          0.13\n",
      "1      FMP     2.34          0.07\n",
      "2    B-GNN     0.57          0.28\n",
      "3    F-GNN     0.56          0.05\n",
      "4  DAF-GNN     0.68          0.05\n",
      "----------------------------------------------------------------------------------------------------\n",
      "german\n",
      "     Model  QR Loss  Fairness Gap\n",
      "0  FairGNN    14.32         18.23\n",
      "1      FMP     0.05          0.05\n",
      "2    B-GNN     0.13          0.17\n",
      "3    F-GNN     0.04          0.02\n",
      "4  DAF-GNN     0.12          0.08\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 실험 결과 출력\n",
    "for dataset in ['region_job', 'region_job_2', 'nba', 'german']:\n",
    "    df = pd.DataFrame(all_dataset_results[dataset]).round(2)\n",
    "    print(dataset)\n",
    "    print(df)\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 상세 비교 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading region_job dataset from ./dataset/pokec\n",
      "[INFO] 유효하지 않은 user_id로 인해 제거된 edge 수: 0\n",
      "sens 0 count: 43962\n",
      "sens 1 count: 23834\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [FairGNN_Q] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.3512\n",
      "[Epoch 100] Loss: 11.1862\n",
      "[Epoch 200] Loss: 9.6813\n",
      "[Epoch 300] Loss: 8.0757\n",
      "[Epoch 400] Loss: 7.3618\n",
      "[Epoch 500] Loss: 6.9662\n",
      "\n",
      " [FairGNN_Q] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.6733\n",
      "[Epoch 100] Loss: 10.1462\n",
      "[Epoch 200] Loss: 10.6282\n",
      "[Epoch 300] Loss: 9.9922\n",
      "[Epoch 400] Loss: 10.2252\n",
      "[Epoch 500] Loss: 8.8750\n",
      "\n",
      " [FairGNN_Q] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.6660\n",
      "[Epoch 100] Loss: 9.2867\n",
      "[Epoch 200] Loss: 8.9342\n",
      "[Epoch 300] Loss: 8.4234\n",
      "[Epoch 400] Loss: 8.0159\n",
      "[Epoch 500] Loss: 7.4976\n",
      "\n",
      " [FairGNN_Q] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.9543\n",
      "[Epoch 100] Loss: 13.1066\n",
      "[Epoch 200] Loss: 10.4655\n",
      "[Epoch 300] Loss: 9.3968\n",
      "[Epoch 400] Loss: 8.4165\n",
      "[Epoch 500] Loss: 8.0809\n",
      "\n",
      " [FairGNN_Q] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.8638\n",
      "[Epoch 100] Loss: 12.5663\n",
      "[Epoch 200] Loss: 9.9474\n",
      "[Epoch 300] Loss: 9.6492\n",
      "[Epoch 400] Loss: 8.5242\n",
      "[Epoch 500] Loss: 8.1752\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [FMP_Q] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.6136\n",
      "[Epoch 100] Loss: 3.9286\n",
      "[Epoch 200] Loss: 1.0816\n",
      "[Epoch 300] Loss: 0.8441\n",
      "[Epoch 400] Loss: 0.7466\n",
      "[Epoch 500] Loss: 0.6567\n",
      "\n",
      " [FMP_Q] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.6836\n",
      "[Epoch 100] Loss: 7.4383\n",
      "[Epoch 200] Loss: 1.6806\n",
      "[Epoch 300] Loss: 0.9546\n",
      "[Epoch 400] Loss: 0.8297\n",
      "[Epoch 500] Loss: 0.9353\n",
      "\n",
      " [FMP_Q] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.6617\n",
      "[Epoch 100] Loss: 3.4449\n",
      "[Epoch 200] Loss: 1.0380\n",
      "[Epoch 300] Loss: 0.8579\n",
      "[Epoch 400] Loss: 0.7407\n",
      "[Epoch 500] Loss: 0.5890\n",
      "\n",
      " [FMP_Q] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.7313\n",
      "[Epoch 100] Loss: 4.7761\n",
      "[Epoch 200] Loss: 1.2689\n",
      "[Epoch 300] Loss: 1.4244\n",
      "[Epoch 400] Loss: 0.7461\n",
      "[Epoch 500] Loss: 0.8666\n",
      "\n",
      " [FMP_Q] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.7237\n",
      "[Epoch 100] Loss: 4.1881\n",
      "[Epoch 200] Loss: 1.1475\n",
      "[Epoch 300] Loss: 0.8241\n",
      "[Epoch 400] Loss: 0.9435\n",
      "[Epoch 500] Loss: 1.9944\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [MYPlainGNN] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.8054\n",
      "[Epoch 100] Loss: 8.1909\n",
      "[Epoch 200] Loss: 6.2665\n",
      "[Epoch 300] Loss: 5.4079\n",
      "[Epoch 400] Loss: 5.4363\n",
      "[Epoch 500] Loss: 6.6372\n",
      "\n",
      " [MYPlainGNN] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.5346\n",
      "[Epoch 100] Loss: 7.8968\n",
      "[Epoch 200] Loss: 6.1390\n",
      "[Epoch 300] Loss: 5.2850\n",
      "[Epoch 400] Loss: 4.7600\n",
      "[Epoch 500] Loss: 4.4143\n",
      "\n",
      " [MYPlainGNN] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.4395\n",
      "[Epoch 100] Loss: 7.8415\n",
      "[Epoch 200] Loss: 6.0780\n",
      "[Epoch 300] Loss: 5.2634\n",
      "[Epoch 400] Loss: 4.7289\n",
      "[Epoch 500] Loss: 4.7261\n",
      "\n",
      " [MYPlainGNN] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.5065\n",
      "[Epoch 100] Loss: 7.7782\n",
      "[Epoch 200] Loss: 6.1428\n",
      "[Epoch 300] Loss: 5.1542\n",
      "[Epoch 400] Loss: 4.8229\n",
      "[Epoch 500] Loss: 4.4320\n",
      "\n",
      " [MYPlainGNN] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.5459\n",
      "[Epoch 100] Loss: 7.7712\n",
      "[Epoch 200] Loss: 6.1780\n",
      "[Epoch 300] Loss: 5.3794\n",
      "[Epoch 400] Loss: 5.3091\n",
      "[Epoch 500] Loss: 4.8113\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [MYFairGNN] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.8680\n",
      "[Epoch 100] Loss: 8.3160\n",
      "[Epoch 200] Loss: 6.7413\n",
      "[Epoch 300] Loss: 6.0004\n",
      "[Epoch 400] Loss: 5.6439\n",
      "[Epoch 500] Loss: 5.0050\n",
      "\n",
      " [MYFairGNN] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.8479\n",
      "[Epoch 100] Loss: 8.5211\n",
      "[Epoch 200] Loss: 6.8422\n",
      "[Epoch 300] Loss: 5.6971\n",
      "[Epoch 400] Loss: 5.4810\n",
      "[Epoch 500] Loss: 5.4211\n",
      "\n",
      " [MYFairGNN] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.9391\n",
      "[Epoch 100] Loss: 8.3467\n",
      "[Epoch 200] Loss: 6.5914\n",
      "[Epoch 300] Loss: 6.0212\n",
      "[Epoch 400] Loss: 5.4515\n",
      "[Epoch 500] Loss: 5.4613\n",
      "\n",
      " [MYFairGNN] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.5349\n",
      "[Epoch 100] Loss: 8.1101\n",
      "[Epoch 200] Loss: 6.6172\n",
      "[Epoch 300] Loss: 5.6651\n",
      "[Epoch 400] Loss: 5.3777\n",
      "[Epoch 500] Loss: 4.9870\n",
      "\n",
      " [MYFairGNN] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.7211\n",
      "[Epoch 100] Loss: 8.3464\n",
      "[Epoch 200] Loss: 6.6673\n",
      "[Epoch 300] Loss: 6.5649\n",
      "[Epoch 400] Loss: 5.5974\n",
      "[Epoch 500] Loss: 5.4727\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [DAFGNN_2] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.8352\n",
      "[Epoch 100] Loss: 7.7001\n",
      "[Epoch 200] Loss: 4.1455\n",
      "[Epoch 300] Loss: 3.5271\n",
      "[Epoch 400] Loss: 4.4301\n",
      "[Epoch 500] Loss: 2.5488\n",
      "\n",
      " [DAFGNN_2] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.4745\n",
      "[Epoch 100] Loss: 8.2598\n",
      "[Epoch 200] Loss: 5.4264\n",
      "[Epoch 300] Loss: 3.9750\n",
      "[Epoch 400] Loss: 3.7960\n",
      "[Epoch 500] Loss: 2.6895\n",
      "\n",
      " [DAFGNN_2] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.6054\n",
      "[Epoch 100] Loss: 9.3851\n",
      "[Epoch 200] Loss: 7.4318\n",
      "[Epoch 300] Loss: 4.5156\n",
      "[Epoch 400] Loss: 3.3394\n",
      "[Epoch 500] Loss: 3.8352\n",
      "\n",
      " [DAFGNN_2] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.5372\n",
      "[Epoch 100] Loss: 6.8452\n",
      "[Epoch 200] Loss: 4.9929\n",
      "[Epoch 300] Loss: 4.7141\n",
      "[Epoch 400] Loss: 4.4581\n",
      "[Epoch 500] Loss: 4.8036\n",
      "\n",
      " [DAFGNN_2] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.5742\n",
      "[Epoch 100] Loss: 9.3037\n",
      "[Epoch 200] Loss: 7.8982\n",
      "[Epoch 300] Loss: 6.5778\n",
      "[Epoch 400] Loss: 8.3045\n",
      "[Epoch 500] Loss: 6.9250\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [DAFGNN] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.6901\n",
      "[Epoch 100] Loss: 10.2191\n",
      "[Epoch 200] Loss: 9.5067\n",
      "[Epoch 300] Loss: 8.9514\n",
      "[Epoch 400] Loss: 8.5635\n",
      "[Epoch 500] Loss: 8.7235\n",
      "\n",
      " [DAFGNN] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.7773\n",
      "[Epoch 100] Loss: 9.8511\n",
      "[Epoch 200] Loss: 9.0899\n",
      "[Epoch 300] Loss: 9.5373\n",
      "[Epoch 400] Loss: 8.7062\n",
      "[Epoch 500] Loss: 9.1640\n",
      "\n",
      " [DAFGNN] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.9794\n",
      "[Epoch 100] Loss: 9.4372\n",
      "[Epoch 200] Loss: 9.6017\n",
      "[Epoch 300] Loss: 9.2079\n",
      "[Epoch 400] Loss: 9.1429\n",
      "[Epoch 500] Loss: 9.0515\n",
      "\n",
      " [DAFGNN] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 22.0108\n",
      "[Epoch 100] Loss: 9.5326\n",
      "[Epoch 200] Loss: 9.3259\n",
      "[Epoch 300] Loss: 9.0176\n",
      "[Epoch 400] Loss: 9.5675\n",
      "[Epoch 500] Loss: 8.5941\n",
      "\n",
      " [DAFGNN] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.9069\n",
      "[Epoch 100] Loss: 9.7396\n",
      "[Epoch 200] Loss: 9.4764\n",
      "[Epoch 300] Loss: 8.8092\n",
      "[Epoch 400] Loss: 9.2373\n",
      "[Epoch 500] Loss: 9.0187\n",
      "Loading region_job_2 dataset from ./dataset/pokec\n",
      "[INFO] 유효하지 않은 user_id로 인해 제거된 edge 수: 0\n",
      "sens 0 count: 47338\n",
      "sens 1 count: 19231\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [FairGNN_Q] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.0460\n",
      "[Epoch 100] Loss: 10.1834\n",
      "[Epoch 200] Loss: 9.0401\n",
      "[Epoch 300] Loss: 7.8365\n",
      "[Epoch 400] Loss: 7.2303\n",
      "[Epoch 500] Loss: 6.9143\n",
      "\n",
      " [FairGNN_Q] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.7972\n",
      "[Epoch 100] Loss: 12.3943\n",
      "[Epoch 200] Loss: 10.5939\n",
      "[Epoch 300] Loss: 9.6536\n",
      "[Epoch 400] Loss: 8.9855\n",
      "[Epoch 500] Loss: 8.3982\n",
      "\n",
      " [FairGNN_Q] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.7618\n",
      "[Epoch 100] Loss: 10.9275\n",
      "[Epoch 200] Loss: 9.6266\n",
      "[Epoch 300] Loss: 9.9682\n",
      "[Epoch 400] Loss: 8.6012\n",
      "[Epoch 500] Loss: 8.3838\n",
      "\n",
      " [FairGNN_Q] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.6473\n",
      "[Epoch 100] Loss: 13.4852\n",
      "[Epoch 200] Loss: 10.2767\n",
      "[Epoch 300] Loss: 9.0017\n",
      "[Epoch 400] Loss: 8.2137\n",
      "[Epoch 500] Loss: 7.7831\n",
      "\n",
      " [FairGNN_Q] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.8975\n",
      "[Epoch 100] Loss: 10.6168\n",
      "[Epoch 200] Loss: 9.3486\n",
      "[Epoch 300] Loss: 8.2637\n",
      "[Epoch 400] Loss: 7.9069\n",
      "[Epoch 500] Loss: 7.3443\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [FMP_Q] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.8121\n",
      "[Epoch 100] Loss: 3.5288\n",
      "[Epoch 200] Loss: 1.1578\n",
      "[Epoch 300] Loss: 0.8371\n",
      "[Epoch 400] Loss: 0.8721\n",
      "[Epoch 500] Loss: 0.6805\n",
      "\n",
      " [FMP_Q] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.7521\n",
      "[Epoch 100] Loss: 4.3666\n",
      "[Epoch 200] Loss: 1.3593\n",
      "[Epoch 300] Loss: 0.9382\n",
      "[Epoch 400] Loss: 0.7654\n",
      "[Epoch 500] Loss: 0.6790\n",
      "\n",
      " [FMP_Q] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.8407\n",
      "[Epoch 100] Loss: 4.1614\n",
      "[Epoch 200] Loss: 1.4096\n",
      "[Epoch 300] Loss: 0.8910\n",
      "[Epoch 400] Loss: 0.7592\n",
      "[Epoch 500] Loss: 0.6891\n",
      "\n",
      " [FMP_Q] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.8506\n",
      "[Epoch 100] Loss: 2.8808\n",
      "[Epoch 200] Loss: 0.9286\n",
      "[Epoch 300] Loss: 0.7150\n",
      "[Epoch 400] Loss: 3.4487\n",
      "[Epoch 500] Loss: 0.6246\n",
      "\n",
      " [FMP_Q] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.6404\n",
      "[Epoch 100] Loss: 6.0079\n",
      "[Epoch 200] Loss: 1.6652\n",
      "[Epoch 300] Loss: 0.9021\n",
      "[Epoch 400] Loss: 0.8480\n",
      "[Epoch 500] Loss: 0.6642\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [MYPlainGNN] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.4399\n",
      "[Epoch 100] Loss: 7.2727\n",
      "[Epoch 200] Loss: 6.0708\n",
      "[Epoch 300] Loss: 5.1269\n",
      "[Epoch 400] Loss: 4.7697\n",
      "[Epoch 500] Loss: 5.4512\n",
      "\n",
      " [MYPlainGNN] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.8609\n",
      "[Epoch 100] Loss: 7.3082\n",
      "[Epoch 200] Loss: 5.9642\n",
      "[Epoch 300] Loss: 5.1720\n",
      "[Epoch 400] Loss: 5.0698\n",
      "[Epoch 500] Loss: 4.9867\n",
      "\n",
      " [MYPlainGNN] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.9201\n",
      "[Epoch 100] Loss: 7.2635\n",
      "[Epoch 200] Loss: 5.9652\n",
      "[Epoch 300] Loss: 5.0525\n",
      "[Epoch 400] Loss: 5.1366\n",
      "[Epoch 500] Loss: 4.2861\n",
      "\n",
      " [MYPlainGNN] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.7932\n",
      "[Epoch 100] Loss: 7.2149\n",
      "[Epoch 200] Loss: 5.8572\n",
      "[Epoch 300] Loss: 5.1441\n",
      "[Epoch 400] Loss: 4.7979\n",
      "[Epoch 500] Loss: 4.5215\n",
      "\n",
      " [MYPlainGNN] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.5555\n",
      "[Epoch 100] Loss: 7.0134\n",
      "[Epoch 200] Loss: 5.7564\n",
      "[Epoch 300] Loss: 5.2277\n",
      "[Epoch 400] Loss: 4.8841\n",
      "[Epoch 500] Loss: 4.9201\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [MYFairGNN] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.9903\n",
      "[Epoch 100] Loss: 7.8758\n",
      "[Epoch 200] Loss: 7.2173\n",
      "[Epoch 300] Loss: 6.3845\n",
      "[Epoch 400] Loss: 5.9110\n",
      "[Epoch 500] Loss: 5.7734\n",
      "\n",
      " [MYFairGNN] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.6390\n",
      "[Epoch 100] Loss: 7.7507\n",
      "[Epoch 200] Loss: 6.5849\n",
      "[Epoch 300] Loss: 6.3360\n",
      "[Epoch 400] Loss: 5.8123\n",
      "[Epoch 500] Loss: 5.5797\n",
      "\n",
      " [MYFairGNN] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.5260\n",
      "[Epoch 100] Loss: 7.7561\n",
      "[Epoch 200] Loss: 7.4439\n",
      "[Epoch 300] Loss: 6.2019\n",
      "[Epoch 400] Loss: 5.5600\n",
      "[Epoch 500] Loss: 5.9918\n",
      "\n",
      " [MYFairGNN] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.9191\n",
      "[Epoch 100] Loss: 7.6931\n",
      "[Epoch 200] Loss: 6.8395\n",
      "[Epoch 300] Loss: 6.1099\n",
      "[Epoch 400] Loss: 5.7341\n",
      "[Epoch 500] Loss: 5.1341\n",
      "\n",
      " [MYFairGNN] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.8222\n",
      "[Epoch 100] Loss: 7.9774\n",
      "[Epoch 200] Loss: 6.6511\n",
      "[Epoch 300] Loss: 6.3772\n",
      "[Epoch 400] Loss: 6.3201\n",
      "[Epoch 500] Loss: 5.7210\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [DAFGNN_2] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 21.0654\n",
      "[Epoch 100] Loss: 9.7183\n",
      "[Epoch 200] Loss: 5.2877\n",
      "[Epoch 300] Loss: 4.7950\n",
      "[Epoch 400] Loss: 3.0360\n",
      "[Epoch 500] Loss: 1.7000\n",
      "\n",
      " [DAFGNN_2] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.9358\n",
      "[Epoch 100] Loss: 10.0253\n",
      "[Epoch 200] Loss: 9.5543\n",
      "[Epoch 300] Loss: 4.9019\n",
      "[Epoch 400] Loss: 4.0084\n",
      "[Epoch 500] Loss: 3.7311\n",
      "\n",
      " [DAFGNN_2] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.9511\n",
      "[Epoch 100] Loss: 7.0871\n",
      "[Epoch 200] Loss: 7.2756\n",
      "[Epoch 300] Loss: 7.4098\n",
      "[Epoch 400] Loss: 6.4538\n",
      "[Epoch 500] Loss: 9.3092\n",
      "\n",
      " [DAFGNN_2] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.9860\n",
      "[Epoch 100] Loss: 7.6110\n",
      "[Epoch 200] Loss: 15.7770\n",
      "[Epoch 300] Loss: 7.3284\n",
      "[Epoch 400] Loss: 4.4102\n",
      "[Epoch 500] Loss: 3.3632\n",
      "\n",
      " [DAFGNN_2] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.4124\n",
      "[Epoch 100] Loss: 10.3416\n",
      "[Epoch 200] Loss: 7.6345\n",
      "[Epoch 300] Loss: 4.8915\n",
      "[Epoch 400] Loss: 3.2742\n",
      "[Epoch 500] Loss: 3.2067\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [DAFGNN] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.6827\n",
      "[Epoch 100] Loss: 10.6631\n",
      "[Epoch 200] Loss: 8.8827\n",
      "[Epoch 300] Loss: 8.3681\n",
      "[Epoch 400] Loss: 8.4331\n",
      "[Epoch 500] Loss: 7.9201\n",
      "\n",
      " [DAFGNN] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.7311\n",
      "[Epoch 100] Loss: 10.9923\n",
      "[Epoch 200] Loss: 9.4621\n",
      "[Epoch 300] Loss: 9.0723\n",
      "[Epoch 400] Loss: 8.7301\n",
      "[Epoch 500] Loss: 7.9139\n",
      "\n",
      " [DAFGNN] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.9572\n",
      "[Epoch 100] Loss: 10.5321\n",
      "[Epoch 200] Loss: 8.4961\n",
      "[Epoch 300] Loss: 9.2105\n",
      "[Epoch 400] Loss: 9.1015\n",
      "[Epoch 500] Loss: 8.6138\n",
      "\n",
      " [DAFGNN] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.6969\n",
      "[Epoch 100] Loss: 10.6846\n",
      "[Epoch 200] Loss: 9.2704\n",
      "[Epoch 300] Loss: 8.9327\n",
      "[Epoch 400] Loss: 8.3272\n",
      "[Epoch 500] Loss: 8.5915\n",
      "\n",
      " [DAFGNN] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.5038\n",
      "[Epoch 100] Loss: 10.5559\n",
      "[Epoch 200] Loss: 9.1626\n",
      "[Epoch 300] Loss: 8.4707\n",
      "[Epoch 400] Loss: 8.1193\n",
      "[Epoch 500] Loss: 8.0320\n",
      "Loading nba dataset from ./dataset/NBA\n",
      "[INFO] 유효하지 않은 user_id로 인해 제거된 edge 수: 1641\n",
      "sens 0 count: 296\n",
      "sens 1 count: 107\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [FairGNN_Q] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.5099\n",
      "[Epoch 100] Loss: -3.1310\n",
      "[Epoch 200] Loss: 1.9209\n",
      "[Epoch 300] Loss: 2.7097\n",
      "[Epoch 400] Loss: 1.5308\n",
      "[Epoch 500] Loss: 1.6055\n",
      "\n",
      " [FairGNN_Q] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.5907\n",
      "[Epoch 100] Loss: 1.9003\n",
      "[Epoch 200] Loss: 1.7165\n",
      "[Epoch 300] Loss: 1.4081\n",
      "[Epoch 400] Loss: 1.2642\n",
      "[Epoch 500] Loss: 1.2330\n",
      "\n",
      " [FairGNN_Q] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.5304\n",
      "[Epoch 100] Loss: 1.6493\n",
      "[Epoch 200] Loss: 1.4727\n",
      "[Epoch 300] Loss: 1.3595\n",
      "[Epoch 400] Loss: 1.3266\n",
      "[Epoch 500] Loss: 1.2974\n",
      "\n",
      " [FairGNN_Q] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.5427\n",
      "[Epoch 100] Loss: 1.7855\n",
      "[Epoch 200] Loss: 1.5391\n",
      "[Epoch 300] Loss: 1.3209\n",
      "[Epoch 400] Loss: 1.2212\n",
      "[Epoch 500] Loss: 1.2729\n",
      "\n",
      " [FairGNN_Q] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.5579\n",
      "[Epoch 100] Loss: 2.6075\n",
      "[Epoch 200] Loss: 1.4118\n",
      "[Epoch 300] Loss: 1.3456\n",
      "[Epoch 400] Loss: 1.2495\n",
      "[Epoch 500] Loss: 1.2298\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [FMP_Q] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.5475\n",
      "[Epoch 100] Loss: 1.1047\n",
      "[Epoch 200] Loss: 0.5088\n",
      "[Epoch 300] Loss: 0.3450\n",
      "[Epoch 400] Loss: 0.2506\n",
      "[Epoch 500] Loss: 0.2343\n",
      "\n",
      " [FMP_Q] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.4955\n",
      "[Epoch 100] Loss: 0.6406\n",
      "[Epoch 200] Loss: 0.3770\n",
      "[Epoch 300] Loss: 0.3455\n",
      "[Epoch 400] Loss: 0.3019\n",
      "[Epoch 500] Loss: 0.2049\n",
      "\n",
      " [FMP_Q] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.4783\n",
      "[Epoch 100] Loss: 0.7132\n",
      "[Epoch 200] Loss: 0.4169\n",
      "[Epoch 300] Loss: 0.3283\n",
      "[Epoch 400] Loss: 0.2689\n",
      "[Epoch 500] Loss: 0.1422\n",
      "\n",
      " [FMP_Q] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.5718\n",
      "[Epoch 100] Loss: 0.6562\n",
      "[Epoch 200] Loss: 0.3835\n",
      "[Epoch 300] Loss: 0.2963\n",
      "[Epoch 400] Loss: 0.3852\n",
      "[Epoch 500] Loss: 0.1563\n",
      "\n",
      " [FMP_Q] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.6891\n",
      "[Epoch 100] Loss: 0.5542\n",
      "[Epoch 200] Loss: 0.3576\n",
      "[Epoch 300] Loss: 0.3051\n",
      "[Epoch 400] Loss: 0.2454\n",
      "[Epoch 500] Loss: 0.2648\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [MYPlainGNN] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.5136\n",
      "[Epoch 100] Loss: 1.3339\n",
      "[Epoch 200] Loss: 1.1615\n",
      "[Epoch 300] Loss: 1.1301\n",
      "[Epoch 400] Loss: 1.0604\n",
      "[Epoch 500] Loss: 1.0713\n",
      "\n",
      " [MYPlainGNN] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.5022\n",
      "[Epoch 100] Loss: 1.3332\n",
      "[Epoch 200] Loss: 1.1736\n",
      "[Epoch 300] Loss: 1.1236\n",
      "[Epoch 400] Loss: 1.1040\n",
      "[Epoch 500] Loss: 1.0310\n",
      "\n",
      " [MYPlainGNN] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.4564\n",
      "[Epoch 100] Loss: 1.3417\n",
      "[Epoch 200] Loss: 1.1636\n",
      "[Epoch 300] Loss: 1.0859\n",
      "[Epoch 400] Loss: 1.1375\n",
      "[Epoch 500] Loss: 1.0538\n",
      "\n",
      " [MYPlainGNN] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.5154\n",
      "[Epoch 100] Loss: 1.3326\n",
      "[Epoch 200] Loss: 1.1939\n",
      "[Epoch 300] Loss: 1.1310\n",
      "[Epoch 400] Loss: 1.1013\n",
      "[Epoch 500] Loss: 1.0507\n",
      "\n",
      " [MYPlainGNN] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.5581\n",
      "[Epoch 100] Loss: 1.3667\n",
      "[Epoch 200] Loss: 1.2168\n",
      "[Epoch 300] Loss: 1.1584\n",
      "[Epoch 400] Loss: 1.1048\n",
      "[Epoch 500] Loss: 1.0623\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [MYFairGNN] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.4695\n",
      "[Epoch 100] Loss: 1.3876\n",
      "[Epoch 200] Loss: 1.2472\n",
      "[Epoch 300] Loss: 1.1397\n",
      "[Epoch 400] Loss: 1.0998\n",
      "[Epoch 500] Loss: 1.0829\n",
      "\n",
      " [MYFairGNN] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.7508\n",
      "[Epoch 100] Loss: 1.3971\n",
      "[Epoch 200] Loss: 1.2836\n",
      "[Epoch 300] Loss: 1.1836\n",
      "[Epoch 400] Loss: 1.1320\n",
      "[Epoch 500] Loss: 1.0888\n",
      "\n",
      " [MYFairGNN] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.5642\n",
      "[Epoch 100] Loss: 1.3703\n",
      "[Epoch 200] Loss: 1.2657\n",
      "[Epoch 300] Loss: 1.1634\n",
      "[Epoch 400] Loss: 1.1778\n",
      "[Epoch 500] Loss: 1.1427\n",
      "\n",
      " [MYFairGNN] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.4791\n",
      "[Epoch 100] Loss: 1.3806\n",
      "[Epoch 200] Loss: 1.2576\n",
      "[Epoch 300] Loss: 1.1350\n",
      "[Epoch 400] Loss: 1.0879\n",
      "[Epoch 500] Loss: 1.0740\n",
      "\n",
      " [MYFairGNN] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.4149\n",
      "[Epoch 100] Loss: 1.3603\n",
      "[Epoch 200] Loss: 1.2173\n",
      "[Epoch 300] Loss: 1.1688\n",
      "[Epoch 400] Loss: 1.2130\n",
      "[Epoch 500] Loss: 1.1123\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [DAFGNN_2] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.6745\n",
      "[Epoch 100] Loss: 1.3031\n",
      "[Epoch 200] Loss: 1.3342\n",
      "[Epoch 300] Loss: 1.2693\n",
      "[Epoch 400] Loss: 1.2151\n",
      "[Epoch 500] Loss: 1.2114\n",
      "\n",
      " [DAFGNN_2] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.5855\n",
      "[Epoch 100] Loss: 1.4875\n",
      "[Epoch 200] Loss: 1.2924\n",
      "[Epoch 300] Loss: 1.3891\n",
      "[Epoch 400] Loss: 1.2337\n",
      "[Epoch 500] Loss: 1.1995\n",
      "\n",
      " [DAFGNN_2] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.7468\n",
      "[Epoch 100] Loss: 1.2711\n",
      "[Epoch 200] Loss: 1.2662\n",
      "[Epoch 300] Loss: 1.2246\n",
      "[Epoch 400] Loss: 1.3147\n",
      "[Epoch 500] Loss: 1.1997\n",
      "\n",
      " [DAFGNN_2] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.5494\n",
      "[Epoch 100] Loss: 1.2825\n",
      "[Epoch 200] Loss: 1.2687\n",
      "[Epoch 300] Loss: 1.1991\n",
      "[Epoch 400] Loss: 1.2387\n",
      "[Epoch 500] Loss: 1.2077\n",
      "\n",
      " [DAFGNN_2] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.5289\n",
      "[Epoch 100] Loss: 1.3802\n",
      "[Epoch 200] Loss: 1.2733\n",
      "[Epoch 300] Loss: 1.5015\n",
      "[Epoch 400] Loss: 1.2291\n",
      "[Epoch 500] Loss: 1.3182\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [DAFGNN] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.4416\n",
      "[Epoch 100] Loss: 1.6249\n",
      "[Epoch 200] Loss: 1.2754\n",
      "[Epoch 300] Loss: 1.2814\n",
      "[Epoch 400] Loss: 1.2075\n",
      "[Epoch 500] Loss: 1.2317\n",
      "\n",
      " [DAFGNN] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.5397\n",
      "[Epoch 100] Loss: 1.5715\n",
      "[Epoch 200] Loss: 1.3165\n",
      "[Epoch 300] Loss: 1.2501\n",
      "[Epoch 400] Loss: 1.2500\n",
      "[Epoch 500] Loss: 1.2913\n",
      "\n",
      " [DAFGNN] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.6031\n",
      "[Epoch 100] Loss: 1.5561\n",
      "[Epoch 200] Loss: 1.3204\n",
      "[Epoch 300] Loss: 1.2830\n",
      "[Epoch 400] Loss: 1.2084\n",
      "[Epoch 500] Loss: 1.2520\n",
      "\n",
      " [DAFGNN] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.4259\n",
      "[Epoch 100] Loss: 1.5781\n",
      "[Epoch 200] Loss: 1.3494\n",
      "[Epoch 300] Loss: 1.2688\n",
      "[Epoch 400] Loss: 1.2570\n",
      "[Epoch 500] Loss: 1.2360\n",
      "\n",
      " [DAFGNN] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4.6119\n",
      "[Epoch 100] Loss: 1.6027\n",
      "[Epoch 200] Loss: 1.3032\n",
      "[Epoch 300] Loss: 1.2924\n",
      "[Epoch 400] Loss: 1.2823\n",
      "[Epoch 500] Loss: 1.2166\n",
      "sens 0 count: 310\n",
      "sens 1 count: 690\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [FairGNN_Q] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 528.6573\n",
      "[Epoch 100] Loss: -2652.5044\n",
      "[Epoch 200] Loss: 1244.0808\n",
      "[Epoch 300] Loss: 596.5317\n",
      "[Epoch 400] Loss: 2890.0002\n",
      "[Epoch 500] Loss: 216.0986\n",
      "\n",
      " [FairGNN_Q] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 753.6680\n",
      "[Epoch 100] Loss: 419.3385\n",
      "[Epoch 200] Loss: 484.9182\n",
      "[Epoch 300] Loss: 299.9341\n",
      "[Epoch 400] Loss: 85.8263\n",
      "[Epoch 500] Loss: 110.4168\n",
      "\n",
      " [FairGNN_Q] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 674.7241\n",
      "[Epoch 100] Loss: -19874.1523\n",
      "[Epoch 200] Loss: 15909.3574\n",
      "[Epoch 300] Loss: 2735.7080\n",
      "[Epoch 400] Loss: 8583.5186\n",
      "[Epoch 500] Loss: 4968.4839\n",
      "\n",
      " [FairGNN_Q] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 6738.4800\n",
      "[Epoch 100] Loss: 3175.9326\n",
      "[Epoch 200] Loss: 3652.5552\n",
      "[Epoch 300] Loss: 3968.8428\n",
      "[Epoch 400] Loss: 1726.7645\n",
      "[Epoch 500] Loss: 361.0824\n",
      "\n",
      " [FairGNN_Q] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 8547.8828\n",
      "[Epoch 100] Loss: -13897.5645\n",
      "[Epoch 200] Loss: 33625.2891\n",
      "[Epoch 300] Loss: 9852.9414\n",
      "[Epoch 400] Loss: 856.9988\n",
      "[Epoch 500] Loss: 138.8094\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [FMP_Q] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 10.1906\n",
      "[Epoch 100] Loss: 0.2531\n",
      "[Epoch 200] Loss: 0.1865\n",
      "[Epoch 300] Loss: 0.1686\n",
      "[Epoch 400] Loss: 0.1635\n",
      "[Epoch 500] Loss: 0.1701\n",
      "\n",
      " [FMP_Q] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 41.3334\n",
      "[Epoch 100] Loss: 0.3148\n",
      "[Epoch 200] Loss: 0.2572\n",
      "[Epoch 300] Loss: 0.2653\n",
      "[Epoch 400] Loss: 0.1804\n",
      "[Epoch 500] Loss: 0.2070\n",
      "\n",
      " [FMP_Q] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 26.8356\n",
      "[Epoch 100] Loss: 0.1916\n",
      "[Epoch 200] Loss: 0.2528\n",
      "[Epoch 300] Loss: 0.1882\n",
      "[Epoch 400] Loss: 0.1694\n",
      "[Epoch 500] Loss: 0.1681\n",
      "\n",
      " [FMP_Q] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 24.1004\n",
      "[Epoch 100] Loss: 0.2213\n",
      "[Epoch 200] Loss: 0.3262\n",
      "[Epoch 300] Loss: 0.1967\n",
      "[Epoch 400] Loss: 0.1990\n",
      "[Epoch 500] Loss: 0.1802\n",
      "\n",
      " [FMP_Q] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 8.3032\n",
      "[Epoch 100] Loss: 0.1845\n",
      "[Epoch 200] Loss: 0.3011\n",
      "[Epoch 300] Loss: 0.1862\n",
      "[Epoch 400] Loss: 0.1949\n",
      "[Epoch 500] Loss: 0.1667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [MYPlainGNN] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 34.3358\n",
      "[Epoch 100] Loss: 0.7035\n",
      "[Epoch 200] Loss: 0.2509\n",
      "[Epoch 300] Loss: 0.1942\n",
      "[Epoch 400] Loss: 0.2322\n",
      "[Epoch 500] Loss: 0.1776\n",
      "\n",
      " [MYPlainGNN] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 30.0253\n",
      "[Epoch 100] Loss: 0.8262\n",
      "[Epoch 200] Loss: 0.3758\n",
      "[Epoch 300] Loss: 0.2059\n",
      "[Epoch 400] Loss: 0.7847\n",
      "[Epoch 500] Loss: 0.2003\n",
      "\n",
      " [MYPlainGNN] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 52.8299\n",
      "[Epoch 100] Loss: 0.3076\n",
      "[Epoch 200] Loss: 0.1973\n",
      "[Epoch 300] Loss: 0.2271\n",
      "[Epoch 400] Loss: 0.1793\n",
      "[Epoch 500] Loss: 0.1699\n",
      "\n",
      " [MYPlainGNN] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 27.9032\n",
      "[Epoch 100] Loss: 1.0894\n",
      "[Epoch 200] Loss: 0.2822\n",
      "[Epoch 300] Loss: 0.2195\n",
      "[Epoch 400] Loss: 0.3958\n",
      "[Epoch 500] Loss: 0.3091\n",
      "\n",
      " [MYPlainGNN] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 137.3392\n",
      "[Epoch 100] Loss: 1.0266\n",
      "[Epoch 200] Loss: 0.1952\n",
      "[Epoch 300] Loss: 0.1791\n",
      "[Epoch 400] Loss: 0.2019\n",
      "[Epoch 500] Loss: 0.2127\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [MYFairGNN] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 33.1453\n",
      "[Epoch 100] Loss: 0.2797\n",
      "[Epoch 200] Loss: 0.2632\n",
      "[Epoch 300] Loss: 0.2398\n",
      "[Epoch 400] Loss: 0.2561\n",
      "[Epoch 500] Loss: 0.5277\n",
      "\n",
      " [MYFairGNN] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 109.3886\n",
      "[Epoch 100] Loss: 0.3901\n",
      "[Epoch 200] Loss: 0.2350\n",
      "[Epoch 300] Loss: 0.2040\n",
      "[Epoch 400] Loss: 0.1876\n",
      "[Epoch 500] Loss: 0.1711\n",
      "\n",
      " [MYFairGNN] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 39.1670\n",
      "[Epoch 100] Loss: 1.3074\n",
      "[Epoch 200] Loss: 0.2502\n",
      "[Epoch 300] Loss: 0.2538\n",
      "[Epoch 400] Loss: 0.1954\n",
      "[Epoch 500] Loss: 0.2097\n",
      "\n",
      " [MYFairGNN] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 75.6321\n",
      "[Epoch 100] Loss: 0.2595\n",
      "[Epoch 200] Loss: 0.1987\n",
      "[Epoch 300] Loss: 0.1835\n",
      "[Epoch 400] Loss: 0.1501\n",
      "[Epoch 500] Loss: 0.1505\n",
      "\n",
      " [MYFairGNN] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 16.0817\n",
      "[Epoch 100] Loss: 1.6636\n",
      "[Epoch 200] Loss: 0.2881\n",
      "[Epoch 300] Loss: 0.2417\n",
      "[Epoch 400] Loss: 0.3285\n",
      "[Epoch 500] Loss: 0.2113\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [DAFGNN_2] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 2035.8488\n",
      "[Epoch 100] Loss: 0.5295\n",
      "[Epoch 200] Loss: 0.2217\n",
      "[Epoch 300] Loss: 0.2433\n",
      "[Epoch 400] Loss: 0.2041\n",
      "[Epoch 500] Loss: 0.3130\n",
      "\n",
      " [DAFGNN_2] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 1486.5958\n",
      "[Epoch 100] Loss: 0.9713\n",
      "[Epoch 200] Loss: 0.3739\n",
      "[Epoch 300] Loss: 1.0674\n",
      "[Epoch 400] Loss: 0.5074\n",
      "[Epoch 500] Loss: 0.2788\n",
      "\n",
      " [DAFGNN_2] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 295.2654\n",
      "[Epoch 100] Loss: 6.2343\n",
      "[Epoch 200] Loss: 5.2213\n",
      "[Epoch 300] Loss: 2.8473\n",
      "[Epoch 400] Loss: 1.0180\n",
      "[Epoch 500] Loss: 0.4781\n",
      "\n",
      " [DAFGNN_2] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 4060.7461\n",
      "[Epoch 100] Loss: 18.3354\n",
      "[Epoch 200] Loss: 1.4383\n",
      "[Epoch 300] Loss: 0.7223\n",
      "[Epoch 400] Loss: 0.7937\n",
      "[Epoch 500] Loss: 0.3219\n",
      "\n",
      " [DAFGNN_2] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 7346.1494\n",
      "[Epoch 100] Loss: 2.5193\n",
      "[Epoch 200] Loss: 0.6187\n",
      "[Epoch 300] Loss: 0.3197\n",
      "[Epoch 400] Loss: 0.5573\n",
      "[Epoch 500] Loss: 1.1661\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [DAFGNN] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 75.9556\n",
      "[Epoch 100] Loss: 0.5068\n",
      "[Epoch 200] Loss: 0.8345\n",
      "[Epoch 300] Loss: 0.7978\n",
      "[Epoch 400] Loss: 0.6556\n",
      "[Epoch 500] Loss: 0.5239\n",
      "\n",
      " [DAFGNN] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 12.6866\n",
      "[Epoch 100] Loss: 0.2946\n",
      "[Epoch 200] Loss: 0.3274\n",
      "[Epoch 300] Loss: 1.2019\n",
      "[Epoch 400] Loss: 1.6464\n",
      "[Epoch 500] Loss: 0.5502\n",
      "\n",
      " [DAFGNN] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 413.5307\n",
      "[Epoch 100] Loss: 1.9474\n",
      "[Epoch 200] Loss: 0.6418\n",
      "[Epoch 300] Loss: 1.0500\n",
      "[Epoch 400] Loss: 0.8678\n",
      "[Epoch 500] Loss: 0.2010\n",
      "\n",
      " [DAFGNN] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 319.8308\n",
      "[Epoch 100] Loss: 1.6170\n",
      "[Epoch 200] Loss: 1.6711\n",
      "[Epoch 300] Loss: 0.7704\n",
      "[Epoch 400] Loss: 0.2880\n",
      "[Epoch 500] Loss: 0.1969\n",
      "\n",
      " [DAFGNN] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 10.7347\n",
      "[Epoch 100] Loss: 0.6265\n",
      "[Epoch 200] Loss: 0.5966\n",
      "[Epoch 300] Loss: 0.5476\n",
      "[Epoch 400] Loss: 0.3640\n",
      "[Epoch 500] Loss: 0.3051\n"
     ]
    }
   ],
   "source": [
    "# 전체 모델에 대해서 수행\n",
    "tau = 0.5\n",
    "lambda_fair = 0.5\n",
    "all_dataset_results = {}\n",
    "\n",
    "for dataset in ['region_job', 'region_job_2', 'nba', 'german']:\n",
    "    seed = 1127\n",
    "    if dataset == 'nba':\n",
    "        predict_attr, sens_attr, label_number, sens_number, test_idx, path = nba_predict_attr, nba_sens_attr, nba_label_number, nba_sens_number, nba_test_idx, nba_path\n",
    "    elif dataset == 'german':\n",
    "        predict_attr, sens_attr, path = german_predict_attr, german_sens_attr, german_path\n",
    "    else:\n",
    "        predict_attr, sens_attr, label_number, sens_number, test_idx, path = pokec_predict_attr, pokec_sens_attr, pokec_label_number, pokec_sens_number, pokec_test_idx, pokec_path\n",
    "\n",
    "    adj, features, labels, idx_train, idx_val, idx_test, sens, idx_sens_train = load_dataset_unified(dataset, sens_attr, predict_attr, path, label_number, sens_number, test_idx, seed=seed)\n",
    "    \n",
    "    print(\"sens 0 count:\", torch.sum(sens == 0).item())\n",
    "    print(\"sens 1 count:\", torch.sum(sens == 1).item())\n",
    "\n",
    "    if dataset == 'nba':\n",
    "        features = feature_norm(features)\n",
    "\n",
    "    if isinstance(adj, torch.Tensor):  # torch.sparse_coo_tensor\n",
    "        src, dst = adj._indices()\n",
    "        g = dgl.graph((src, dst))\n",
    "        edge_index = adj._indices()\n",
    "    else:  # scipy sparse matrix\n",
    "        g = dgl.from_scipy(adj)\n",
    "        adj_coo = adj.tocoo()\n",
    "        edge_index = torch.tensor([adj_coo.row, adj_coo.col], dtype=torch.long)\n",
    "\n",
    "    data = Data(x=features, edge_index=edge_index, y=labels.float(), sensitive_attr=sens, quantile_tau=torch.full((features.shape[0],), tau))\n",
    "    data.idx_train = idx_train\n",
    "    data.idx_val = idx_val\n",
    "    data.idx_test = idx_test\n",
    "    data.idx_sens_train = idx_sens_train\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    fq_results, fq_all_metrics = multiple_runs_with_full_metrics(FairGNN_Q, data, device, n_runs, hidden_dim, n_epochs, tau, lambda_fair, reg=True)\n",
    "    fq_results['Model'] = 'FairGNN'\n",
    "    all_results.append(fq_results)\n",
    "\n",
    "    fmp_results, fmp_all_metrics = multiple_runs_with_full_metrics(FMP_Q, data, device, n_runs, hidden_dim, n_epochs, tau, lambda_fair, reg=True)\n",
    "    fmp_results['Model'] = 'FMP'\n",
    "    all_results.append(fmp_results)\n",
    "\n",
    "    plain_results, plain_all_metrics = multiple_runs_with_full_metrics(MYPlainGNN, data, device, n_runs, hidden_dim, n_epochs, tau, lambda_fair, reg=True)\n",
    "    plain_results['Model'] = 'B-GNN'\n",
    "    all_results.append(plain_results)\n",
    "\n",
    "    fair_results, fair_all_metrics = multiple_runs_with_full_metrics(MYFairGNN, data, device, n_runs, hidden_dim, n_epochs, tau, lambda_fair, reg=True)\n",
    "    fair_results['Model'] = 'F-GNN'\n",
    "    all_results.append(fair_results)\n",
    "\n",
    "    dist_results, dist_all_metrics = multiple_runs_with_full_metrics(DAFGNN_2, data, device, n_runs, hidden_dim, n_epochs, tau, lambda_fair, reg=True)\n",
    "    dist_results['Model'] = 'DAF-GNN-2'\n",
    "    all_results.append(dist_results)\n",
    "\n",
    "    dist_results, dist_all_metrics = multiple_runs_with_full_metrics(DAFGNN, data, device, n_runs, hidden_dim, n_epochs, tau, lambda_fair, reg=True)\n",
    "    dist_results['Model'] = 'DAF-GNN'\n",
    "    all_results.append(dist_results)\n",
    "\n",
    "    # 결과 입력\n",
    "    all_dataset_results[dataset] = all_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "region_job\n",
      "----------------------------------------------------------------------------------------------------\n",
      "& FairGNN & $8.86 \\pm 0.58$ & $17.72 \\pm 1.16$ & $1.34 \\pm 0.67$ & $187.21 \\pm 30.91$  \\\\\n",
      "& FMP & $1.67 \\pm 0.19$ & $3.35 \\pm 0.37$ & $1.80 \\pm 0.08$ & $14.53 \\pm 2.68$  \\\\\n",
      "& B-GNN & $8.54 \\pm 0.17$ & $17.09 \\pm 0.34$ & $2.37 \\pm 0.11$ & $223.94 \\pm 17.16$  \\\\\n",
      "& F-GNN & $8.41 \\pm 0.07$ & $16.83 \\pm 0.13$ & $0.17 \\pm 0.13$ & $204.66 \\pm 6.59$  \\\\\n",
      "& DAF-GNN-2 & $10.64 \\pm 0.28$ & $21.28 \\pm 0.57$ & $0.13 \\pm 0.10$ & $301.50 \\pm 21.44$  \\\\\n",
      "& DAF-GNN & $8.60 \\pm 0.07$ & $17.20 \\pm 0.13$ & $1.37 \\pm 0.08$ & $157.97 \\pm 9.18$  \\\\\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "region_job_2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "& FairGNN & $8.87 \\pm 0.78$ & $17.75 \\pm 1.56$ & $3.87 \\pm 0.66$ & $164.05 \\pm 19.92$  \\\\\n",
      "& FMP & $1.58 \\pm 0.09$ & $3.15 \\pm 0.18$ & $3.02 \\pm 0.06$ & $11.32 \\pm 0.34$  \\\\\n",
      "& B-GNN & $8.53 \\pm 0.08$ & $17.07 \\pm 0.17$ & $4.09 \\pm 0.32$ & $195.75 \\pm 3.25$  \\\\\n",
      "& F-GNN & $8.12 \\pm 0.10$ & $16.23 \\pm 0.20$ & $0.39 \\pm 0.31$ & $170.75 \\pm 10.00$  \\\\\n",
      "& DAF-GNN-2 & $13.16 \\pm 4.82$ & $26.32 \\pm 9.64$ & $0.81 \\pm 0.71$ & $361.92 \\pm 155.56$  \\\\\n",
      "& DAF-GNN & $8.25 \\pm 0.05$ & $16.50 \\pm 0.09$ & $1.60 \\pm 0.10$ & $135.77 \\pm 2.05$  \\\\\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "nba\n",
      "----------------------------------------------------------------------------------------------------\n",
      "& FairGNN & $1.23 \\pm 0.01$ & $2.47 \\pm 0.01$ & $0.10 \\pm 0.11$ & $4.13 \\pm 0.13$  \\\\\n",
      "& FMP & $0.60 \\pm 0.01$ & $1.19 \\pm 0.02$ & $0.11 \\pm 0.02$ & $0.90 \\pm 0.03$  \\\\\n",
      "& B-GNN & $1.35 \\pm 0.03$ & $2.69 \\pm 0.05$ & $0.20 \\pm 0.15$ & $5.91 \\pm 1.25$  \\\\\n",
      "& F-GNN & $1.34 \\pm 0.03$ & $2.68 \\pm 0.07$ & $0.13 \\pm 0.06$ & $5.05 \\pm 0.42$  \\\\\n",
      "& DAF-GNN-2 & $1.32 \\pm 0.04$ & $2.65 \\pm 0.08$ & $0.07 \\pm 0.01$ & $5.14 \\pm 0.70$  \\\\\n",
      "& DAF-GNN & $1.24 \\pm 0.02$ & $2.49 \\pm 0.04$ & $0.03 \\pm 0.03$ & $4.27 \\pm 0.24$  \\\\\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "german\n",
      "----------------------------------------------------------------------------------------------------\n",
      "& FairGNN & $38.71 \\pm 71.16$ & $77.43 \\pm 142.32$ & $9.78 \\pm 17.97$ & $3261.62 \\pm 6508.26$  \\\\\n",
      "& FMP & $0.18 \\pm 0.01$ & $0.36 \\pm 0.03$ & $0.04 \\pm 0.01$ & $0.11 \\pm 0.04$  \\\\\n",
      "& B-GNN & $0.19 \\pm 0.01$ & $0.37 \\pm 0.02$ & $0.03 \\pm 0.01$ & $0.11 \\pm 0.03$  \\\\\n",
      "& F-GNN & $0.28 \\pm 0.18$ & $0.56 \\pm 0.36$ & $0.05 \\pm 0.06$ & $0.17 \\pm 0.13$  \\\\\n",
      "& DAF-GNN-2 & $0.47 \\pm 0.22$ & $0.93 \\pm 0.45$ & $0.00 \\pm 0.00$ & $0.17 \\pm 0.08$  \\\\\n",
      "& DAF-GNN & $0.42 \\pm 0.25$ & $0.83 \\pm 0.50$ & $0.10 \\pm 0.07$ & $0.29 \\pm 0.13$  \\\\\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 실험 결과 출력\n",
    "for dataset in ['region_job', 'region_job_2', 'nba', 'german']:\n",
    "    df = pd.DataFrame(all_dataset_results[dataset]).round(2)\n",
    "    print('-' * 100)\n",
    "    print(dataset)\n",
    "    print('-' * 100)\n",
    "\n",
    "    # 정확도(예측 성능) 관련 지표 # 불확실성(분산, 안정성) 관련 지표\n",
    "    # df_1 = df[['Model', \n",
    "    #                     'QR Loss Mean', 'QR Loss Std', \n",
    "    #                     'MAE Mean', 'MAE Std', \n",
    "    #                     'MAPE Mean', 'MAPE Std', \n",
    "    #                     'RMSE Mean', 'RMSE Std', \n",
    "    #                     'R2 Mean', 'R2 Std',\n",
    "    #                     'MAE-Var Mean', 'MAE-Var Std', \n",
    "    #                     'MAPE-Var Mean', 'MAPE-Var Std'\n",
    "    #                     ]].round(2)\n",
    "\n",
    "    # metrics = [\n",
    "    #     'QR Loss', \n",
    "    #     'MAE', \n",
    "        # 'MAPE', \n",
    "        # 'RMSE', \n",
    "        # 'R2', \n",
    "    #     'MAE-Var', 'MAPE-Var']\n",
    "    # latex_lines = generate_latex_rows(df_1, metrics, end_column=' & --')\n",
    "    # for l in latex_lines:\n",
    "    #     print(l)\n",
    "    # print('-' * 100)\n",
    "\n",
    "    # 공정성 관련 지표 # 그룹별 성능 지표\n",
    "    # df_2 = df[['Model', \n",
    "    #                     'Mean Gap Mean', 'Mean Gap Std', \n",
    "    #                     'MAE Gap Mean', 'MAE Gap Std', \n",
    "    #                     'RMSE Gap Mean', 'RMSE Gap Std', \n",
    "    #                     'Error-Sensitive Corr Mean', 'Error-Sensitive Corr Std', \n",
    "    #                     'mean_group_0 Mean', 'mean_group_0 Std',\n",
    "    #                     'mae_group_0 Mean', 'mae_group_0 Std', \n",
    "    #                     'rmse_group_0 Mean', 'rmse_group_0 Std', \n",
    "    #                     'mean_group_1 Mean', 'mean_group_1 Std',\n",
    "    #                     'mae_group_1 Mean', 'mae_group_1 Std', \n",
    "    #                     'rmse_group_1 Mean', 'rmse_group_1 Std']].round(2)\n",
    "\n",
    "    # metrics = [\n",
    "            #     'mean_group_0', 'mean_group_1', \n",
    "            #    'mae_group_0', 'mae_group_1', \n",
    "            #    'rmse_group_0', 'rmse_group_1', \n",
    "    #            'Mean Gap', \n",
    "    #            'MAE Gap', \n",
    "    #            'RMSE Gap', \n",
    "    #            'Error-Sensitive Corr'\n",
    "    #            ]\n",
    "    # latex_lines = generate_latex_rows(df_2, metrics, end_column='')\n",
    "    # for l in latex_lines:\n",
    "    #     print(l)\n",
    "\n",
    "    # 수정된 메트릭스 추출\n",
    "    metrics = [\n",
    "        'QR Loss', 'MAE',\n",
    "         # 'RMSE', \n",
    "        'Mean Gap', \n",
    "        # 'MAE Gap', \n",
    "        # 'Error-Sensitive Corr',\n",
    "        'MAE-Var'\n",
    "        # , 'MAPE-Var'\n",
    "        ]\n",
    "    latex_lines = generate_latex_rows(df, metrics, end_column='')\n",
    "    for l in latex_lines:\n",
    "        print(l)\n",
    "\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "models = [\n",
    "    # 'FairGNN', \n",
    "    'FMP', 'B-GNN', 'F-GNN', 'DAF-GNN', 'DAF-GNN-2'\n",
    "    ]\n",
    "colors = sns.color_palette(\"muted\")\n",
    "datasets = list(all_dataset_results.keys())\n",
    "\n",
    "qr_means_raw = [model_result['QR Loss Mean'] for results in all_dataset_results.values() for model_result in results]\n",
    "qr_stds_raw = [model_result['QR Loss Std'] for results in all_dataset_results.values() for model_result in results]\n",
    "fair_means_raw = [model_result['Mean Gap Mean'] for results in all_dataset_results.values() for model_result in results]\n",
    "fair_stds_raw = [model_result['Mean Gap Std'] for results in all_dataset_results.values() for model_result in results]\n",
    "mae_means_raw = [model_result['MAE Mean'] for results in all_dataset_results.values() for model_result in results]\n",
    "mae_stds_raw = [model_result['MAE Std'] for results in all_dataset_results.values() for model_result in results]\n",
    "mae_var_means_raw = [model_result['MAE-Var Mean'] for results in all_dataset_results.values() for model_result in results]\n",
    "mae_var_stds_raw = [model_result['MAE-Var Std'] for results in all_dataset_results.values() for model_result in results]\n",
    "\n",
    "qr_means = [np.mean(qr_means_raw[i::5]) for i in range(5)]\n",
    "qr_stds = [np.mean(qr_stds_raw[i::5]) for i in range(5)]\n",
    "fair_means = [np.mean(fair_means_raw[i::5]) for i in range(5)]\n",
    "fair_stds = [np.mean(fair_stds_raw[i::5]) for i in range(5)]\n",
    "mae_means = [np.mean(mae_means_raw[i::5]) for i in range(5)]\n",
    "mae_stds = [np.mean(mae_stds_raw[i::5]) for i in range(5)]\n",
    "mae_var_means = [np.mean(mae_var_means_raw[i::5]) for i in range(5)]\n",
    "mae_var_stds = [np.mean(mae_var_stds_raw[i::5]) for i in range(5)]\n",
    "\n",
    "qr_gap = np.round(np.max(qr_means) - np.min(qr_means), 3)\n",
    "fair_gap = np.round(np.max(fair_means) - np.min(fair_means), 3)\n",
    "width = 0.2  \n",
    "\n",
    "# fig, ax1 = plt.subplots(figsize=(8, 3))  # 약간 넓은 plot 사이즈\n",
    "# bars1 = ax1.bar(x - 1.5 * width, qr_means, width, yerr=qr_stds, capsize=5, label='QR Loss')\n",
    "# bars2 = ax1.bar(x - 0.5 * width, mae_means, width, yerr=mae_stds, capsize=5, label='MAE')\n",
    "# bars3 = ax1.bar(x + 0.5 * width, mae_var_means, width, yerr=mae_var_stds, capsize=5, label='MAE-var')\n",
    "# bars4 = ax1.bar(x + 1.5 * width, fair_means, width, yerr=fair_stds, capsize=5, label='Fairness (Mean Gap)')\n",
    "\n",
    "# for bar in bars1:\n",
    "#     height = bar.get_height()\n",
    "#     ax1.annotate(f'{height:.2f}',\n",
    "#                  xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "#                  xytext=(12, 3),  # 위로 3pt\n",
    "#                  textcoords=\"offset points\",\n",
    "#                  ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# for bar in bars4:\n",
    "#     height = bar.get_height()\n",
    "#     ax1.annotate(f'{height:.2f}',\n",
    "#                  xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "#                  xytext=(12, 3),\n",
    "#                  textcoords=\"offset points\",\n",
    "#                  ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# ax1.set_ylabel('Score')\n",
    "# ax1.set_xticks(x)\n",
    "# ax1.set_xticklabels(models)\n",
    "# ax1.legend(loc='best', fontsize=9)\n",
    "# ax1.set_title('Performance and Fairness Comparison', size=11)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(12, 3))  # 1행 4열, 약간 더 넓게\n",
    "axs[0].bar(x, qr_means, yerr=qr_stds, capsize=5, color='tab:blue')\n",
    "axs[0].set_title('QR Loss')\n",
    "axs[0].set_xticks(x)\n",
    "axs[0].set_xticklabels(models, rotation=45, ha='right')\n",
    "\n",
    "axs[1].bar(x, mae_means, yerr=mae_stds, capsize=5, color='tab:orange')\n",
    "axs[1].set_title('MAE')\n",
    "axs[1].set_xticks(x)\n",
    "axs[1].set_xticklabels(models, rotation=45, ha='right')\n",
    "\n",
    "axs[2].bar(x, mae_var_means, yerr=mae_var_stds, capsize=5, color='tab:green')\n",
    "axs[2].set_title('MAE-var')\n",
    "axs[2].set_xticks(x)\n",
    "axs[2].set_xticklabels(models, rotation=45, ha='right')\n",
    "\n",
    "axs[3].bar(x, fair_means, yerr=fair_stds, capsize=5, color='tab:red')\n",
    "axs[3].set_title('Fairness (Mean Gap)')\n",
    "axs[3].set_xticks(x)\n",
    "axs[3].set_xticklabels(models, rotation=45, ha='right')\n",
    "\n",
    "# fig.suptitle('Performance and Fairness Comparison', fontsize=12)\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.95])  # title 공간 확보\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "idx = 0\n",
    "for dataset in datasets:\n",
    "    results = all_dataset_results[dataset]\n",
    "    for i, model in enumerate(models):\n",
    "        plt.scatter(\n",
    "            fair_means_raw[idx], qr_means_raw[idx],\n",
    "            color=colors[i], s=100, alpha=0.7, label=model\n",
    "        )\n",
    "        idx += 1\n",
    "\n",
    "plt.xlabel('Fairness (Mean Gap)')\n",
    "plt.ylabel('QR Loss')\n",
    "plt.title('Performance-Fairness Trade-off across Datasets', size=11)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict()\n",
    "for handle, label in zip(handles, labels):\n",
    "    if label not in by_label:\n",
    "        by_label[label] = handle\n",
    "plt.legend(by_label.values(), by_label.keys(), loc='lower right', frameon=True, fontsize=9)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 하나에 대해서 수행\n",
    "all_results = []\n",
    "\n",
    "fq_results, fq_all_metrics = multiple_runs_with_full_metrics(FairGNN_Q, data, device, n_runs, hidden_dim, n_epochs, tau, lambda_fair, reg=True)\n",
    "fq_results['Model'] = 'FairGNN'\n",
    "all_results.append(fq_results)\n",
    "\n",
    "fmp_results, fmp_all_metrics = multiple_runs_with_full_metrics(FMP_Q, data, device, n_runs, hidden_dim, n_epochs, tau, lambda_fair, reg=True)\n",
    "fmp_results['Model'] = 'FMP'\n",
    "all_results.append(fmp_results)\n",
    "\n",
    "plain_results, plain_all_metrics = multiple_runs_with_full_metrics(MYPlainGNN, data, device, n_runs, hidden_dim, n_epochs, tau, lambda_fair, reg=True)\n",
    "plain_results['Model'] = 'B-GNN'\n",
    "all_results.append(plain_results)\n",
    "\n",
    "fair_results, fair_all_metrics = multiple_runs_with_full_metrics(MYFairGNN, data, device, n_runs, hidden_dim, n_epochs, tau, lambda_fair, reg=True)\n",
    "fair_results['Model'] = 'F-GNN'\n",
    "all_results.append(fair_results)\n",
    "\n",
    "dist_results, dist_all_metrics = multiple_runs_with_full_metrics(DAFGNN, data, device, n_runs, hidden_dim, n_epochs, tau, lambda_fair, reg=True)\n",
    "dist_results['Model'] = 'DAF-GNN'\n",
    "all_results.append(dist_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비교 실험 결과\n",
    "all_results = pd.DataFrame(all_results)\n",
    "all_results.round(2)\n",
    "\n",
    "# 정확도(예측 성능) 관련 지표 # 불확실성(분산, 안정성) 관련 지표\n",
    "df_1 = all_results[['Model', 'QR Loss Mean', 'QR Loss Std', 'MAE Mean', 'MAE Std', 'MAPE Mean', 'MAPE Std', 'RMSE Mean', 'RMSE Std', 'MAE-Var Mean', 'MAE-Var Std', 'MAPE-Var Mean', 'MAPE-Var Std']].round(2)\n",
    "\n",
    "metrics = ['QR Loss', 'MAE', 'MAPE', 'RMSE', 'MAE-Var', 'MAPE-Var']\n",
    "latex_lines = generate_latex_rows(df_1, metrics, end_column=' & --')\n",
    "for l in latex_lines:\n",
    "    print(l)\n",
    "\n",
    "# 공정성 관련 지표 # 그룹별 성능 지표\n",
    "df_2 = all_results[['Model', 'MAE Gap (group 0 vs 1) Mean', 'MAE Gap (group 0 vs 1) Std', 'RMSE Gap (group 0 vs 1) Mean', 'RMSE Gap (group 0 vs 1) Std', 'Error-Sensitive Corr Mean', 'Error-Sensitive Corr Std', 'mae_group_0 Mean', 'mae_group_0 Std', 'rmse_group_0 Mean', 'rmse_group_0 Std', 'mae_group_1 Mean', 'mae_group_1 Std', 'rmse_group_1 Mean', 'rmse_group_1 Std']].round(2)\n",
    "\n",
    "metrics = ['mae_group_0', 'mae_group_1', 'rmse_group_0', 'rmse_group_1', 'MAE Gap (group 0 vs 1)', 'RMSE Gap (group 0 vs 1)', 'Error-Sensitive Corr']\n",
    "latex_lines = generate_latex_rows(df_2, metrics, end_column='')\n",
    "for l in latex_lines:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "models = ['B-GNN', 'F-GNN', 'FairGNN', 'FMP', 'DAF-GNN']\n",
    "colors = sns.set_palette(\"muted\")\n",
    "\n",
    "qr_means = all_results['QR Loss Mean']\n",
    "qr_stds = all_results['QR Loss Std']\n",
    "fair_means = all_results['Fairness Gap Mean']\n",
    "fair_stds = all_results['Fairness Gap Std']\n",
    "qr_gap = np.round(np.max(qr_means) - np.min(qr_means), 3)\n",
    "fair_gap = np.round(np.max(fair_means) - np.min(fair_means), 3)\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "fig, ax1 = plt.subplots(figsize=(6,3))\n",
    "ax1.bar(x - width/2, qr_means, width, yerr=qr_stds, capsize=5, label='QR Loss')\n",
    "ax1.bar(x + width/2, fair_means, width, yerr=fair_stds, capsize=5, label='Fairness Gap')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models)\n",
    "ax1.legend(loc='best')\n",
    "ax1.set_title('Performance and Fairness Comparison')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "for i in range(len(models)):\n",
    "    plt.scatter(fair_means[i], qr_means[i], color=colors[i], label=models[i], s=100, alpha=0.5)\n",
    "for i in range(len(models)):\n",
    "    plt.text(fair_means[i], qr_means[i], models[i], fontsize=11)\n",
    "\n",
    "plt.ylabel('QR Loss')\n",
    "plt.xlabel('Fair Loss')\n",
    "plt.ylim(0, 11)\n",
    "plt.xlim(0, 10)\n",
    "# plt.xlim(right = np.max(fair_means) + fair_gap/10)\n",
    "plt.title('Trade-off between Performance and Fairness')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our 단일 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our\n",
    "class DAFGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, device, lambda_fair=1.0, gamma=1.0, alpha=1.0, use_projection=True, use_fairness=True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.lambda_fair = lambda_fair\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.use_projection = use_projection\n",
    "        self.use_fairness = use_fairness\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, x, edge_index, return_hidden=False):\n",
    "        h = F.relu(self.conv1(x, edge_index))\n",
    "        h = F.relu(self.conv2(h, edge_index))\n",
    "        \n",
    "        preds = self.fc(h).squeeze(-1)\n",
    "\n",
    "        if return_hidden:\n",
    "            return preds, h\n",
    "        else:\n",
    "            return preds\n",
    "        \n",
    "    def distribution_aware_fairness_loss(self, preds, sensitive, var_gap=True):\n",
    "        g0 = preds[sensitive == 0]\n",
    "        g1 = preds[sensitive == 1]\n",
    "\n",
    "        if len(g0) == 0 or len(g1) == 0:\n",
    "            return torch.tensor(0.0, device=preds.device)\n",
    "\n",
    "        mean_gap = torch.abs(g0.mean() - g1.mean())\n",
    "        gap = mean_gap\n",
    "        \n",
    "        if var_gap:\n",
    "            var_gap = torch.abs(g0.var(unbiased=False) - g1.var(unbiased=False))\n",
    "            gap = mean_gap + self.alpha * var_gap\n",
    "        return gap\n",
    "\n",
    "    def compute_bias_score(self, hidden, sensitive):\n",
    "        g0 = hidden[sensitive == 0]\n",
    "        g1 = hidden[sensitive == 1]\n",
    "        bias = torch.norm(g0.mean(dim=0) - g1.mean(dim=0), p=2)\n",
    "        return bias\n",
    "\n",
    "    def adaptive_projection(self, h, bias_score):\n",
    "        h_norm = h.norm(p=2, dim=1, keepdim=True) + 1e-6\n",
    "        adaptive_lambda = self.lambda_fair * (1 + self.gamma * bias_score)\n",
    "        scale = torch.min(torch.ones_like(h_norm), adaptive_lambda.unsqueeze(-1) / h_norm)\n",
    "        return h * scale\n",
    "\n",
    "    def hierarchical_lambda(self, epoch, max_lambda=1.0, beta=0.05):\n",
    "        device = next(self.parameters()).device  # 모델 파라미터의 디바이스 확인\n",
    "        return max_lambda * (1 - torch.exp(-beta * torch.tensor(epoch, dtype=torch.float32, device=device)))\n",
    "\n",
    "    def total_loss(self, preds, hidden, sensitive, y_true, epoch, tau=0.9):\n",
    "        \n",
    "        loss_qr = quantile_loss(y_true, preds, tau)\n",
    "        \n",
    "        # Projection\n",
    "        if self.use_projection:\n",
    "            bias_score = self.compute_bias_score(hidden, sensitive)\n",
    "            hidden = self.adaptive_projection(hidden, bias_score)\n",
    "        \n",
    "        # Fairness\n",
    "        if self.use_fairness:\n",
    "            fair_loss = self.distribution_aware_fairness_loss(preds, sensitive)  \n",
    "        else:\n",
    "            fair_loss =  torch.tensor(0.0, device=preds.device)\n",
    "\n",
    "        lambda_fair = self.hierarchical_lambda(epoch, max_lambda=self.lambda_fair)\n",
    "\n",
    "        return loss_qr + lambda_fair * fair_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_fair = 0.5\n",
    "all_dataset_results = {}\n",
    "\n",
    "for dataset in ['region_job', 'region_job_2', 'nba', 'german']:\n",
    "    seed = 1127\n",
    "    if dataset == 'nba':\n",
    "        predict_attr, sens_attr, label_number, sens_number, test_idx, path = nba_predict_attr, nba_sens_attr, nba_label_number, nba_sens_number, nba_test_idx, nba_path\n",
    "    elif dataset == 'german':\n",
    "        predict_attr, sens_attr, path = german_predict_attr, german_sens_attr, german_path\n",
    "    else:\n",
    "        predict_attr, sens_attr, label_number, sens_number, test_idx, path = pokec_predict_attr, pokec_sens_attr, pokec_label_number, pokec_sens_number, pokec_test_idx, pokec_path\n",
    "\n",
    "    adj, features, labels, idx_train, idx_val, idx_test, sens, idx_sens_train = load_dataset_unified(dataset, sens_attr, predict_attr, path, label_number, sens_number, test_idx, seed=seed)\n",
    "\n",
    "    if dataset == 'nba':\n",
    "        features = feature_norm(features)\n",
    "\n",
    "    if isinstance(adj, torch.Tensor):  # torch.sparse_coo_tensor\n",
    "        src, dst = adj._indices()\n",
    "        g = dgl.graph((src, dst))\n",
    "        edge_index = adj._indices()\n",
    "    else:  # scipy sparse matrix\n",
    "        g = dgl.from_scipy(adj)\n",
    "        adj_coo = adj.tocoo()\n",
    "        edge_index = torch.tensor([adj_coo.row, adj_coo.col], dtype=torch.long)\n",
    "\n",
    "    data = Data(x=features, edge_index=edge_index, y=labels.float(), sensitive_attr=sens, quantile_tau=torch.full((num_nodes,), tau))\n",
    "    data.idx_train = idx_train\n",
    "    data.idx_val = idx_val\n",
    "    data.idx_test = idx_test\n",
    "    data.idx_sens_train = idx_sens_train\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    dist_results, dist_all_metrics = multiple_runs_with_full_metrics(DAFGNN, data, device, n_runs, hidden_dim, n_epochs, tau, lambda_fair, reg=True)\n",
    "    # dist_results['Model'] = 'DAF-GNN'\n",
    "    all_results.append(dist_results)\n",
    "\n",
    "    # 결과 입력\n",
    "    all_dataset_results[dataset] = all_results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_means_raw = [model_result['QR Loss Mean'] for results in all_dataset_results.values() for model_result in results]\n",
    "qr_stds_raw = [model_result['QR Loss Std'] for results in all_dataset_results.values() for model_result in results]\n",
    "fair_means_raw = [model_result['Mean Gap Mean'] for results in all_dataset_results.values() for model_result in results]\n",
    "fair_stds_raw = [model_result['Mean Gap Std'] for results in all_dataset_results.values() for model_result in results]\n",
    "mae_means_raw = [model_result['MAE Mean'] for results in all_dataset_results.values() for model_result in results]\n",
    "mae_stds_raw = [model_result['MAE Std'] for results in all_dataset_results.values() for model_result in results]\n",
    "mae_var_means_raw = [model_result['MAE-Var Mean'] for results in all_dataset_results.values() for model_result in results]\n",
    "mae_var_stds_raw = [model_result['MAE-Var Std'] for results in all_dataset_results.values() for model_result in results]\n",
    "\n",
    "qr_means = [np.mean(qr_means_raw[i::5]) for i in range(5)]\n",
    "qr_stds = [np.mean(qr_stds_raw[i::5]) for i in range(5)]\n",
    "fair_means = [np.mean(fair_means_raw[i::5]) for i in range(5)]\n",
    "fair_stds = [np.mean(fair_stds_raw[i::5]) for i in range(5)]\n",
    "mae_means = [np.mean(mae_means_raw[i::5]) for i in range(5)]\n",
    "mae_stds = [np.mean(mae_stds_raw[i::5]) for i in range(5)]\n",
    "mae_var_means = [np.mean(mae_var_means_raw[i::5]) for i in range(5)]\n",
    "mae_var_stds = [np.mean(mae_var_stds_raw[i::5]) for i in range(5)]\n",
    "\n",
    "qr_gap = np.round(np.max(qr_means) - np.min(qr_means), 3)\n",
    "fair_gap = np.round(np.max(fair_means) - np.min(fair_means), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추가 실험"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lambda 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Lambda 0.0 ===\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 7.6071\n",
      "[Epoch 100] Loss: 0.8239\n",
      "[Epoch 200] Loss: 0.8123\n",
      "[Epoch 300] Loss: 0.7989\n",
      "[Epoch 400] Loss: 0.7734\n",
      "[Epoch 500] Loss: 0.7184\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 25.9505\n",
      "[Epoch 100] Loss: 0.8263\n",
      "[Epoch 200] Loss: 0.8060\n",
      "[Epoch 300] Loss: 0.7888\n",
      "[Epoch 400] Loss: 0.7265\n",
      "[Epoch 500] Loss: 0.7776\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 10.9024\n",
      "[Epoch 100] Loss: 0.8252\n",
      "[Epoch 200] Loss: 0.8108\n",
      "[Epoch 300] Loss: 0.7871\n",
      "[Epoch 400] Loss: 0.7388\n",
      "[Epoch 500] Loss: 0.8424\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 1.6837\n",
      "[Epoch 100] Loss: 0.8257\n",
      "[Epoch 200] Loss: 0.8005\n",
      "[Epoch 300] Loss: 0.7234\n",
      "[Epoch 400] Loss: 0.7498\n",
      "[Epoch 500] Loss: 0.7031\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 14.0569\n",
      "[Epoch 100] Loss: 0.8270\n",
      "[Epoch 200] Loss: 0.8087\n",
      "[Epoch 300] Loss: 0.8552\n",
      "[Epoch 400] Loss: 0.7530\n",
      "[Epoch 500] Loss: 0.7149\n",
      "\n",
      "=== Lambda 0.01 ===\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 5.1975\n",
      "[Epoch 100] Loss: 0.8319\n",
      "[Epoch 200] Loss: 0.8175\n",
      "[Epoch 300] Loss: 0.7871\n",
      "[Epoch 400] Loss: 0.7669\n",
      "[Epoch 500] Loss: 0.7851\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 8.9498\n",
      "[Epoch 100] Loss: 0.8336\n",
      "[Epoch 200] Loss: 0.8188\n",
      "[Epoch 300] Loss: 0.7925\n",
      "[Epoch 400] Loss: 0.7738\n",
      "[Epoch 500] Loss: 0.7731\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 5.6192\n",
      "[Epoch 100] Loss: 0.8346\n",
      "[Epoch 200] Loss: 0.8200\n",
      "[Epoch 300] Loss: 0.8033\n",
      "[Epoch 400] Loss: 0.7987\n",
      "[Epoch 500] Loss: 0.8145\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 0.8554\n",
      "[Epoch 100] Loss: 0.8369\n",
      "[Epoch 200] Loss: 0.8261\n",
      "[Epoch 300] Loss: 0.8045\n",
      "[Epoch 400] Loss: 0.7493\n",
      "[Epoch 500] Loss: 0.6757\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 9.7186\n",
      "[Epoch 100] Loss: 0.8373\n",
      "[Epoch 200] Loss: 0.7983\n",
      "[Epoch 300] Loss: 0.8397\n",
      "[Epoch 400] Loss: 0.7368\n",
      "[Epoch 500] Loss: 0.7432\n",
      "\n",
      "=== Lambda 0.05 ===\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 15.9223\n",
      "[Epoch 100] Loss: 0.8677\n",
      "[Epoch 200] Loss: 0.8442\n",
      "[Epoch 300] Loss: 0.8750\n",
      "[Epoch 400] Loss: 0.9518\n",
      "[Epoch 500] Loss: 0.7807\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 7.1610\n",
      "[Epoch 100] Loss: 0.8688\n",
      "[Epoch 200] Loss: 0.8226\n",
      "[Epoch 300] Loss: 0.7726\n",
      "[Epoch 400] Loss: 0.7799\n",
      "[Epoch 500] Loss: 0.7588\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 5.1636\n",
      "[Epoch 100] Loss: 0.8739\n",
      "[Epoch 200] Loss: 0.7824\n",
      "[Epoch 300] Loss: 0.8063\n",
      "[Epoch 400] Loss: 0.7711\n",
      "[Epoch 500] Loss: 0.7568\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 10.8755\n",
      "[Epoch 100] Loss: 0.8729\n",
      "[Epoch 200] Loss: 0.8540\n",
      "[Epoch 300] Loss: 0.8267\n",
      "[Epoch 400] Loss: 0.8092\n",
      "[Epoch 500] Loss: 0.7922\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 13.8633\n",
      "[Epoch 100] Loss: 0.8739\n",
      "[Epoch 200] Loss: 0.8486\n",
      "[Epoch 300] Loss: 0.7898\n",
      "[Epoch 400] Loss: 0.8034\n",
      "[Epoch 500] Loss: 0.7685\n",
      "\n",
      "=== Lambda 0.1 ===\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 6.4272\n",
      "[Epoch 100] Loss: 0.9169\n",
      "[Epoch 200] Loss: 0.8543\n",
      "[Epoch 300] Loss: 0.8288\n",
      "[Epoch 400] Loss: 0.7799\n",
      "[Epoch 500] Loss: 0.7368\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 17.6193\n",
      "[Epoch 100] Loss: 0.9163\n",
      "[Epoch 200] Loss: 0.8172\n",
      "[Epoch 300] Loss: 0.7837\n",
      "[Epoch 400] Loss: 0.7453\n",
      "[Epoch 500] Loss: 0.7125\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 14.2565\n",
      "[Epoch 100] Loss: 0.9285\n",
      "[Epoch 200] Loss: 0.9087\n",
      "[Epoch 300] Loss: 0.9198\n",
      "[Epoch 400] Loss: 0.8746\n",
      "[Epoch 500] Loss: 0.8347\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 0.9017\n",
      "[Epoch 100] Loss: 0.9248\n",
      "[Epoch 200] Loss: 0.8897\n",
      "[Epoch 300] Loss: 0.8033\n",
      "[Epoch 400] Loss: 0.7444\n",
      "[Epoch 500] Loss: 0.7820\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 7.3153\n",
      "[Epoch 100] Loss: 0.9170\n",
      "[Epoch 200] Loss: 0.8985\n",
      "[Epoch 300] Loss: 0.8397\n",
      "[Epoch 400] Loss: 0.7568\n",
      "[Epoch 500] Loss: 0.7344\n",
      "\n",
      "=== Lambda 0.5 ===\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 1.1735\n",
      "[Epoch 100] Loss: 1.1670\n",
      "[Epoch 200] Loss: 0.9060\n",
      "[Epoch 300] Loss: 0.9154\n",
      "[Epoch 400] Loss: 0.7897\n",
      "[Epoch 500] Loss: 0.7698\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 10.6762\n",
      "[Epoch 100] Loss: 1.0917\n",
      "[Epoch 200] Loss: 0.8803\n",
      "[Epoch 300] Loss: 0.8406\n",
      "[Epoch 400] Loss: 0.8331\n",
      "[Epoch 500] Loss: 0.7989\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 10.1462\n",
      "[Epoch 100] Loss: 0.9289\n",
      "[Epoch 200] Loss: 0.8496\n",
      "[Epoch 300] Loss: 0.9381\n",
      "[Epoch 400] Loss: 0.9212\n",
      "[Epoch 500] Loss: 0.8557\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 7.2123\n",
      "[Epoch 100] Loss: 0.8881\n",
      "[Epoch 200] Loss: 0.8329\n",
      "[Epoch 300] Loss: 0.8125\n",
      "[Epoch 400] Loss: 0.7928\n",
      "[Epoch 500] Loss: 0.7929\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 0.8608\n",
      "[Epoch 100] Loss: 1.4113\n",
      "[Epoch 200] Loss: 0.8185\n",
      "[Epoch 300] Loss: 0.8152\n",
      "[Epoch 400] Loss: 0.9241\n",
      "[Epoch 500] Loss: 0.7402\n",
      "\n",
      "=== Lambda 1.0 ===\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 10.3666\n",
      "[Epoch 100] Loss: 0.9844\n",
      "[Epoch 200] Loss: 0.8627\n",
      "[Epoch 300] Loss: 0.8443\n",
      "[Epoch 400] Loss: 0.8146\n",
      "[Epoch 500] Loss: 0.8201\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 7.5563\n",
      "[Epoch 100] Loss: 0.8563\n",
      "[Epoch 200] Loss: 0.9830\n",
      "[Epoch 300] Loss: 0.9683\n",
      "[Epoch 400] Loss: 0.8333\n",
      "[Epoch 500] Loss: 0.9738\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 1.5425\n",
      "[Epoch 100] Loss: 0.9346\n",
      "[Epoch 200] Loss: 1.1294\n",
      "[Epoch 300] Loss: 0.9368\n",
      "[Epoch 400] Loss: 0.9594\n",
      "[Epoch 500] Loss: 0.7823\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 7.9581\n",
      "[Epoch 100] Loss: 0.8695\n",
      "[Epoch 200] Loss: 0.9332\n",
      "[Epoch 300] Loss: 0.8537\n",
      "[Epoch 400] Loss: 0.9646\n",
      "[Epoch 500] Loss: 0.8620\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 10.0401\n",
      "[Epoch 100] Loss: 1.0518\n",
      "[Epoch 200] Loss: 0.8455\n",
      "[Epoch 300] Loss: 0.9062\n",
      "[Epoch 400] Loss: 0.8105\n",
      "[Epoch 500] Loss: 0.8304\n",
      "\n",
      "=== Lambda 5.0 ===\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 17.4270\n",
      "[Epoch 100] Loss: 1.7104\n",
      "[Epoch 200] Loss: 1.0621\n",
      "[Epoch 300] Loss: 1.3018\n",
      "[Epoch 400] Loss: 0.9824\n",
      "[Epoch 500] Loss: 0.9851\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 10.4100\n",
      "[Epoch 100] Loss: 1.2188\n",
      "[Epoch 200] Loss: 2.7423\n",
      "[Epoch 300] Loss: 1.1770\n",
      "[Epoch 400] Loss: 1.5591\n",
      "[Epoch 500] Loss: 1.1210\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 7.4259\n",
      "[Epoch 100] Loss: 1.3333\n",
      "[Epoch 200] Loss: 0.9480\n",
      "[Epoch 300] Loss: 1.1725\n",
      "[Epoch 400] Loss: 0.9870\n",
      "[Epoch 500] Loss: 0.8620\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 2.0858\n",
      "[Epoch 100] Loss: 1.4860\n",
      "[Epoch 200] Loss: 0.8937\n",
      "[Epoch 300] Loss: 0.9973\n",
      "[Epoch 400] Loss: 0.8433\n",
      "[Epoch 500] Loss: 0.8898\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 20.5572\n",
      "[Epoch 100] Loss: 1.0385\n",
      "[Epoch 200] Loss: 0.9690\n",
      "[Epoch 300] Loss: 0.9705\n",
      "[Epoch 400] Loss: 1.2153\n",
      "[Epoch 500] Loss: 0.9137\n",
      "\n",
      "=== Lambda 10.0 ===\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 1/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 2.1606\n",
      "[Epoch 100] Loss: 2.4664\n",
      "[Epoch 200] Loss: 1.5427\n",
      "[Epoch 300] Loss: 1.3961\n",
      "[Epoch 400] Loss: 1.1046\n",
      "[Epoch 500] Loss: 0.8258\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 2/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 1.4451\n",
      "[Epoch 100] Loss: 2.3832\n",
      "[Epoch 200] Loss: 1.4030\n",
      "[Epoch 300] Loss: 0.8974\n",
      "[Epoch 400] Loss: 1.5264\n",
      "[Epoch 500] Loss: 1.4358\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 3/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 3.2141\n",
      "[Epoch 100] Loss: 1.4520\n",
      "[Epoch 200] Loss: 1.2688\n",
      "[Epoch 300] Loss: 0.9390\n",
      "[Epoch 400] Loss: 1.7474\n",
      "[Epoch 500] Loss: 0.9146\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 4/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 18.6268\n",
      "[Epoch 100] Loss: 1.3186\n",
      "[Epoch 200] Loss: 1.7870\n",
      "[Epoch 300] Loss: 1.1878\n",
      "[Epoch 400] Loss: 1.1006\n",
      "[Epoch 500] Loss: 0.8918\n",
      "\n",
      " [DistributionAwareFairGNN] --- Run 5/5 ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Loss: 16.6513\n",
      "[Epoch 100] Loss: 1.0610\n",
      "[Epoch 200] Loss: 1.2034\n",
      "[Epoch 300] Loss: 2.0372\n",
      "[Epoch 400] Loss: 0.8864\n",
      "[Epoch 500] Loss: 2.4953\n"
     ]
    }
   ],
   "source": [
    "# lambda_fair Trade-off 실험\n",
    "lambdas = [0.0, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0]\n",
    "results = []\n",
    "\n",
    "for lambda_fair in lambdas:\n",
    "    print(f\"\\n=== Lambda {lambda_fair} ===\")\n",
    "    fair_results, _ = multiple_runs_with_full_metrics(DistributionAwareFairGNN, data, device, n_runs, hidden_dim, n_epochs, tau, lambda_fair)\n",
    "\n",
    "    fair_results['Lambda'] = lambda_fair\n",
    "    results.append(fair_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QR Loss Mean</th>\n",
       "      <th>QR Loss Std</th>\n",
       "      <th>MAE Mean</th>\n",
       "      <th>MAE Std</th>\n",
       "      <th>RMSE Mean</th>\n",
       "      <th>RMSE Std</th>\n",
       "      <th>R2 Mean</th>\n",
       "      <th>R2 Std</th>\n",
       "      <th>Fairness Gap Mean</th>\n",
       "      <th>Fairness Gap Std</th>\n",
       "      <th>Lambda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6398</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>5.0347</td>\n",
       "      <td>1.0447</td>\n",
       "      <td>5.9431</td>\n",
       "      <td>1.0952</td>\n",
       "      <td>-2.2302</td>\n",
       "      <td>1.0297</td>\n",
       "      <td>0.7486</td>\n",
       "      <td>0.0783</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6177</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>4.8036</td>\n",
       "      <td>0.9577</td>\n",
       "      <td>5.6771</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>-1.9341</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>0.6464</td>\n",
       "      <td>0.0794</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6471</td>\n",
       "      <td>0.1212</td>\n",
       "      <td>5.6536</td>\n",
       "      <td>1.4627</td>\n",
       "      <td>6.4156</td>\n",
       "      <td>1.4446</td>\n",
       "      <td>-2.8251</td>\n",
       "      <td>1.9108</td>\n",
       "      <td>0.4309</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5897</td>\n",
       "      <td>0.0279</td>\n",
       "      <td>4.5419</td>\n",
       "      <td>0.5585</td>\n",
       "      <td>5.3314</td>\n",
       "      <td>0.6264</td>\n",
       "      <td>-1.5488</td>\n",
       "      <td>0.5855</td>\n",
       "      <td>0.2008</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6132</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>4.6082</td>\n",
       "      <td>0.6055</td>\n",
       "      <td>5.4219</td>\n",
       "      <td>0.6064</td>\n",
       "      <td>-1.6327</td>\n",
       "      <td>0.5944</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6652</td>\n",
       "      <td>0.0372</td>\n",
       "      <td>5.2318</td>\n",
       "      <td>0.9125</td>\n",
       "      <td>6.0888</td>\n",
       "      <td>0.9047</td>\n",
       "      <td>-2.3515</td>\n",
       "      <td>0.9695</td>\n",
       "      <td>0.1166</td>\n",
       "      <td>0.0790</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.6584</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>5.2408</td>\n",
       "      <td>0.3457</td>\n",
       "      <td>6.1462</td>\n",
       "      <td>0.3360</td>\n",
       "      <td>-2.3513</td>\n",
       "      <td>0.3651</td>\n",
       "      <td>0.1804</td>\n",
       "      <td>0.0267</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.6399</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>4.9940</td>\n",
       "      <td>0.5115</td>\n",
       "      <td>5.8484</td>\n",
       "      <td>0.5442</td>\n",
       "      <td>-2.0516</td>\n",
       "      <td>0.5335</td>\n",
       "      <td>0.1825</td>\n",
       "      <td>0.0573</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   QR Loss Mean  QR Loss Std  MAE Mean  MAE Std  RMSE Mean  RMSE Std  R2 Mean  \\\n",
       "0        0.6398       0.0300    5.0347   1.0447     5.9431    1.0952  -2.2302   \n",
       "1        0.6177       0.0369    4.8036   0.9577     5.6771    0.9710  -1.9341   \n",
       "2        0.6471       0.1212    5.6536   1.4627     6.4156    1.4446  -2.8251   \n",
       "3        0.5897       0.0279    4.5419   0.5585     5.3314    0.6264  -1.5488   \n",
       "4        0.6132       0.0221    4.6082   0.6055     5.4219    0.6064  -1.6327   \n",
       "5        0.6652       0.0372    5.2318   0.9125     6.0888    0.9047  -2.3515   \n",
       "6        0.6584       0.0141    5.2408   0.3457     6.1462    0.3360  -2.3513   \n",
       "7        0.6399       0.0165    4.9940   0.5115     5.8484    0.5442  -2.0516   \n",
       "\n",
       "   R2 Std  Fairness Gap Mean  Fairness Gap Std  Lambda  \n",
       "0  1.0297             0.7486            0.0783    0.00  \n",
       "1  0.9896             0.6464            0.0794    0.01  \n",
       "2  1.9108             0.4309            0.1394    0.05  \n",
       "3  0.5855             0.2008            0.1480    0.10  \n",
       "4  0.5944             0.0564            0.0295    0.50  \n",
       "5  0.9695             0.1166            0.0790    1.00  \n",
       "6  0.3651             0.1804            0.0267    5.00  \n",
       "7  0.5335             0.1825            0.0573   10.00  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lambda = pd.DataFrame(results)\n",
    "df_lambda.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 시각화\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(df_lambda['Lambda'], df_lambda['QR Loss Mean'], marker='o', label='QR Loss (↓)')\n",
    "plt.plot(df_lambda['Lambda'], df_lambda['Fairness Gap Mean'], marker='o', label='Fairness Gap (↓)')\n",
    "plt.xscale('log')  # λ는 보통 로그스케일로 표현\n",
    "plt.xlabel('Lambda Fair (log scale)')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Trade-off between Fairness and Performance by Lambda')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ablation 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading region_job dataset from ./dataset/pokec\n",
      "[INFO] 유효하지 않은 user_id로 인해 제거된 edge 수: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "region_job\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[Full] Run 1/5\n",
      "[Epoch 1] Loss: 38.7758\n",
      "[Epoch 100] Loss: 8.4558\n",
      "[Epoch 200] Loss: 6.4128\n",
      "[Epoch 300] Loss: 6.4133\n",
      "[Epoch 400] Loss: 4.8929\n",
      "[Epoch 500] Loss: 4.5515\n",
      "\n",
      "[Full] Run 2/5\n",
      "[Epoch 1] Loss: 38.8719\n",
      "[Epoch 100] Loss: 7.9785\n",
      "[Epoch 200] Loss: 5.2511\n",
      "[Epoch 300] Loss: 6.5498\n",
      "[Epoch 400] Loss: 5.5234\n",
      "[Epoch 500] Loss: 4.2117\n",
      "\n",
      "[Full] Run 3/5\n",
      "[Epoch 1] Loss: 39.1599\n",
      "[Epoch 100] Loss: 6.2895\n",
      "[Epoch 200] Loss: 7.1458\n",
      "[Epoch 300] Loss: 5.5243\n",
      "[Epoch 400] Loss: 4.6411\n",
      "[Epoch 500] Loss: 4.1230\n",
      "\n",
      "[Full] Run 4/5\n",
      "[Epoch 1] Loss: 39.1481\n",
      "[Epoch 100] Loss: 6.8560\n",
      "[Epoch 200] Loss: 5.3702\n",
      "[Epoch 300] Loss: 5.9302\n",
      "[Epoch 400] Loss: 4.6596\n",
      "[Epoch 500] Loss: 5.4981\n",
      "\n",
      "[Full] Run 5/5\n",
      "[Epoch 1] Loss: 39.0324\n",
      "[Epoch 100] Loss: 8.7846\n",
      "[Epoch 200] Loss: 5.8817\n",
      "[Epoch 300] Loss: 5.1488\n",
      "[Epoch 400] Loss: 6.0887\n",
      "[Epoch 500] Loss: 4.1256\n",
      "\n",
      "[w/o Projection] Run 1/5\n",
      "[Epoch 1] Loss: 39.4157\n",
      "[Epoch 100] Loss: 8.2104\n",
      "[Epoch 200] Loss: 7.3163\n",
      "[Epoch 300] Loss: 6.2746\n",
      "[Epoch 400] Loss: 4.5343\n",
      "[Epoch 500] Loss: 4.0593\n",
      "\n",
      "[w/o Projection] Run 2/5\n",
      "[Epoch 1] Loss: 38.7403\n",
      "[Epoch 100] Loss: 6.6229\n",
      "[Epoch 200] Loss: 5.2569\n",
      "[Epoch 300] Loss: 4.9598\n",
      "[Epoch 400] Loss: 5.4127\n",
      "[Epoch 500] Loss: 5.7615\n",
      "\n",
      "[w/o Projection] Run 3/5\n",
      "[Epoch 1] Loss: 39.2467\n",
      "[Epoch 100] Loss: 6.4683\n",
      "[Epoch 200] Loss: 5.4946\n",
      "[Epoch 300] Loss: 5.7285\n",
      "[Epoch 400] Loss: 4.8031\n",
      "[Epoch 500] Loss: 4.4427\n",
      "\n",
      "[w/o Projection] Run 4/5\n",
      "[Epoch 1] Loss: 38.8404\n",
      "[Epoch 100] Loss: 6.6056\n",
      "[Epoch 200] Loss: 5.6493\n",
      "[Epoch 300] Loss: 4.7402\n",
      "[Epoch 400] Loss: 4.4386\n",
      "[Epoch 500] Loss: 4.3296\n",
      "\n",
      "[w/o Projection] Run 5/5\n",
      "[Epoch 1] Loss: 39.1279\n",
      "[Epoch 100] Loss: 9.1741\n",
      "[Epoch 200] Loss: 6.4599\n",
      "[Epoch 300] Loss: 5.1968\n",
      "[Epoch 400] Loss: 4.9622\n",
      "[Epoch 500] Loss: 3.8843\n",
      "\n",
      "[w/o Fairness] Run 1/5\n",
      "[Epoch 1] Loss: 39.2981\n",
      "[Epoch 100] Loss: 4.2036\n",
      "[Epoch 200] Loss: 3.1745\n",
      "[Epoch 300] Loss: 2.6782\n",
      "[Epoch 400] Loss: 2.5348\n",
      "[Epoch 500] Loss: 2.4962\n",
      "\n",
      "[w/o Fairness] Run 2/5\n",
      "[Epoch 1] Loss: 38.7681\n",
      "[Epoch 100] Loss: 4.5347\n",
      "[Epoch 200] Loss: 3.8143\n",
      "[Epoch 300] Loss: 2.8302\n",
      "[Epoch 400] Loss: 2.3986\n",
      "[Epoch 500] Loss: 2.2736\n",
      "\n",
      "[w/o Fairness] Run 3/5\n",
      "[Epoch 1] Loss: 39.4090\n",
      "[Epoch 100] Loss: 4.5592\n",
      "[Epoch 200] Loss: 3.1823\n",
      "[Epoch 300] Loss: 2.4477\n",
      "[Epoch 400] Loss: 2.2230\n",
      "[Epoch 500] Loss: 2.2062\n",
      "\n",
      "[w/o Fairness] Run 4/5\n",
      "[Epoch 1] Loss: 39.1242\n",
      "[Epoch 100] Loss: 4.6037\n",
      "[Epoch 200] Loss: 3.9548\n",
      "[Epoch 300] Loss: 3.0959\n",
      "[Epoch 400] Loss: 2.4725\n",
      "[Epoch 500] Loss: 2.3617\n",
      "\n",
      "[w/o Fairness] Run 5/5\n",
      "[Epoch 1] Loss: 39.0194\n",
      "[Epoch 100] Loss: 4.5223\n",
      "[Epoch 200] Loss: 3.8077\n",
      "[Epoch 300] Loss: 2.8302\n",
      "[Epoch 400] Loss: 2.3764\n",
      "[Epoch 500] Loss: 2.1500\n",
      "\n",
      "[w/o Lambda] Run 1/5\n",
      "[Epoch 1] Loss: 38.8554\n",
      "[Epoch 100] Loss: 7.1416\n",
      "[Epoch 200] Loss: 6.5002\n",
      "[Epoch 300] Loss: 5.7379\n",
      "[Epoch 400] Loss: 4.2672\n",
      "[Epoch 500] Loss: 4.0090\n",
      "\n",
      "[w/o Lambda] Run 2/5\n",
      "[Epoch 1] Loss: 38.5076\n",
      "[Epoch 100] Loss: 7.0998\n",
      "[Epoch 200] Loss: 5.3149\n",
      "[Epoch 300] Loss: 5.1737\n",
      "[Epoch 400] Loss: 5.6323\n",
      "[Epoch 500] Loss: 4.1714\n",
      "\n",
      "[w/o Lambda] Run 3/5\n",
      "[Epoch 1] Loss: 39.4356\n",
      "[Epoch 100] Loss: 8.5718\n",
      "[Epoch 200] Loss: 5.9637\n",
      "[Epoch 300] Loss: 5.2859\n",
      "[Epoch 400] Loss: 5.0004\n",
      "[Epoch 500] Loss: 4.6492\n",
      "\n",
      "[w/o Lambda] Run 4/5\n",
      "[Epoch 1] Loss: 38.7898\n",
      "[Epoch 100] Loss: 6.2003\n",
      "[Epoch 200] Loss: 5.4937\n",
      "[Epoch 300] Loss: 5.9651\n",
      "[Epoch 400] Loss: 4.2781\n",
      "[Epoch 500] Loss: 4.3974\n",
      "\n",
      "[w/o Lambda] Run 5/5\n",
      "[Epoch 1] Loss: 38.9790\n",
      "[Epoch 100] Loss: 7.8583\n",
      "[Epoch 200] Loss: 7.2573\n",
      "[Epoch 300] Loss: 7.8945\n",
      "[Epoch 400] Loss: 4.7461\n",
      "[Epoch 500] Loss: 4.5697\n",
      "\n",
      "[w/o Projection, Fairness] Run 1/5\n",
      "[Epoch 1] Loss: 38.4852\n",
      "[Epoch 100] Loss: 4.2785\n",
      "[Epoch 200] Loss: 3.2675\n",
      "[Epoch 300] Loss: 2.6664\n",
      "[Epoch 400] Loss: 2.5400\n",
      "[Epoch 500] Loss: 2.1243\n",
      "\n",
      "[w/o Projection, Fairness] Run 2/5\n",
      "[Epoch 1] Loss: 39.0936\n",
      "[Epoch 100] Loss: 4.3449\n",
      "[Epoch 200] Loss: 3.3021\n",
      "[Epoch 300] Loss: 2.6837\n",
      "[Epoch 400] Loss: 2.4969\n",
      "[Epoch 500] Loss: 2.1650\n",
      "\n",
      "[w/o Projection, Fairness] Run 3/5\n",
      "[Epoch 1] Loss: 39.1978\n",
      "[Epoch 100] Loss: 4.4062\n",
      "[Epoch 200] Loss: 3.0656\n",
      "[Epoch 300] Loss: 2.6273\n",
      "[Epoch 400] Loss: 2.4397\n",
      "[Epoch 500] Loss: 2.2614\n",
      "\n",
      "[w/o Projection, Fairness] Run 4/5\n",
      "[Epoch 1] Loss: 39.5251\n",
      "[Epoch 100] Loss: 4.3317\n",
      "[Epoch 200] Loss: 3.3477\n",
      "[Epoch 300] Loss: 2.7935\n",
      "[Epoch 400] Loss: 2.4892\n",
      "[Epoch 500] Loss: 2.2591\n",
      "\n",
      "[w/o Projection, Fairness] Run 5/5\n",
      "[Epoch 1] Loss: 39.3173\n",
      "[Epoch 100] Loss: 4.5633\n",
      "[Epoch 200] Loss: 3.8903\n",
      "[Epoch 300] Loss: 2.9618\n",
      "[Epoch 400] Loss: 2.4068\n",
      "[Epoch 500] Loss: 2.2170\n",
      "\n",
      "[w/o Projection, Lambda] Run 1/5\n",
      "[Epoch 1] Loss: 39.0809\n",
      "[Epoch 100] Loss: 8.4822\n",
      "[Epoch 200] Loss: 5.4032\n",
      "[Epoch 300] Loss: 7.6547\n",
      "[Epoch 400] Loss: 4.3256\n",
      "[Epoch 500] Loss: 4.5520\n",
      "\n",
      "[w/o Projection, Lambda] Run 2/5\n",
      "[Epoch 1] Loss: 39.0260\n",
      "[Epoch 100] Loss: 7.6437\n",
      "[Epoch 200] Loss: 5.3870\n",
      "[Epoch 300] Loss: 4.9031\n",
      "[Epoch 400] Loss: 5.1610\n",
      "[Epoch 500] Loss: 3.9943\n",
      "\n",
      "[w/o Projection, Lambda] Run 3/5\n",
      "[Epoch 1] Loss: 38.3487\n",
      "[Epoch 100] Loss: 5.7362\n",
      "[Epoch 200] Loss: 6.8015\n",
      "[Epoch 300] Loss: 4.5501\n",
      "[Epoch 400] Loss: 4.2272\n",
      "[Epoch 500] Loss: 4.1030\n",
      "\n",
      "[w/o Projection, Lambda] Run 4/5\n",
      "[Epoch 1] Loss: 38.9336\n",
      "[Epoch 100] Loss: 7.6526\n",
      "[Epoch 200] Loss: 8.3244\n",
      "[Epoch 300] Loss: 7.0034\n",
      "[Epoch 400] Loss: 5.2207\n",
      "[Epoch 500] Loss: 4.1177\n",
      "\n",
      "[w/o Projection, Lambda] Run 5/5\n",
      "[Epoch 1] Loss: 38.9911\n",
      "[Epoch 100] Loss: 6.7017\n",
      "[Epoch 200] Loss: 5.7536\n",
      "[Epoch 300] Loss: 5.7381\n",
      "[Epoch 400] Loss: 4.4059\n",
      "[Epoch 500] Loss: 3.9537\n",
      "\n",
      "[w/o Fairness, Lambda] Run 1/5\n",
      "[Epoch 1] Loss: 38.9008\n",
      "[Epoch 100] Loss: 4.5464\n",
      "[Epoch 200] Loss: 3.4014\n",
      "[Epoch 300] Loss: 2.5652\n",
      "[Epoch 400] Loss: 2.4516\n",
      "[Epoch 500] Loss: 2.2330\n",
      "\n",
      "[w/o Fairness, Lambda] Run 2/5\n",
      "[Epoch 1] Loss: 39.4172\n",
      "[Epoch 100] Loss: 4.1242\n",
      "[Epoch 200] Loss: 3.0387\n",
      "[Epoch 300] Loss: 2.7905\n",
      "[Epoch 400] Loss: 2.3718\n",
      "[Epoch 500] Loss: 2.1299\n",
      "\n",
      "[w/o Fairness, Lambda] Run 3/5\n",
      "[Epoch 1] Loss: 38.5215\n",
      "[Epoch 100] Loss: 4.1875\n",
      "[Epoch 200] Loss: 2.8200\n",
      "[Epoch 300] Loss: 2.4730\n",
      "[Epoch 400] Loss: 2.4195\n",
      "[Epoch 500] Loss: 1.9790\n",
      "\n",
      "[w/o Fairness, Lambda] Run 4/5\n",
      "[Epoch 1] Loss: 38.8906\n",
      "[Epoch 100] Loss: 4.2034\n",
      "[Epoch 200] Loss: 3.1330\n",
      "[Epoch 300] Loss: 2.6064\n",
      "[Epoch 400] Loss: 2.5093\n",
      "[Epoch 500] Loss: 2.1874\n",
      "\n",
      "[w/o Fairness, Lambda] Run 5/5\n",
      "[Epoch 1] Loss: 39.0039\n",
      "[Epoch 100] Loss: 4.5184\n",
      "[Epoch 200] Loss: 3.8269\n",
      "[Epoch 300] Loss: 2.8634\n",
      "[Epoch 400] Loss: 2.6657\n",
      "[Epoch 500] Loss: 2.1961\n",
      "\n",
      "[w/o All] Run 1/5\n",
      "[Epoch 1] Loss: 38.6104\n",
      "[Epoch 100] Loss: 4.4452\n",
      "[Epoch 200] Loss: 3.1996\n",
      "[Epoch 300] Loss: 2.6746\n",
      "[Epoch 400] Loss: 2.3500\n",
      "[Epoch 500] Loss: 2.4389\n",
      "\n",
      "[w/o All] Run 2/5\n",
      "[Epoch 1] Loss: 39.1448\n",
      "[Epoch 100] Loss: 4.4107\n",
      "[Epoch 200] Loss: 3.3536\n",
      "[Epoch 300] Loss: 2.5472\n",
      "[Epoch 400] Loss: 2.5713\n",
      "[Epoch 500] Loss: 2.2881\n",
      "\n",
      "[w/o All] Run 3/5\n",
      "[Epoch 1] Loss: 39.0496\n",
      "[Epoch 100] Loss: 4.4786\n",
      "[Epoch 200] Loss: 3.5246\n",
      "[Epoch 300] Loss: 2.6838\n",
      "[Epoch 400] Loss: 2.3304\n",
      "[Epoch 500] Loss: 2.2136\n",
      "\n",
      "[w/o All] Run 4/5\n",
      "[Epoch 1] Loss: 39.0225\n",
      "[Epoch 100] Loss: 4.5195\n",
      "[Epoch 200] Loss: 3.5293\n",
      "[Epoch 300] Loss: 2.8365\n",
      "[Epoch 400] Loss: 2.2527\n",
      "[Epoch 500] Loss: 2.0516\n",
      "\n",
      "[w/o All] Run 5/5\n",
      "[Epoch 1] Loss: 39.0754\n",
      "[Epoch 100] Loss: 4.4246\n",
      "[Epoch 200] Loss: 3.5244\n",
      "[Epoch 300] Loss: 2.7979\n",
      "[Epoch 400] Loss: 2.4720\n",
      "[Epoch 500] Loss: 2.1828\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loading region_job_2 dataset from ./dataset/pokec\n",
      "[INFO] 유효하지 않은 user_id로 인해 제거된 edge 수: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "region_job_2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[Full] Run 1/5\n",
      "[Epoch 1] Loss: 37.0325\n",
      "[Epoch 100] Loss: 9.8575\n",
      "[Epoch 200] Loss: 7.1022\n",
      "[Epoch 300] Loss: 5.0857\n",
      "[Epoch 400] Loss: 3.9214\n",
      "[Epoch 500] Loss: 3.4557\n",
      "\n",
      "[Full] Run 2/5\n",
      "[Epoch 1] Loss: 37.8744\n",
      "[Epoch 100] Loss: 9.1738\n",
      "[Epoch 200] Loss: 8.1244\n",
      "[Epoch 300] Loss: 4.9144\n",
      "[Epoch 400] Loss: 3.9936\n",
      "[Epoch 500] Loss: 3.4096\n",
      "\n",
      "[Full] Run 3/5\n",
      "[Epoch 1] Loss: 37.0657\n",
      "[Epoch 100] Loss: 8.8418\n",
      "[Epoch 200] Loss: 7.8034\n",
      "[Epoch 300] Loss: 4.3769\n",
      "[Epoch 400] Loss: 4.3751\n",
      "[Epoch 500] Loss: 3.5015\n",
      "\n",
      "[Full] Run 4/5\n",
      "[Epoch 1] Loss: 37.5212\n",
      "[Epoch 100] Loss: 9.5158\n",
      "[Epoch 200] Loss: 6.6446\n",
      "[Epoch 300] Loss: 4.5669\n",
      "[Epoch 400] Loss: 3.7207\n",
      "[Epoch 500] Loss: 3.2997\n",
      "\n",
      "[Full] Run 5/5\n",
      "[Epoch 1] Loss: 37.3672\n",
      "[Epoch 100] Loss: 9.1411\n",
      "[Epoch 200] Loss: 7.3579\n",
      "[Epoch 300] Loss: 4.8408\n",
      "[Epoch 400] Loss: 3.8818\n",
      "[Epoch 500] Loss: 3.2758\n",
      "\n",
      "[w/o Projection] Run 1/5\n",
      "[Epoch 1] Loss: 37.6148\n",
      "[Epoch 100] Loss: 9.8889\n",
      "[Epoch 200] Loss: 8.5968\n",
      "[Epoch 300] Loss: 7.4809\n",
      "[Epoch 400] Loss: 4.0533\n",
      "[Epoch 500] Loss: 3.5143\n",
      "\n",
      "[w/o Projection] Run 2/5\n",
      "[Epoch 1] Loss: 37.6947\n",
      "[Epoch 100] Loss: 8.4044\n",
      "[Epoch 200] Loss: 7.7460\n",
      "[Epoch 300] Loss: 7.5983\n",
      "[Epoch 400] Loss: 4.2059\n",
      "[Epoch 500] Loss: 3.2939\n",
      "\n",
      "[w/o Projection] Run 3/5\n",
      "[Epoch 1] Loss: 36.8450\n",
      "[Epoch 100] Loss: 8.4580\n",
      "[Epoch 200] Loss: 7.7606\n",
      "[Epoch 300] Loss: 5.0555\n",
      "[Epoch 400] Loss: 4.1461\n",
      "[Epoch 500] Loss: 3.3921\n",
      "\n",
      "[w/o Projection] Run 4/5\n",
      "[Epoch 1] Loss: 36.7253\n",
      "[Epoch 100] Loss: 8.9600\n",
      "[Epoch 200] Loss: 7.2098\n",
      "[Epoch 300] Loss: 5.0483\n",
      "[Epoch 400] Loss: 3.8378\n",
      "[Epoch 500] Loss: 3.3264\n",
      "\n",
      "[w/o Projection] Run 5/5\n",
      "[Epoch 1] Loss: 36.8096\n",
      "[Epoch 100] Loss: 8.3742\n",
      "[Epoch 200] Loss: 7.1976\n",
      "[Epoch 300] Loss: 5.6692\n",
      "[Epoch 400] Loss: 4.0446\n",
      "[Epoch 500] Loss: 3.1868\n",
      "\n",
      "[w/o Fairness] Run 1/5\n",
      "[Epoch 1] Loss: 36.9017\n",
      "[Epoch 100] Loss: 4.0294\n",
      "[Epoch 200] Loss: 3.3023\n",
      "[Epoch 300] Loss: 2.7775\n",
      "[Epoch 400] Loss: 2.5240\n",
      "[Epoch 500] Loss: 2.3866\n",
      "\n",
      "[w/o Fairness] Run 2/5\n",
      "[Epoch 1] Loss: 37.4912\n",
      "[Epoch 100] Loss: 3.7064\n",
      "[Epoch 200] Loss: 2.9182\n",
      "[Epoch 300] Loss: 2.8131\n",
      "[Epoch 400] Loss: 2.4235\n",
      "[Epoch 500] Loss: 2.2585\n",
      "\n",
      "[w/o Fairness] Run 3/5\n",
      "[Epoch 1] Loss: 37.4894\n",
      "[Epoch 100] Loss: 3.8139\n",
      "[Epoch 200] Loss: 3.0654\n",
      "[Epoch 300] Loss: 2.7550\n",
      "[Epoch 400] Loss: 2.6530\n",
      "[Epoch 500] Loss: 2.3664\n",
      "\n",
      "[w/o Fairness] Run 4/5\n",
      "[Epoch 1] Loss: 37.2638\n",
      "[Epoch 100] Loss: 3.9924\n",
      "[Epoch 200] Loss: 2.9564\n",
      "[Epoch 300] Loss: 2.4440\n",
      "[Epoch 400] Loss: 2.4182\n",
      "[Epoch 500] Loss: 2.4074\n",
      "\n",
      "[w/o Fairness] Run 5/5\n",
      "[Epoch 1] Loss: 37.0129\n",
      "[Epoch 100] Loss: 4.0335\n",
      "[Epoch 200] Loss: 3.1432\n",
      "[Epoch 300] Loss: 2.6581\n",
      "[Epoch 400] Loss: 2.3951\n",
      "[Epoch 500] Loss: 2.1727\n",
      "\n",
      "[w/o Lambda] Run 1/5\n",
      "[Epoch 1] Loss: 37.3974\n",
      "[Epoch 100] Loss: 9.9120\n",
      "[Epoch 200] Loss: 7.5779\n",
      "[Epoch 300] Loss: 4.4146\n",
      "[Epoch 400] Loss: 3.5407\n",
      "[Epoch 500] Loss: 3.2020\n",
      "\n",
      "[w/o Lambda] Run 2/5\n",
      "[Epoch 1] Loss: 37.4254\n",
      "[Epoch 100] Loss: 9.3628\n",
      "[Epoch 200] Loss: 7.6096\n",
      "[Epoch 300] Loss: 6.8498\n",
      "[Epoch 400] Loss: 3.9968\n",
      "[Epoch 500] Loss: 3.6639\n",
      "\n",
      "[w/o Lambda] Run 3/5\n",
      "[Epoch 1] Loss: 37.5385\n",
      "[Epoch 100] Loss: 8.3864\n",
      "[Epoch 200] Loss: 6.8778\n",
      "[Epoch 300] Loss: 4.5832\n",
      "[Epoch 400] Loss: 3.9674\n",
      "[Epoch 500] Loss: 3.7384\n",
      "\n",
      "[w/o Lambda] Run 4/5\n",
      "[Epoch 1] Loss: 37.3673\n",
      "[Epoch 100] Loss: 8.4431\n",
      "[Epoch 200] Loss: 7.2123\n",
      "[Epoch 300] Loss: 4.1166\n",
      "[Epoch 400] Loss: 3.7157\n",
      "[Epoch 500] Loss: 3.4465\n",
      "\n",
      "[w/o Lambda] Run 5/5\n",
      "[Epoch 1] Loss: 37.9906\n",
      "[Epoch 100] Loss: 8.7352\n",
      "[Epoch 200] Loss: 8.0486\n",
      "[Epoch 300] Loss: 6.4918\n",
      "[Epoch 400] Loss: 4.3042\n",
      "[Epoch 500] Loss: 3.6123\n",
      "\n",
      "[w/o Projection, Fairness] Run 1/5\n",
      "[Epoch 1] Loss: 37.4108\n",
      "[Epoch 100] Loss: 4.0762\n",
      "[Epoch 200] Loss: 3.3256\n",
      "[Epoch 300] Loss: 2.7808\n",
      "[Epoch 400] Loss: 2.3933\n",
      "[Epoch 500] Loss: 2.3717\n",
      "\n",
      "[w/o Projection, Fairness] Run 2/5\n",
      "[Epoch 1] Loss: 37.2086\n",
      "[Epoch 100] Loss: 4.0761\n",
      "[Epoch 200] Loss: 3.1345\n",
      "[Epoch 300] Loss: 2.6991\n",
      "[Epoch 400] Loss: 2.5406\n",
      "[Epoch 500] Loss: 2.2878\n",
      "\n",
      "[w/o Projection, Fairness] Run 3/5\n",
      "[Epoch 1] Loss: 37.3890\n",
      "[Epoch 100] Loss: 3.7118\n",
      "[Epoch 200] Loss: 2.9933\n",
      "[Epoch 300] Loss: 2.6987\n",
      "[Epoch 400] Loss: 2.5423\n",
      "[Epoch 500] Loss: 2.2152\n",
      "\n",
      "[w/o Projection, Fairness] Run 4/5\n",
      "[Epoch 1] Loss: 37.2012\n",
      "[Epoch 100] Loss: 3.8627\n",
      "[Epoch 200] Loss: 2.9448\n",
      "[Epoch 300] Loss: 2.6682\n",
      "[Epoch 400] Loss: 2.6300\n",
      "[Epoch 500] Loss: 2.3205\n",
      "\n",
      "[w/o Projection, Fairness] Run 5/5\n",
      "[Epoch 1] Loss: 37.2213\n",
      "[Epoch 100] Loss: 4.0079\n",
      "[Epoch 200] Loss: 2.9162\n",
      "[Epoch 300] Loss: 2.5310\n",
      "[Epoch 400] Loss: 2.3042\n",
      "[Epoch 500] Loss: 2.0430\n",
      "\n",
      "[w/o Projection, Lambda] Run 1/5\n",
      "[Epoch 1] Loss: 37.3661\n",
      "[Epoch 100] Loss: 8.4140\n",
      "[Epoch 200] Loss: 7.6223\n",
      "[Epoch 300] Loss: 5.3540\n",
      "[Epoch 400] Loss: 4.2244\n",
      "[Epoch 500] Loss: 3.5929\n",
      "\n",
      "[w/o Projection, Lambda] Run 2/5\n",
      "[Epoch 1] Loss: 37.7011\n",
      "[Epoch 100] Loss: 8.6102\n",
      "[Epoch 200] Loss: 7.6059\n",
      "[Epoch 300] Loss: 4.4831\n",
      "[Epoch 400] Loss: 4.7104\n",
      "[Epoch 500] Loss: 3.4668\n",
      "\n",
      "[w/o Projection, Lambda] Run 3/5\n",
      "[Epoch 1] Loss: 37.2829\n",
      "[Epoch 100] Loss: 9.7087\n",
      "[Epoch 200] Loss: 7.2936\n",
      "[Epoch 300] Loss: 4.8114\n",
      "[Epoch 400] Loss: 3.6049\n",
      "[Epoch 500] Loss: 3.4809\n",
      "\n",
      "[w/o Projection, Lambda] Run 4/5\n",
      "[Epoch 1] Loss: 37.8350\n",
      "[Epoch 100] Loss: 8.2413\n",
      "[Epoch 200] Loss: 8.0009\n",
      "[Epoch 300] Loss: 5.0951\n",
      "[Epoch 400] Loss: 3.8049\n",
      "[Epoch 500] Loss: 3.6915\n",
      "\n",
      "[w/o Projection, Lambda] Run 5/5\n",
      "[Epoch 1] Loss: 37.0655\n",
      "[Epoch 100] Loss: 8.2185\n",
      "[Epoch 200] Loss: 8.0044\n",
      "[Epoch 300] Loss: 4.6078\n",
      "[Epoch 400] Loss: 4.5252\n",
      "[Epoch 500] Loss: 3.3477\n",
      "\n",
      "[w/o Fairness, Lambda] Run 1/5\n",
      "[Epoch 1] Loss: 37.2052\n",
      "[Epoch 100] Loss: 3.9686\n",
      "[Epoch 200] Loss: 3.2350\n",
      "[Epoch 300] Loss: 2.7201\n",
      "[Epoch 400] Loss: 2.4151\n",
      "[Epoch 500] Loss: 2.2406\n",
      "\n",
      "[w/o Fairness, Lambda] Run 2/5\n",
      "[Epoch 1] Loss: 37.1614\n",
      "[Epoch 100] Loss: 3.7193\n",
      "[Epoch 200] Loss: 3.0124\n",
      "[Epoch 300] Loss: 2.7028\n",
      "[Epoch 400] Loss: 2.4917\n",
      "[Epoch 500] Loss: 2.2978\n",
      "\n",
      "[w/o Fairness, Lambda] Run 3/5\n",
      "[Epoch 1] Loss: 37.3427\n",
      "[Epoch 100] Loss: 4.0463\n",
      "[Epoch 200] Loss: 3.3411\n",
      "[Epoch 300] Loss: 2.7766\n",
      "[Epoch 400] Loss: 2.4283\n",
      "[Epoch 500] Loss: 2.0921\n",
      "\n",
      "[w/o Fairness, Lambda] Run 4/5\n",
      "[Epoch 1] Loss: 37.4512\n",
      "[Epoch 100] Loss: 3.7128\n",
      "[Epoch 200] Loss: 2.9941\n",
      "[Epoch 300] Loss: 2.7046\n",
      "[Epoch 400] Loss: 2.4248\n",
      "[Epoch 500] Loss: 2.1994\n",
      "\n",
      "[w/o Fairness, Lambda] Run 5/5\n",
      "[Epoch 1] Loss: 37.8463\n",
      "[Epoch 100] Loss: 4.0457\n",
      "[Epoch 200] Loss: 3.3186\n",
      "[Epoch 300] Loss: 2.7882\n",
      "[Epoch 400] Loss: 2.5245\n",
      "[Epoch 500] Loss: 2.3005\n",
      "\n",
      "[w/o All] Run 1/5\n",
      "[Epoch 1] Loss: 37.5345\n",
      "[Epoch 100] Loss: 3.6652\n",
      "[Epoch 200] Loss: 2.9112\n",
      "[Epoch 300] Loss: 2.5869\n",
      "[Epoch 400] Loss: 2.4410\n",
      "[Epoch 500] Loss: 2.1052\n",
      "\n",
      "[w/o All] Run 2/5\n",
      "[Epoch 1] Loss: 37.6542\n",
      "[Epoch 100] Loss: 4.0529\n",
      "[Epoch 200] Loss: 3.0965\n",
      "[Epoch 300] Loss: 2.7349\n",
      "[Epoch 400] Loss: 2.6373\n",
      "[Epoch 500] Loss: 2.2523\n",
      "\n",
      "[w/o All] Run 3/5\n",
      "[Epoch 1] Loss: 37.3928\n",
      "[Epoch 100] Loss: 3.8575\n",
      "[Epoch 200] Loss: 3.0021\n",
      "[Epoch 300] Loss: 2.6866\n",
      "[Epoch 400] Loss: 2.4237\n",
      "[Epoch 500] Loss: 2.1992\n",
      "\n",
      "[w/o All] Run 4/5\n",
      "[Epoch 1] Loss: 37.0745\n",
      "[Epoch 100] Loss: 3.8701\n",
      "[Epoch 200] Loss: 2.9291\n",
      "[Epoch 300] Loss: 2.6145\n",
      "[Epoch 400] Loss: 2.3828\n",
      "[Epoch 500] Loss: 2.2010\n",
      "\n",
      "[w/o All] Run 5/5\n",
      "[Epoch 1] Loss: 36.8377\n",
      "[Epoch 100] Loss: 4.0661\n",
      "[Epoch 200] Loss: 3.0631\n",
      "[Epoch 300] Loss: 2.7279\n",
      "[Epoch 400] Loss: 2.5493\n",
      "[Epoch 500] Loss: 2.2545\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loading nba dataset from ./dataset/NBA\n",
      "[INFO] 유효하지 않은 user_id로 인해 제거된 edge 수: 1641\n",
      "----------------------------------------------------------------------------------------------------\n",
      "nba\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[Full] Run 1/5\n",
      "[Epoch 1] Loss: 8.4924\n",
      "[Epoch 100] Loss: 1.3387\n",
      "[Epoch 200] Loss: 0.8659\n",
      "[Epoch 300] Loss: 0.7494\n",
      "[Epoch 400] Loss: 0.7262\n",
      "[Epoch 500] Loss: 0.7570\n",
      "\n",
      "[Full] Run 2/5\n",
      "[Epoch 1] Loss: 7.8363\n",
      "[Epoch 100] Loss: 1.2711\n",
      "[Epoch 200] Loss: 0.8145\n",
      "[Epoch 300] Loss: 0.7589\n",
      "[Epoch 400] Loss: 0.7160\n",
      "[Epoch 500] Loss: 0.7162\n",
      "\n",
      "[Full] Run 3/5\n",
      "[Epoch 1] Loss: 8.1216\n",
      "[Epoch 100] Loss: 1.2382\n",
      "[Epoch 200] Loss: 0.7667\n",
      "[Epoch 300] Loss: 0.7348\n",
      "[Epoch 400] Loss: 0.7318\n",
      "[Epoch 500] Loss: 0.7063\n",
      "\n",
      "[Full] Run 4/5\n",
      "[Epoch 1] Loss: 8.4346\n",
      "[Epoch 100] Loss: 1.3130\n",
      "[Epoch 200] Loss: 0.8501\n",
      "[Epoch 300] Loss: 0.7406\n",
      "[Epoch 400] Loss: 0.7102\n",
      "[Epoch 500] Loss: 0.7196\n",
      "\n",
      "[Full] Run 5/5\n",
      "[Epoch 1] Loss: 8.3224\n",
      "[Epoch 100] Loss: 1.2836\n",
      "[Epoch 200] Loss: 0.7484\n",
      "[Epoch 300] Loss: 0.7585\n",
      "[Epoch 400] Loss: 0.7034\n",
      "[Epoch 500] Loss: 0.7329\n",
      "\n",
      "[w/o Projection] Run 1/5\n",
      "[Epoch 1] Loss: 8.6540\n",
      "[Epoch 100] Loss: 1.2595\n",
      "[Epoch 200] Loss: 0.9625\n",
      "[Epoch 300] Loss: 0.7300\n",
      "[Epoch 400] Loss: 0.7806\n",
      "[Epoch 500] Loss: 0.7172\n",
      "\n",
      "[w/o Projection] Run 2/5\n",
      "[Epoch 1] Loss: 8.3905\n",
      "[Epoch 100] Loss: 1.2800\n",
      "[Epoch 200] Loss: 0.7663\n",
      "[Epoch 300] Loss: 0.7572\n",
      "[Epoch 400] Loss: 0.7112\n",
      "[Epoch 500] Loss: 0.7328\n",
      "\n",
      "[w/o Projection] Run 3/5\n",
      "[Epoch 1] Loss: 7.7818\n",
      "[Epoch 100] Loss: 1.2255\n",
      "[Epoch 200] Loss: 0.7435\n",
      "[Epoch 300] Loss: 0.7351\n",
      "[Epoch 400] Loss: 0.7586\n",
      "[Epoch 500] Loss: 0.7637\n",
      "\n",
      "[w/o Projection] Run 4/5\n",
      "[Epoch 1] Loss: 8.1729\n",
      "[Epoch 100] Loss: 1.3107\n",
      "[Epoch 200] Loss: 0.8685\n",
      "[Epoch 300] Loss: 0.7506\n",
      "[Epoch 400] Loss: 0.7200\n",
      "[Epoch 500] Loss: 0.7187\n",
      "\n",
      "[w/o Projection] Run 5/5\n",
      "[Epoch 1] Loss: 8.2134\n",
      "[Epoch 100] Loss: 1.3901\n",
      "[Epoch 200] Loss: 0.8146\n",
      "[Epoch 300] Loss: 0.7577\n",
      "[Epoch 400] Loss: 0.7275\n",
      "[Epoch 500] Loss: 0.7421\n",
      "\n",
      "[w/o Fairness] Run 1/5\n",
      "[Epoch 1] Loss: 7.9636\n",
      "[Epoch 100] Loss: 0.7807\n",
      "[Epoch 200] Loss: 0.7633\n",
      "[Epoch 300] Loss: 0.7322\n",
      "[Epoch 400] Loss: 0.6817\n",
      "[Epoch 500] Loss: 0.6411\n",
      "\n",
      "[w/o Fairness] Run 2/5\n",
      "[Epoch 1] Loss: 8.1202\n",
      "[Epoch 100] Loss: 0.7807\n",
      "[Epoch 200] Loss: 0.7634\n",
      "[Epoch 300] Loss: 0.7288\n",
      "[Epoch 400] Loss: 0.6631\n",
      "[Epoch 500] Loss: 0.6497\n",
      "\n",
      "[w/o Fairness] Run 3/5\n",
      "[Epoch 1] Loss: 8.1085\n",
      "[Epoch 100] Loss: 0.7800\n",
      "[Epoch 200] Loss: 0.7624\n",
      "[Epoch 300] Loss: 0.7317\n",
      "[Epoch 400] Loss: 0.6686\n",
      "[Epoch 500] Loss: 0.6241\n",
      "\n",
      "[w/o Fairness] Run 4/5\n",
      "[Epoch 1] Loss: 7.7101\n",
      "[Epoch 100] Loss: 0.7772\n",
      "[Epoch 200] Loss: 0.7574\n",
      "[Epoch 300] Loss: 0.6700\n",
      "[Epoch 400] Loss: 0.6741\n",
      "[Epoch 500] Loss: 0.6452\n",
      "\n",
      "[w/o Fairness] Run 5/5\n",
      "[Epoch 1] Loss: 8.3579\n",
      "[Epoch 100] Loss: 0.7807\n",
      "[Epoch 200] Loss: 0.7654\n",
      "[Epoch 300] Loss: 0.7397\n",
      "[Epoch 400] Loss: 0.6886\n",
      "[Epoch 500] Loss: 0.6554\n",
      "\n",
      "[w/o Lambda] Run 1/5\n",
      "[Epoch 1] Loss: 8.8827\n",
      "[Epoch 100] Loss: 1.3342\n",
      "[Epoch 200] Loss: 0.7901\n",
      "[Epoch 300] Loss: 0.7572\n",
      "[Epoch 400] Loss: 0.7622\n",
      "[Epoch 500] Loss: 0.7333\n",
      "\n",
      "[w/o Lambda] Run 2/5\n",
      "[Epoch 1] Loss: 8.2342\n",
      "[Epoch 100] Loss: 1.2352\n",
      "[Epoch 200] Loss: 0.8858\n",
      "[Epoch 300] Loss: 0.7281\n",
      "[Epoch 400] Loss: 0.7508\n",
      "[Epoch 500] Loss: 0.7482\n",
      "\n",
      "[w/o Lambda] Run 3/5\n",
      "[Epoch 1] Loss: 8.2155\n",
      "[Epoch 100] Loss: 1.2818\n",
      "[Epoch 200] Loss: 0.9473\n",
      "[Epoch 300] Loss: 0.7198\n",
      "[Epoch 400] Loss: 0.7170\n",
      "[Epoch 500] Loss: 0.7360\n",
      "\n",
      "[w/o Lambda] Run 4/5\n",
      "[Epoch 1] Loss: 8.2036\n",
      "[Epoch 100] Loss: 1.3295\n",
      "[Epoch 200] Loss: 0.7311\n",
      "[Epoch 300] Loss: 0.8056\n",
      "[Epoch 400] Loss: 0.7179\n",
      "[Epoch 500] Loss: 0.7289\n",
      "\n",
      "[w/o Lambda] Run 5/5\n",
      "[Epoch 1] Loss: 8.5833\n",
      "[Epoch 100] Loss: 1.2898\n",
      "[Epoch 200] Loss: 0.8631\n",
      "[Epoch 300] Loss: 0.8100\n",
      "[Epoch 400] Loss: 0.7246\n",
      "[Epoch 500] Loss: 0.7150\n",
      "\n",
      "[w/o Projection, Fairness] Run 1/5\n",
      "[Epoch 1] Loss: 7.8767\n",
      "[Epoch 100] Loss: 0.7822\n",
      "[Epoch 200] Loss: 0.7635\n",
      "[Epoch 300] Loss: 0.7261\n",
      "[Epoch 400] Loss: 0.6361\n",
      "[Epoch 500] Loss: 0.6386\n",
      "\n",
      "[w/o Projection, Fairness] Run 2/5\n",
      "[Epoch 1] Loss: 8.3390\n",
      "[Epoch 100] Loss: 0.7822\n",
      "[Epoch 200] Loss: 0.7658\n",
      "[Epoch 300] Loss: 0.7356\n",
      "[Epoch 400] Loss: 0.6710\n",
      "[Epoch 500] Loss: 0.6375\n",
      "\n",
      "[w/o Projection, Fairness] Run 3/5\n",
      "[Epoch 1] Loss: 8.2053\n",
      "[Epoch 100] Loss: 0.7805\n",
      "[Epoch 200] Loss: 0.7614\n",
      "[Epoch 300] Loss: 0.7258\n",
      "[Epoch 400] Loss: 0.6648\n",
      "[Epoch 500] Loss: 0.6747\n",
      "\n",
      "[w/o Projection, Fairness] Run 4/5\n",
      "[Epoch 1] Loss: 7.9886\n",
      "[Epoch 100] Loss: 0.7819\n",
      "[Epoch 200] Loss: 0.7687\n",
      "[Epoch 300] Loss: 0.7414\n",
      "[Epoch 400] Loss: 0.6952\n",
      "[Epoch 500] Loss: 0.6716\n",
      "\n",
      "[w/o Projection, Fairness] Run 5/5\n",
      "[Epoch 1] Loss: 8.1893\n",
      "[Epoch 100] Loss: 0.7841\n",
      "[Epoch 200] Loss: 0.7681\n",
      "[Epoch 300] Loss: 0.7433\n",
      "[Epoch 400] Loss: 0.6925\n",
      "[Epoch 500] Loss: 0.6541\n",
      "\n",
      "[w/o Projection, Lambda] Run 1/5\n",
      "[Epoch 1] Loss: 8.1950\n",
      "[Epoch 100] Loss: 1.2000\n",
      "[Epoch 200] Loss: 0.7804\n",
      "[Epoch 300] Loss: 0.7578\n",
      "[Epoch 400] Loss: 0.7537\n",
      "[Epoch 500] Loss: 0.7149\n",
      "\n",
      "[w/o Projection, Lambda] Run 2/5\n",
      "[Epoch 1] Loss: 8.0598\n",
      "[Epoch 100] Loss: 1.3611\n",
      "[Epoch 200] Loss: 0.8274\n",
      "[Epoch 300] Loss: 0.7181\n",
      "[Epoch 400] Loss: 0.7160\n",
      "[Epoch 500] Loss: 0.7540\n",
      "\n",
      "[w/o Projection, Lambda] Run 3/5\n",
      "[Epoch 1] Loss: 7.9562\n",
      "[Epoch 100] Loss: 1.1390\n",
      "[Epoch 200] Loss: 0.8234\n",
      "[Epoch 300] Loss: 0.7406\n",
      "[Epoch 400] Loss: 0.7190\n",
      "[Epoch 500] Loss: 0.7420\n",
      "\n",
      "[w/o Projection, Lambda] Run 4/5\n",
      "[Epoch 1] Loss: 8.3164\n",
      "[Epoch 100] Loss: 1.2814\n",
      "[Epoch 200] Loss: 0.7970\n",
      "[Epoch 300] Loss: 0.7526\n",
      "[Epoch 400] Loss: 0.7141\n",
      "[Epoch 500] Loss: 0.7147\n",
      "\n",
      "[w/o Projection, Lambda] Run 5/5\n",
      "[Epoch 1] Loss: 8.1573\n",
      "[Epoch 100] Loss: 1.2217\n",
      "[Epoch 200] Loss: 0.7853\n",
      "[Epoch 300] Loss: 0.7285\n",
      "[Epoch 400] Loss: 0.7201\n",
      "[Epoch 500] Loss: 0.7423\n",
      "\n",
      "[w/o Fairness, Lambda] Run 1/5\n",
      "[Epoch 1] Loss: 8.3016\n",
      "[Epoch 100] Loss: 0.7805\n",
      "[Epoch 200] Loss: 0.7671\n",
      "[Epoch 300] Loss: 0.7370\n",
      "[Epoch 400] Loss: 0.6809\n",
      "[Epoch 500] Loss: 0.6785\n",
      "\n",
      "[w/o Fairness, Lambda] Run 2/5\n",
      "[Epoch 1] Loss: 8.1875\n",
      "[Epoch 100] Loss: 0.7802\n",
      "[Epoch 200] Loss: 0.7654\n",
      "[Epoch 300] Loss: 0.7371\n",
      "[Epoch 400] Loss: 0.6814\n",
      "[Epoch 500] Loss: 0.6499\n",
      "\n",
      "[w/o Fairness, Lambda] Run 3/5\n",
      "[Epoch 1] Loss: 7.9730\n",
      "[Epoch 100] Loss: 0.7805\n",
      "[Epoch 200] Loss: 0.7611\n",
      "[Epoch 300] Loss: 0.7222\n",
      "[Epoch 400] Loss: 0.6596\n",
      "[Epoch 500] Loss: 0.6334\n",
      "\n",
      "[w/o Fairness, Lambda] Run 4/5\n",
      "[Epoch 1] Loss: 8.1564\n",
      "[Epoch 100] Loss: 0.7826\n",
      "[Epoch 200] Loss: 0.7656\n",
      "[Epoch 300] Loss: 0.7331\n",
      "[Epoch 400] Loss: 0.6824\n",
      "[Epoch 500] Loss: 0.6389\n",
      "\n",
      "[w/o Fairness, Lambda] Run 5/5\n",
      "[Epoch 1] Loss: 8.0832\n",
      "[Epoch 100] Loss: 0.7794\n",
      "[Epoch 200] Loss: 0.7645\n",
      "[Epoch 300] Loss: 0.7328\n",
      "[Epoch 400] Loss: 0.6658\n",
      "[Epoch 500] Loss: 0.7643\n",
      "\n",
      "[w/o All] Run 1/5\n",
      "[Epoch 1] Loss: 8.1169\n",
      "[Epoch 100] Loss: 0.7807\n",
      "[Epoch 200] Loss: 0.7600\n",
      "[Epoch 300] Loss: 0.7126\n",
      "[Epoch 400] Loss: 0.6495\n",
      "[Epoch 500] Loss: 0.6137\n",
      "\n",
      "[w/o All] Run 2/5\n",
      "[Epoch 1] Loss: 7.9746\n",
      "[Epoch 100] Loss: 0.7799\n",
      "[Epoch 200] Loss: 0.7633\n",
      "[Epoch 300] Loss: 0.7324\n",
      "[Epoch 400] Loss: 0.6721\n",
      "[Epoch 500] Loss: 0.6465\n",
      "\n",
      "[w/o All] Run 3/5\n",
      "[Epoch 1] Loss: 8.2192\n",
      "[Epoch 100] Loss: 0.7786\n",
      "[Epoch 200] Loss: 0.7619\n",
      "[Epoch 300] Loss: 0.7297\n",
      "[Epoch 400] Loss: 0.6724\n",
      "[Epoch 500] Loss: 0.6450\n",
      "\n",
      "[w/o All] Run 4/5\n",
      "[Epoch 1] Loss: 8.2042\n",
      "[Epoch 100] Loss: 0.7776\n",
      "[Epoch 200] Loss: 0.7579\n",
      "[Epoch 300] Loss: 0.7148\n",
      "[Epoch 400] Loss: 0.6597\n",
      "[Epoch 500] Loss: 0.6312\n",
      "\n",
      "[w/o All] Run 5/5\n",
      "[Epoch 1] Loss: 8.2090\n",
      "[Epoch 100] Loss: 0.7812\n",
      "[Epoch 200] Loss: 0.7667\n",
      "[Epoch 300] Loss: 0.7416\n",
      "[Epoch 400] Loss: 0.6954\n",
      "[Epoch 500] Loss: 0.6529\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "german\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[Full] Run 1/5\n",
      "[Epoch 1] Loss: 152.7594\n",
      "[Epoch 100] Loss: 0.0698\n",
      "[Epoch 200] Loss: 0.6495\n",
      "[Epoch 300] Loss: 0.2581\n",
      "[Epoch 400] Loss: 0.0470\n",
      "[Epoch 500] Loss: 0.0433\n",
      "\n",
      "[Full] Run 2/5\n",
      "[Epoch 1] Loss: 47.4317\n",
      "[Epoch 100] Loss: 0.0819\n",
      "[Epoch 200] Loss: 0.1830\n",
      "[Epoch 300] Loss: 0.0987\n",
      "[Epoch 400] Loss: 0.3401\n",
      "[Epoch 500] Loss: 0.3592\n",
      "\n",
      "[Full] Run 3/5\n",
      "[Epoch 1] Loss: 98.8328\n",
      "[Epoch 100] Loss: 0.1864\n",
      "[Epoch 200] Loss: 0.0868\n",
      "[Epoch 300] Loss: 0.0840\n",
      "[Epoch 400] Loss: 0.0709\n",
      "[Epoch 500] Loss: 0.0964\n",
      "\n",
      "[Full] Run 4/5\n",
      "[Epoch 1] Loss: 13.2941\n",
      "[Epoch 100] Loss: 2.2556\n",
      "[Epoch 200] Loss: 0.1264\n",
      "[Epoch 300] Loss: 0.1904\n",
      "[Epoch 400] Loss: 0.1645\n",
      "[Epoch 500] Loss: 0.0861\n",
      "\n",
      "[Full] Run 5/5\n",
      "[Epoch 1] Loss: 130.8854\n",
      "[Epoch 100] Loss: 0.6699\n",
      "[Epoch 200] Loss: 0.1002\n",
      "[Epoch 300] Loss: 0.2802\n",
      "[Epoch 400] Loss: 0.0663\n",
      "[Epoch 500] Loss: 0.1118\n",
      "\n",
      "[w/o Projection] Run 1/5\n",
      "[Epoch 1] Loss: 2.0010\n",
      "[Epoch 100] Loss: 0.0732\n",
      "[Epoch 200] Loss: 0.4481\n",
      "[Epoch 300] Loss: 0.1046\n",
      "[Epoch 400] Loss: 0.2055\n",
      "[Epoch 500] Loss: 0.3666\n",
      "\n",
      "[w/o Projection] Run 2/5\n",
      "[Epoch 1] Loss: 25.8520\n",
      "[Epoch 100] Loss: 0.0913\n",
      "[Epoch 200] Loss: 0.0510\n",
      "[Epoch 300] Loss: 0.1064\n",
      "[Epoch 400] Loss: 0.1563\n",
      "[Epoch 500] Loss: 0.0459\n",
      "\n",
      "[w/o Projection] Run 3/5\n",
      "[Epoch 1] Loss: 38.0376\n",
      "[Epoch 100] Loss: 1.0530\n",
      "[Epoch 200] Loss: 0.0975\n",
      "[Epoch 300] Loss: 0.2409\n",
      "[Epoch 400] Loss: 0.1084\n",
      "[Epoch 500] Loss: 0.0916\n",
      "\n",
      "[w/o Projection] Run 4/5\n",
      "[Epoch 1] Loss: 52.1789\n",
      "[Epoch 100] Loss: 0.1487\n",
      "[Epoch 200] Loss: 0.1452\n",
      "[Epoch 300] Loss: 0.5055\n",
      "[Epoch 400] Loss: 0.2129\n",
      "[Epoch 500] Loss: 0.1638\n",
      "\n",
      "[w/o Projection] Run 5/5\n",
      "[Epoch 1] Loss: 24.6049\n",
      "[Epoch 100] Loss: 0.1207\n",
      "[Epoch 200] Loss: 0.9940\n",
      "[Epoch 300] Loss: 0.3041\n",
      "[Epoch 400] Loss: 0.1298\n",
      "[Epoch 500] Loss: 0.1664\n",
      "\n",
      "[w/o Fairness] Run 1/5\n",
      "[Epoch 1] Loss: 16.1534\n",
      "[Epoch 100] Loss: 2.1192\n",
      "[Epoch 200] Loss: 0.9503\n",
      "[Epoch 300] Loss: 0.7285\n",
      "[Epoch 400] Loss: 0.0843\n",
      "[Epoch 500] Loss: 0.1723\n",
      "\n",
      "[w/o Fairness] Run 2/5\n",
      "[Epoch 1] Loss: 13.3747\n",
      "[Epoch 100] Loss: 0.7445\n",
      "[Epoch 200] Loss: 0.2235\n",
      "[Epoch 300] Loss: 0.1027\n",
      "[Epoch 400] Loss: 0.0903\n",
      "[Epoch 500] Loss: 0.0458\n",
      "\n",
      "[w/o Fairness] Run 3/5\n",
      "[Epoch 1] Loss: 76.1789\n",
      "[Epoch 100] Loss: 0.8564\n",
      "[Epoch 200] Loss: 0.2702\n",
      "[Epoch 300] Loss: 0.1269\n",
      "[Epoch 400] Loss: 0.1453\n",
      "[Epoch 500] Loss: 0.0527\n",
      "\n",
      "[w/o Fairness] Run 4/5\n",
      "[Epoch 1] Loss: 59.1582\n",
      "[Epoch 100] Loss: 0.7594\n",
      "[Epoch 200] Loss: 0.1008\n",
      "[Epoch 300] Loss: 0.2123\n",
      "[Epoch 400] Loss: 0.1214\n",
      "[Epoch 500] Loss: 0.1075\n",
      "\n",
      "[w/o Fairness] Run 5/5\n",
      "[Epoch 1] Loss: 166.9114\n",
      "[Epoch 100] Loss: 1.1326\n",
      "[Epoch 200] Loss: 0.0915\n",
      "[Epoch 300] Loss: 0.1476\n",
      "[Epoch 400] Loss: 0.0644\n",
      "[Epoch 500] Loss: 0.0954\n",
      "\n",
      "[w/o Lambda] Run 1/5\n",
      "[Epoch 1] Loss: 265.8047\n",
      "[Epoch 100] Loss: 0.1038\n",
      "[Epoch 200] Loss: 0.1597\n",
      "[Epoch 300] Loss: 0.0826\n",
      "[Epoch 400] Loss: 0.4344\n",
      "[Epoch 500] Loss: 0.5379\n",
      "\n",
      "[w/o Lambda] Run 2/5\n",
      "[Epoch 1] Loss: 432.7291\n",
      "[Epoch 100] Loss: 0.2179\n",
      "[Epoch 200] Loss: 0.3676\n",
      "[Epoch 300] Loss: 0.3372\n",
      "[Epoch 400] Loss: 0.4317\n",
      "[Epoch 500] Loss: 0.8407\n",
      "\n",
      "[w/o Lambda] Run 3/5\n",
      "[Epoch 1] Loss: 0.5251\n",
      "[Epoch 100] Loss: 0.1521\n",
      "[Epoch 200] Loss: 0.1328\n",
      "[Epoch 300] Loss: 0.2332\n",
      "[Epoch 400] Loss: 0.2186\n",
      "[Epoch 500] Loss: 0.0883\n",
      "\n",
      "[w/o Lambda] Run 4/5\n",
      "[Epoch 1] Loss: 576.6013\n",
      "[Epoch 100] Loss: 0.1305\n",
      "[Epoch 200] Loss: 0.2183\n",
      "[Epoch 300] Loss: 0.6486\n",
      "[Epoch 400] Loss: 0.1196\n",
      "[Epoch 500] Loss: 1.5096\n",
      "\n",
      "[w/o Lambda] Run 5/5\n",
      "[Epoch 1] Loss: 40.9408\n",
      "[Epoch 100] Loss: 0.6481\n",
      "[Epoch 200] Loss: 0.2490\n",
      "[Epoch 300] Loss: 0.1005\n",
      "[Epoch 400] Loss: 0.3488\n",
      "[Epoch 500] Loss: 0.9144\n",
      "\n",
      "[w/o Projection, Fairness] Run 1/5\n",
      "[Epoch 1] Loss: 6.9250\n",
      "[Epoch 100] Loss: 2.1375\n",
      "[Epoch 200] Loss: 0.4115\n",
      "[Epoch 300] Loss: 0.1275\n",
      "[Epoch 400] Loss: 0.2887\n",
      "[Epoch 500] Loss: 0.1990\n",
      "\n",
      "[w/o Projection, Fairness] Run 2/5\n",
      "[Epoch 1] Loss: 9.5492\n",
      "[Epoch 100] Loss: 0.1746\n",
      "[Epoch 200] Loss: 0.0687\n",
      "[Epoch 300] Loss: 0.1935\n",
      "[Epoch 400] Loss: 0.0515\n",
      "[Epoch 500] Loss: 0.0966\n",
      "\n",
      "[w/o Projection, Fairness] Run 3/5\n",
      "[Epoch 1] Loss: 8.4537\n",
      "[Epoch 100] Loss: 0.1046\n",
      "[Epoch 200] Loss: 0.0728\n",
      "[Epoch 300] Loss: 0.0519\n",
      "[Epoch 400] Loss: 0.0326\n",
      "[Epoch 500] Loss: 0.0301\n",
      "\n",
      "[w/o Projection, Fairness] Run 4/5\n",
      "[Epoch 1] Loss: 28.0556\n",
      "[Epoch 100] Loss: 0.1042\n",
      "[Epoch 200] Loss: 0.0821\n",
      "[Epoch 300] Loss: 0.0498\n",
      "[Epoch 400] Loss: 0.0381\n",
      "[Epoch 500] Loss: 0.0304\n",
      "\n",
      "[w/o Projection, Fairness] Run 5/5\n",
      "[Epoch 1] Loss: 44.8941\n",
      "[Epoch 100] Loss: 2.0881\n",
      "[Epoch 200] Loss: 3.4125\n",
      "[Epoch 300] Loss: 0.7515\n",
      "[Epoch 400] Loss: 0.1373\n",
      "[Epoch 500] Loss: 0.2232\n",
      "\n",
      "[w/o Projection, Lambda] Run 1/5\n",
      "[Epoch 1] Loss: 66.9296\n",
      "[Epoch 100] Loss: 0.1486\n",
      "[Epoch 200] Loss: 0.1690\n",
      "[Epoch 300] Loss: 0.2464\n",
      "[Epoch 400] Loss: 0.4356\n",
      "[Epoch 500] Loss: 0.3319\n",
      "\n",
      "[w/o Projection, Lambda] Run 2/5\n",
      "[Epoch 1] Loss: 92.2254\n",
      "[Epoch 100] Loss: 0.4855\n",
      "[Epoch 200] Loss: 0.0744\n",
      "[Epoch 300] Loss: 0.4864\n",
      "[Epoch 400] Loss: 0.2004\n",
      "[Epoch 500] Loss: 0.1060\n",
      "\n",
      "[w/o Projection, Lambda] Run 3/5\n",
      "[Epoch 1] Loss: 2808.4434\n",
      "[Epoch 100] Loss: 0.0916\n",
      "[Epoch 200] Loss: 0.0549\n",
      "[Epoch 300] Loss: 0.0425\n",
      "[Epoch 400] Loss: 0.0444\n",
      "[Epoch 500] Loss: 0.0795\n",
      "\n",
      "[w/o Projection, Lambda] Run 4/5\n",
      "[Epoch 1] Loss: 2.0764\n",
      "[Epoch 100] Loss: 0.1138\n",
      "[Epoch 200] Loss: 0.0510\n",
      "[Epoch 300] Loss: 0.0476\n",
      "[Epoch 400] Loss: 0.2635\n",
      "[Epoch 500] Loss: 0.0405\n",
      "\n",
      "[w/o Projection, Lambda] Run 5/5\n",
      "[Epoch 1] Loss: 594.3955\n",
      "[Epoch 100] Loss: 2.2151\n",
      "[Epoch 200] Loss: 0.9180\n",
      "[Epoch 300] Loss: 0.1451\n",
      "[Epoch 400] Loss: 0.2166\n",
      "[Epoch 500] Loss: 0.1730\n",
      "\n",
      "[w/o Fairness, Lambda] Run 1/5\n",
      "[Epoch 1] Loss: 4.3415\n",
      "[Epoch 100] Loss: 2.9765\n",
      "[Epoch 200] Loss: 2.5277\n",
      "[Epoch 300] Loss: 0.7887\n",
      "[Epoch 400] Loss: 0.3186\n",
      "[Epoch 500] Loss: 0.1267\n",
      "\n",
      "[w/o Fairness, Lambda] Run 2/5\n",
      "[Epoch 1] Loss: 131.5817\n",
      "[Epoch 100] Loss: 0.1127\n",
      "[Epoch 200] Loss: 0.0758\n",
      "[Epoch 300] Loss: 0.0458\n",
      "[Epoch 400] Loss: 0.0362\n",
      "[Epoch 500] Loss: 0.0307\n",
      "\n",
      "[w/o Fairness, Lambda] Run 3/5\n",
      "[Epoch 1] Loss: 0.9056\n",
      "[Epoch 100] Loss: 0.1028\n",
      "[Epoch 200] Loss: 0.0879\n",
      "[Epoch 300] Loss: 0.0608\n",
      "[Epoch 400] Loss: 0.3112\n",
      "[Epoch 500] Loss: 0.0447\n",
      "\n",
      "[w/o Fairness, Lambda] Run 4/5\n",
      "[Epoch 1] Loss: 119.8667\n",
      "[Epoch 100] Loss: 0.0883\n",
      "[Epoch 200] Loss: 0.1595\n",
      "[Epoch 300] Loss: 0.0581\n",
      "[Epoch 400] Loss: 0.0456\n",
      "[Epoch 500] Loss: 0.1077\n",
      "\n",
      "[w/o Fairness, Lambda] Run 5/5\n",
      "[Epoch 1] Loss: 87.6311\n",
      "[Epoch 100] Loss: 0.2310\n",
      "[Epoch 200] Loss: 0.1104\n",
      "[Epoch 300] Loss: 0.0671\n",
      "[Epoch 400] Loss: 0.0528\n",
      "[Epoch 500] Loss: 0.0900\n",
      "\n",
      "[w/o All] Run 1/5\n",
      "[Epoch 1] Loss: 31.7695\n",
      "[Epoch 100] Loss: 3.0967\n",
      "[Epoch 200] Loss: 0.0867\n",
      "[Epoch 300] Loss: 0.1924\n",
      "[Epoch 400] Loss: 0.0938\n",
      "[Epoch 500] Loss: 0.0700\n",
      "\n",
      "[w/o All] Run 2/5\n",
      "[Epoch 1] Loss: 16.8980\n",
      "[Epoch 100] Loss: 2.6243\n",
      "[Epoch 200] Loss: 0.2290\n",
      "[Epoch 300] Loss: 0.1297\n",
      "[Epoch 400] Loss: 0.1124\n",
      "[Epoch 500] Loss: 0.1053\n",
      "\n",
      "[w/o All] Run 3/5\n",
      "[Epoch 1] Loss: 135.1502\n",
      "[Epoch 100] Loss: 1.7464\n",
      "[Epoch 200] Loss: 1.3083\n",
      "[Epoch 300] Loss: 0.3686\n",
      "[Epoch 400] Loss: 0.1656\n",
      "[Epoch 500] Loss: 0.1280\n",
      "\n",
      "[w/o All] Run 4/5\n",
      "[Epoch 1] Loss: 12.1429\n",
      "[Epoch 100] Loss: 0.1818\n",
      "[Epoch 200] Loss: 0.4880\n",
      "[Epoch 300] Loss: 0.1722\n",
      "[Epoch 400] Loss: 0.0921\n",
      "[Epoch 500] Loss: 0.0613\n",
      "\n",
      "[w/o All] Run 5/5\n",
      "[Epoch 1] Loss: 11.2547\n",
      "[Epoch 100] Loss: 2.5422\n",
      "[Epoch 200] Loss: 0.1574\n",
      "[Epoch 300] Loss: 1.2018\n",
      "[Epoch 400] Loss: 0.3022\n",
      "[Epoch 500] Loss: 0.5720\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Ablation study\n",
    "lambda_fair = 0.5\n",
    "all_dataset_results = {}\n",
    "\n",
    "for dataset in ['region_job', 'region_job_2', 'nba', 'german']:\n",
    "    seed = 1127\n",
    "    if dataset == 'nba':\n",
    "        predict_attr, sens_attr, label_number, sens_number, test_idx, path = nba_predict_attr, nba_sens_attr, nba_label_number, nba_sens_number, nba_test_idx, nba_path\n",
    "    elif dataset == 'german':\n",
    "        predict_attr, sens_attr, path = german_predict_attr, german_sens_attr, german_path\n",
    "    else:\n",
    "        predict_attr, sens_attr, label_number, sens_number, test_idx, path = pokec_predict_attr, pokec_sens_attr, pokec_label_number, pokec_sens_number, pokec_test_idx, pokec_path\n",
    "\n",
    "    adj, features, labels, idx_train, idx_val, idx_test, sens, idx_sens_train = load_dataset_unified(dataset, sens_attr, predict_attr, path, label_number, sens_number, test_idx, seed=seed)\n",
    "\n",
    "    if dataset == 'nba':\n",
    "        features = feature_norm(features)\n",
    "\n",
    "    if isinstance(adj, torch.Tensor):  # torch.sparse_coo_tensor\n",
    "        src, dst = adj._indices()\n",
    "        g = dgl.graph((src, dst))\n",
    "        edge_index = adj._indices()\n",
    "    else:  # scipy sparse matrix\n",
    "        g = dgl.from_scipy(adj)\n",
    "        adj_coo = adj.tocoo()\n",
    "        edge_index = torch.tensor([adj_coo.row, adj_coo.col], dtype=torch.long)\n",
    "\n",
    "    data = Data(x=features, edge_index=edge_index, y=labels.float(), sensitive_attr=sens)\n",
    "    data.idx_train = idx_train\n",
    "    data.idx_val = idx_val\n",
    "    data.idx_test = idx_test\n",
    "    data.idx_sens_train = idx_sens_train\n",
    "\n",
    "    all_results = []\n",
    "    \n",
    "    print('-' * 100)\n",
    "    print(dataset)\n",
    "    print('-' * 100)\n",
    "    \n",
    "    df_ablation = run_ablation_experiments(data, device, lambda_fair, n_runs)\n",
    "    print('-' * 100)\n",
    "\n",
    "    all_dataset_results[dataset] = df_ablation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Setting  QR Loss Mean  Fairness Gap Mean\n",
      "0                      Full          3.95               1.84\n",
      "1            w/o Projection          3.90               1.74\n",
      "2              w/o Fairness          4.45               1.33\n",
      "3                w/o Lambda          3.85               1.63\n",
      "4  w/o Projection, Fairness          4.62               1.19\n",
      "5    w/o Projection, Lambda          3.77               1.58\n",
      "6      w/o Fairness, Lambda          4.60               0.94\n",
      "7                   w/o All          4.59               1.14\n",
      "--------------------------------------------------\n",
      "                    Setting  QR Loss Mean  Fairness Gap Mean\n",
      "0                      Full          3.29               0.69\n",
      "1            w/o Projection          3.29               0.67\n",
      "2              w/o Fairness          4.73               3.53\n",
      "3                w/o Lambda          3.35               0.61\n",
      "4  w/o Projection, Fairness          4.74               3.70\n",
      "5    w/o Projection, Lambda          3.33               0.69\n",
      "6      w/o Fairness, Lambda          4.77               3.12\n",
      "7                   w/o All          4.83               3.02\n",
      "--------------------------------------------------\n",
      "                    Setting  QR Loss Mean  Fairness Gap Mean\n",
      "0                      Full          0.55               0.12\n",
      "1            w/o Projection          0.56               0.14\n",
      "2              w/o Fairness          0.57               0.12\n",
      "3                w/o Lambda          0.55               0.12\n",
      "4  w/o Projection, Fairness          0.59               0.15\n",
      "5    w/o Projection, Lambda          0.56               0.11\n",
      "6      w/o Fairness, Lambda          0.60               0.15\n",
      "7                   w/o All          0.60               0.15\n",
      "--------------------------------------------------\n",
      "                    Setting  QR Loss Mean  Fairness Gap Mean\n",
      "0                      Full          0.11               0.06\n",
      "1            w/o Projection          0.13               0.05\n",
      "2              w/o Fairness          0.13               0.11\n",
      "3                w/o Lambda          0.21               0.09\n",
      "4  w/o Projection, Fairness          0.10               0.12\n",
      "5    w/o Projection, Lambda          0.33               0.14\n",
      "6      w/o Fairness, Lambda          0.06               0.06\n",
      "7                   w/o All          0.18               0.24\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['region_job', 'region_job_2', 'nba', 'german']:\n",
    "    print(all_dataset_results[dataset][['Setting', 'QR Loss Mean', 'Fairness Gap Mean']].round(2))\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 시각화\n",
    "x = np.arange(len(df_ablation))\n",
    "width = 0.35\n",
    "fig, ax1 = plt.subplots(figsize=(6,3))\n",
    "ax1.bar(x - width/2, df_ablation['QR Loss Mean'], width, yerr=df_ablation['QR Loss Std'], label='QR Loss')\n",
    "ax1.bar(x + width/2, df_ablation['Fairness Gap Mean'], width, yerr=df_ablation['Fairness Gap Std'], label='Fairness Gap')\n",
    "ax1.set_ylabel('Metric Value')\n",
    "ax1.set_title('Ablation Study: QR Loss vs Fairness Gap')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(df_ablation['Setting'])\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fair-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
