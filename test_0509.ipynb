{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d510bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "import torch_sparse\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.typing import Adj, OptTensor\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "\n",
    "# import warnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d798f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드 함수\n",
    " \n",
    "def feature_norm(features):\n",
    "    min_values = features.min(axis=0)[0]\n",
    "    max_values = features.max(axis=0)[0]\n",
    "    return 2*(features - min_values).div(max_values-min_values) - 1\n",
    "\n",
    "def load_dataset(dataset,sens_attr,predict_attr, path=\"./dataset/pokec/\", \n",
    "               label_number=1000, sens_number=500, test_idx=False, seed=1127):\n",
    "    \"\"\"Load data\"\"\"\n",
    "    print('Loading {} dataset from {}'.format(dataset,path))\n",
    "\n",
    "    idx_features_labels = pd.read_csv(os.path.join(path,\"{}.csv\".format(dataset)))\n",
    "    header = list(idx_features_labels.columns)\n",
    "    header.remove(\"user_id\")\n",
    "\n",
    "    header.remove(sens_attr)\n",
    "    header.remove(predict_attr)\n",
    "\n",
    "    features = sp.csr_matrix(idx_features_labels[header], dtype=np.float32)\n",
    "    labels = idx_features_labels[predict_attr].values\n",
    "    \n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[\"user_id\"], dtype=int)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "\n",
    "    # edges_unordered = np.genfromtxt(os.path.join(path,\"{}_relationship.txt\".format(dataset)), dtype=int)\n",
    "    edges_unordered = np.genfromtxt(os.path.join(path, f\"{dataset}_relationship.txt\"), dtype=np.int64)\n",
    "\n",
    "    # edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "    #                  dtype=int).reshape(edges_unordered.shape)\n",
    "    mapped_edges = []\n",
    "    dropped = 0\n",
    "    for u, v in edges_unordered:\n",
    "        u_mapped = idx_map.get(u)\n",
    "        v_mapped = idx_map.get(v)\n",
    "        if u_mapped is not None and v_mapped is not None:\n",
    "            mapped_edges.append((u_mapped, v_mapped))\n",
    "        else:\n",
    "            dropped += 1\n",
    "\n",
    "    print(f\"[INFO] 유효하지 않은 user_id로 인해 제거된 edge 수: {dropped}\")\n",
    "    edges = np.array(mapped_edges, dtype=int)\n",
    "\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "    \n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    # features = normalize(features)\n",
    "    adj = adj + sp.eye(adj.shape[0])\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(labels)\n",
    "    # adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    random.seed(seed)\n",
    "    label_idx = np.where(labels>=0)[0]\n",
    "    random.shuffle(label_idx)\n",
    "\n",
    "    idx_train = label_idx[:min(int(0.5 * len(label_idx)),label_number)]\n",
    "    idx_val = label_idx[int(0.5 * len(label_idx)):int(0.75 * len(label_idx))]\n",
    "    if test_idx:\n",
    "        idx_test = label_idx[label_number:]\n",
    "        idx_val = idx_test\n",
    "    else:\n",
    "        idx_test = label_idx[int(0.75 * len(label_idx)):]\n",
    "\n",
    "    sens = idx_features_labels[sens_attr].values\n",
    "\n",
    "    sens_idx = set(np.where(sens >= 0)[0])\n",
    "    idx_test = np.asarray(list(sens_idx & set(idx_test)))\n",
    "    sens = torch.FloatTensor(sens)\n",
    "    idx_sens_train = list(sens_idx - set(idx_val) - set(idx_test))\n",
    "    random.seed(seed)\n",
    "    random.shuffle(idx_sens_train)\n",
    "    idx_sens_train = torch.LongTensor(idx_sens_train[:sens_number])\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    \n",
    "    # random.shuffle(sens_idx)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test, sens,idx_sens_train\n",
    "\n",
    "def is_symmetric(m):\n",
    "    '''\n",
    "    Judge whether the matrix is symmetric or not.\n",
    "    :param m: Adjacency matrix(Array)\n",
    "    '''\n",
    "    res = np.int64(np.triu(m).T == np.tril(m))\n",
    "    # if np.where(res==0)[0] != []:\n",
    "    if np.where(res == 0)[0].size > 0:\n",
    "        raise ValueError(\"The matrix is not symmetric!\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "def symetric_normalize(m, half: bool):\n",
    "    '''\n",
    "    Symmetrically normalization for adjacency matrix\n",
    "    :param m: (Array) Adjacency matrix\n",
    "    :param half: (bool) whether m is triu or full\n",
    "    :return: (Array) An symmetric adjacency matrix\n",
    "    '''\n",
    "    if not half:\n",
    "        is_symmetric(m)\n",
    "    else:\n",
    "        m = m + m.T - np.diag(np.diagonal(m))\n",
    "\n",
    "    hat_m = m + np.eye(m.shape[0])\n",
    "    D = np.sum(hat_m, axis=1)\n",
    "    D = np.diag(D)\n",
    "\n",
    "    # D = np.power(D, -0.5)\n",
    "    with np.errstate(divide='ignore'):\n",
    "        D = np.power(D, -0.5)\n",
    "        D[np.isinf(D)] = 0\n",
    "\n",
    "    D[np.isinf(D)] = 0\n",
    "    sn_m = np.matmul(np.matmul(D, hat_m), D)\n",
    "    return sn_m\n",
    "\n",
    "def sp2sptensor(m):\n",
    "    # sparse_m = sp.coo_matrix(m).astype(np.float)\n",
    "    sparse_m = sp.coo_matrix(m).astype(np.float64)\n",
    "    # indices = torch.from_numpy(np.vstack((sparse_m.row, sparse_m.col)).astype(int))\n",
    "    indices = torch.from_numpy(np.vstack((sparse_m.row, sparse_m.col))).long()\n",
    "    values = torch.from_numpy(sparse_m.data)\n",
    "    shape = torch.Size(sparse_m.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "class German:\n",
    "    def __init__(self, path):\n",
    "        if not os.path.exists(path):\n",
    "            raise ValueError(\"Data path doesn't exist!\")\n",
    "        else:\n",
    "            self.data_path = path\n",
    "        self.raw_data = self._node_process()\n",
    "        self.A_tensor, self.A = self._edge_process()\n",
    "        self.senIdx, self.sen_vals, self.trainIdxtensor, self.valIdxTensor, self.testIdxTensor, self.features, self.labels = self._split_data()\n",
    "\n",
    "    def _node_process(self):\n",
    "        filenames = os.listdir(self.data_path)\n",
    "\n",
    "        for file in filenames:\n",
    "            if os.path.splitext(file)[1] != '.csv':\n",
    "                continue\n",
    "            else:\n",
    "                df_data = pd.read_csv(os.path.join(self.data_path, file))\n",
    "\n",
    "                # modify str feature\n",
    "                # df_data['GoodCustomer'].replace(-1, 0, inplace=True)\n",
    "                df_data['GoodCustomer'] = df_data['GoodCustomer'].replace(-1, 0).astype(int)\n",
    "\n",
    "                # df_data['Gender'].replace('Male', 1, inplace=True)\n",
    "                # df_data['Gender'].replace('Female', 0, inplace=True)\n",
    "                gender_map = {'Female': 0, 'Male': 1}\n",
    "                df_data['Gender'] = df_data['Gender'].map(gender_map).astype(int)\n",
    "\n",
    "                purposeList = list(df_data['PurposeOfLoan'])\n",
    "                random.shuffle(purposeList)\n",
    "                purposeDict = {}\n",
    "                index = 0\n",
    "                for pur in purposeList:\n",
    "                    if purposeDict.get(pur, None) is None:\n",
    "                        purposeDict[pur] = index\n",
    "                        index += 1\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                for key in purposeDict.keys():\n",
    "                    # df_data['PurposeOfLoan'].replace(key, purposeDict[key], inplace=True)\n",
    "                    df_data['PurposeOfLoan'] = df_data['PurposeOfLoan'].map(purposeDict).fillna(-1).astype(int)\n",
    "                    \n",
    "                return df_data\n",
    "\n",
    "    def _edge_process(self):\n",
    "        filenames = os.listdir(self.data_path)\n",
    "\n",
    "        for file in filenames:\n",
    "            if os.path.splitext(file)[1] != '.txt':\n",
    "                continue\n",
    "            else:\n",
    "                edges = np.loadtxt(os.path.join(self.data_path, file)).astype(int)\n",
    "\n",
    "                # Adjacency\n",
    "                num_dim = len(self.raw_data)\n",
    "                A = np.zeros((num_dim, num_dim))\n",
    "                for i in range(len(edges)):\n",
    "                    A[edges[i][0]][edges[i][1]] = 1\n",
    "                    A[edges[i][1]][edges[i][0]] = 1\n",
    "                sym_norm_A = symetric_normalize(A, half=False)\n",
    "                syn_norm_A_tensor = sp2sptensor(sym_norm_A)\n",
    "                return syn_norm_A_tensor, A\n",
    "\n",
    "    def _split_data(self):\n",
    "        pos_data = self.raw_data[self.raw_data['GoodCustomer']==1]\n",
    "        pos_index = list(pos_data.index)\n",
    "        neg_data = self.raw_data[self.raw_data['GoodCustomer']==0]\n",
    "        neg_index = list(neg_data.index)\n",
    "\n",
    "        # shuffle the index\n",
    "        random.seed(20)\n",
    "        random.shuffle(pos_index)\n",
    "        random.shuffle(neg_index)\n",
    "\n",
    "        # split the data\n",
    "        train_pos_idx = pos_index[:int(0.5*len(pos_index))]\n",
    "        train_neg_idx = neg_index[:int(0.5*len(neg_index))]\n",
    "        val_pos_idx = pos_index[int(0.5*len(pos_index)): int(0.75*len(pos_index))]\n",
    "        val_neg_idx = neg_index[int(0.5*len(neg_index)): int(0.75*len(neg_index))]\n",
    "        test_pos_idx = pos_index[int(0.75*len(pos_index)):]\n",
    "        test_neg_idx = neg_index[int(0.75*len(neg_index)):]\n",
    "\n",
    "        trainIdx = train_pos_idx + train_neg_idx\n",
    "        random.shuffle(trainIdx)\n",
    "        valIdx = val_pos_idx + val_neg_idx\n",
    "        random.shuffle(valIdx)\n",
    "        testIdx = test_pos_idx + test_neg_idx\n",
    "        random.shuffle(testIdx)\n",
    "\n",
    "        assert len(trainIdx)+len(valIdx)+len(testIdx) == len(self.raw_data), \"Missing data or leaking data!\"\n",
    "\n",
    "        feature_cols = list(self.raw_data.columns)\n",
    "        feature_cols.remove('GoodCustomer')\n",
    "        sen_idx = feature_cols.index('Gender')\n",
    "        sen_vals = self.raw_data['Gender'].values.astype(int)\n",
    "        feature_data = self.raw_data[feature_cols]\n",
    "        labels = self.raw_data['GoodCustomer']\n",
    "\n",
    "        # transform to tensor\n",
    "        trainIdxTensor = torch.LongTensor(trainIdx)\n",
    "        valIdxTensor = torch.LongTensor(valIdx)\n",
    "        testIdxTensor = torch.LongTensor(testIdx)\n",
    "        featuredata = torch.FloatTensor(np.array(feature_data))\n",
    "        labels = torch.LongTensor(np.array(labels))\n",
    "\n",
    "        return sen_idx, sen_vals, trainIdxTensor, valIdxTensor, testIdxTensor, featuredata, labels\n",
    "\n",
    "    def get_index(self):\n",
    "        return [self.trainIdxtensor, self.valIdxTensor, self.testIdxTensor]\n",
    "\n",
    "    def get_raw_data(self):\n",
    "        return [self.features, self.A_tensor, self.labels]\n",
    "\n",
    "    def generate_counterfactual_perturbation(self, data):\n",
    "        '''\n",
    "        Generate counterfactual data by flipping sensitive attribute.\n",
    "        :param data: Tensor\n",
    "        :return: counterfactual data\n",
    "        '''\n",
    "        feature_data = copy.deepcopy(data)\n",
    "        feature_data[:, self.senIdx] = 1 - feature_data[:, self.senIdx]\n",
    "        return feature_data\n",
    "\n",
    "    def generate_node_perturbation(self, prob: float, sen: bool = False):\n",
    "        '''\n",
    "        Perturbing node attributes, except for sensitive attributes.\n",
    "        :param prob: portion of perturbed data\n",
    "        :return: perturbed data\n",
    "        '''\n",
    "        feature_data = copy.deepcopy(self.features)\n",
    "        r = np.random.binomial(n=1, p=prob, size=feature_data.numpy().shape)\n",
    "        for i in range(len(feature_data)):\n",
    "            r[i][self.senIdx] = 0\n",
    "        noise = np.multiply(r, np.random.normal(0., 1., r.shape))\n",
    "        noise_tensor = torch.FloatTensor(noise)\n",
    "        x_hat = feature_data + noise_tensor\n",
    "\n",
    "        if sen:\n",
    "            x_hat = self.generate_counterfactual_perturbation(x_hat)\n",
    "        return x_hat\n",
    "\n",
    "    def generate_struc_perturbation(self, drop_prob: float, tensor: bool = True):\n",
    "        A = copy.deepcopy(self.A)\n",
    "        half_A = np.triu(A)\n",
    "        row, col = np.nonzero(half_A)\n",
    "        idx_perturb = np.random.binomial(n=1, p=1-drop_prob, size=row.shape)\n",
    "        broken_edges = np.where(idx_perturb==0)[0]\n",
    "        for idx in broken_edges:\n",
    "            half_A[row[idx]][col[idx]] = 0\n",
    "        new_A = symetric_normalize(half_A, half=True)\n",
    "        if tensor:\n",
    "            new_A = sp2sptensor(new_A)\n",
    "        return new_A\n",
    "\n",
    "def load_dataset_unified(dataset, sens_attr, predict_attr, path=\"./dataset/\", \n",
    "                         label_number=1000, sens_number=500, test_idx=False, seed=1127):\n",
    "    if dataset.lower() != 'german':\n",
    "        full_path = path\n",
    "        # full_path = os.path.join(path, \"pokec\")\n",
    "        return load_dataset(dataset, sens_attr, predict_attr, full_path, label_number, sens_number, test_idx, seed)\n",
    "\n",
    "    elif dataset.lower() == 'german':\n",
    "        full_path = path\n",
    "        # full_path = os.path.join(path, \"german\")\n",
    "        german_data = German(full_path)\n",
    "\n",
    "        adj = german_data.A_tensor\n",
    "        features = german_data.features\n",
    "        labels = german_data.labels\n",
    "        sens = torch.FloatTensor(german_data.sen_vals)\n",
    "        idx_train, idx_val, idx_test = german_data.get_index()\n",
    "\n",
    "        # 민감 속성 학습용 인덱스 (val/test 제외)\n",
    "        all_idx = set(range(len(sens)))\n",
    "        sens_idx = set(np.where(sens.numpy() >= 0)[0])\n",
    "        idx_sens_train = list(sens_idx - set(idx_val.numpy()) - set(idx_test.numpy()))\n",
    "        random.seed(seed)\n",
    "        random.shuffle(idx_sens_train)\n",
    "        idx_sens_train = torch.LongTensor(idx_sens_train[:sens_number])\n",
    "\n",
    "        return adj, features, labels, idx_train, idx_val, idx_test, sens, idx_sens_train\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset: {dataset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f1f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 함수\n",
    "\n",
    "# FairGNN\n",
    "class GCN_Body(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, dropout):\n",
    "        super(GCN_Body, self).__init__()\n",
    "        self.gc1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.gc2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.gc1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.gc2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.body = GCN_Body(in_channels, hidden_channels, dropout)\n",
    "        self.fc = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.body(x, edge_index)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "class GAT_body(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 in_dim,\n",
    "                 num_hidden,\n",
    "                 heads,\n",
    "                 feat_drop,\n",
    "                 attn_drop,\n",
    "                 negative_slope,\n",
    "                 residual):\n",
    "        super(GAT_body, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        self.activation = F.elu\n",
    "        self.gat_layers.append(GATConv(\n",
    "            in_dim, num_hidden, heads[0],\n",
    "            feat_drop, attn_drop, negative_slope, False, self.activation))\n",
    "        \n",
    "        # hidden layers\n",
    "        for l in range(1, num_layers):\n",
    "            self.gat_layers.append(GATConv(\n",
    "                num_hidden * heads[l-1], num_hidden, heads[l],\n",
    "                feat_drop, attn_drop, negative_slope, residual, self.activation))\n",
    "            \n",
    "        # output projection\n",
    "        self.gat_layers.append(GATConv(\n",
    "            num_hidden * heads[-2], num_hidden, heads[-1],\n",
    "            feat_drop, attn_drop, negative_slope, residual, None))\n",
    "        \n",
    "    def forward(self, g, inputs):\n",
    "        h = inputs\n",
    "        for l in range(self.num_layers):\n",
    "            h = self.gat_layers[l](g, h).flatten(1)\n",
    "        \n",
    "        # output projection\n",
    "        logits = self.gat_layers[-1](g, h).mean(1)\n",
    "\n",
    "        return logits\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 in_dim,\n",
    "                 num_hidden,\n",
    "                 num_classes,\n",
    "                 heads,\n",
    "                 feat_drop,\n",
    "                 attn_drop,\n",
    "                 negative_slope,\n",
    "                 residual):\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        self.body = GAT_body(num_layers, in_dim, num_hidden, heads, feat_drop, attn_drop, negative_slope, residual)\n",
    "        self.fc = nn.Linear(num_hidden,num_classes)\n",
    "    \n",
    "    def forward(self, g, inputs):\n",
    "\n",
    "        logits = self.body(g,inputs)\n",
    "        logits = self.fc(logits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "def get_model(model, nfeat, num_hidden=64, dropout=0.5, num_heads=1, num_layers=1, num_out_heads=1, attn_drop=0.0, negative_slope=0.2, residual=False):\n",
    "    if model == \"GCN\":\n",
    "        model = GCN_Body(nfeat, num_hidden, dropout)\n",
    "    elif model == \"GAT\":\n",
    "        heads = ([num_heads] * num_layers) + [num_out_heads]\n",
    "        model = GAT_body(num_layers, nfeat, num_hidden, heads,\n",
    "                         dropout, attn_drop, negative_slope, residual)\n",
    "    else:\n",
    "        raise ValueError(\"Model not implemented\")\n",
    "    return model\n",
    "\n",
    "class FairGNN_Q_bf(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=64, model='GCN', dropout=0.5, hidden = 128, lr=0.001, weight=1e-5, tau=0.9):\n",
    "        super(FairGNN_Q, self).__init__()\n",
    "        \n",
    "        nfeat = in_dim\n",
    "        nhid = hidden_dim\n",
    "        self.estimator = GCN(nfeat, hidden, 1, dropout)\n",
    "        self.GNN = get_model(model, nfeat)\n",
    "        self.classifier = nn.Linear(nhid, 1)\n",
    "        self.adv = nn.Linear(nhid, 1)\n",
    "\n",
    "        G_params = list(self.GNN.parameters()) + list(self.classifier.parameters()) + list(self.estimator.parameters())\n",
    "        self.optimizer_G = torch.optim.Adam(G_params, lr=lr, weight_decay=weight)\n",
    "        self.optimizer_A = torch.optim.Adam(self.adv.parameters(), lr=lr, weight_decay=weight)\n",
    "\n",
    "        self.tau = tau  # Quantile 설정\n",
    "        self.G_loss = 0\n",
    "        self.A_loss = 0\n",
    "\n",
    "        self.alpha = 4\n",
    "        self.beta = 0.01\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        s = self.estimator(g, x)\n",
    "        z = self.GNN(g, x)\n",
    "        y = self.classifier(z)\n",
    "        return y, s\n",
    "\n",
    "    def optimize(self, g, x, labels, idx_train, sens, idx_sens_train, tau):\n",
    "        self.train()\n",
    "\n",
    "        ### update GNN and Classifier\n",
    "        self.adv.requires_grad_(False)\n",
    "        self.optimizer_G.zero_grad()\n",
    "\n",
    "        s = self.estimator(g, x)\n",
    "        h = self.GNN(g, x)\n",
    "        y = self.classifier(h)\n",
    "\n",
    "        s_g = self.adv(h)\n",
    "\n",
    "        s_score = s.detach()\n",
    "        s_score[idx_sens_train] = sens[idx_sens_train].unsqueeze(1).float()\n",
    "\n",
    "        error = labels[idx_train].unsqueeze(1).float() - y[idx_train]\n",
    "        self.cls_loss = torch.mean(torch.max(tau * error, (tau - 1) * error))\n",
    "        \n",
    "        self.adv_loss = F.mse_loss(s_g, s_score)  # adversary는 MSE\n",
    "\n",
    "        # 공정성 페널티\n",
    "        cov = torch.abs(torch.mean((s_score - torch.mean(s_score)) * (y - torch.mean(y))))\n",
    "\n",
    "        self.G_loss = self.cls_loss + self.alpha * cov - self.beta * self.adv_loss\n",
    "\n",
    "        self.G_loss.backward()\n",
    "        self.optimizer_G.step()\n",
    "\n",
    "        ### update Adversary\n",
    "        self.adv.requires_grad_(True)\n",
    "        self.optimizer_A.zero_grad()\n",
    "        s_g = self.adv(h.detach())\n",
    "        self.A_loss = F.mse_loss(s_g, s_score)\n",
    "\n",
    "        self.A_loss.backward()\n",
    "        self.optimizer_A.step()\n",
    "\n",
    "class FairGNN_Q(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=64, dropout=0.5, hidden=128, lr=0.001, weight=1e-5, tau=0.9):\n",
    "        super(FairGNN_Q, self).__init__()\n",
    "        self.estimator = GCN(in_dim, hidden, 1, dropout)\n",
    "        self.GNN = GCN(in_dim, hidden_dim, hidden_dim, dropout)\n",
    "        self.classifier = nn.Linear(hidden_dim, 1)\n",
    "        self.adv = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        G_params = list(self.GNN.parameters()) + list(self.classifier.parameters()) + list(self.estimator.parameters())\n",
    "        self.optimizer_G = torch.optim.Adam(G_params, lr=lr, weight_decay=weight)\n",
    "        self.optimizer_A = torch.optim.Adam(self.adv.parameters(), lr=lr, weight_decay=weight)\n",
    "\n",
    "        self.tau = tau\n",
    "        self.G_loss = 0\n",
    "        self.A_loss = 0\n",
    "\n",
    "        self.alpha = 4\n",
    "        self.beta = 0.01\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        s = self.estimator(x, edge_index)\n",
    "        z = self.GNN(x, edge_index)\n",
    "        y = self.classifier(z)\n",
    "        return y, s\n",
    "\n",
    "    def optimize(self, data, tau):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        labels = data.y\n",
    "        idx_train = data.idx_train\n",
    "        sens = data.sensitive_attr\n",
    "        idx_sens_train = data.idx_sens_train\n",
    "\n",
    "        self.train()\n",
    "        self.adv.requires_grad_(False)\n",
    "        self.optimizer_G.zero_grad()\n",
    "\n",
    "        s = self.estimator(x, edge_index)\n",
    "        h = self.GNN(x, edge_index)\n",
    "        y = self.classifier(h)\n",
    "\n",
    "        s_g = self.adv(h)\n",
    "\n",
    "        s_score = s.detach()\n",
    "        s_score[idx_sens_train] = sens[idx_sens_train].unsqueeze(1).float()\n",
    "\n",
    "        error = labels[idx_train].unsqueeze(1).float() - y[idx_train]\n",
    "        self.cls_loss = torch.mean(torch.max(tau * error, (tau - 1) * error))\n",
    "\n",
    "        self.adv_loss = F.mse_loss(s_g, s_score)\n",
    "        cov = torch.abs(torch.mean((s_score - s_score.mean()) * (y - y.mean())))\n",
    "\n",
    "        self.G_loss = self.cls_loss + self.alpha * cov - self.beta * self.adv_loss\n",
    "        self.G_loss.backward()\n",
    "        self.optimizer_G.step()\n",
    "\n",
    "        self.adv.requires_grad_(True)\n",
    "        self.optimizer_A.zero_grad()\n",
    "        s_g = self.adv(h.detach())\n",
    "        self.A_loss = F.mse_loss(s_g, s_score)\n",
    "        self.A_loss.backward()\n",
    "        self.optimizer_A.step()\n",
    "\n",
    "# FMP\n",
    "def get_sen(sens, idx_sens_train):\n",
    "    sens_zeros = torch.zeros_like(sens)\n",
    "    sens_1 = sens \n",
    "    sens_0 = (1 - sens) \n",
    "    \n",
    "    sens_1[idx_sens_train] = sens_1[idx_sens_train] / len(sens_1[idx_sens_train])\n",
    "    sens_0[idx_sens_train] = sens_0[idx_sens_train] / len(sens_0[idx_sens_train])\n",
    "\n",
    "    sens_zeros[idx_sens_train] = sens_1[idx_sens_train] - sens_0[idx_sens_train]\n",
    "    sen_mat = torch.unsqueeze(sens_zeros, dim=0)\n",
    "    return sen_mat\n",
    "\n",
    "def check_sen(edge_index, sen):\n",
    "    nnz = edge_index.nnz()\n",
    "    deg = torch.eye(edge_index.sizes()[0]).cuda()\n",
    "    adj = edge_index.to_dense()\n",
    "    lap = (sen.t() @ sen).to_dense()\n",
    "    lap2 = deg - adj\n",
    "    diff = torch.sum(torch.abs(lap2-lap)) / nnz\n",
    "    assert diff < 0.000001, f'error: {diff} need to make sure L=B^TB'\n",
    "\n",
    "class FMP_bf(GraphConv):\n",
    "    r\"\"\"Fair message passing layer\n",
    "    \"\"\"\n",
    "    _cached_sen = Optional[SparseTensor]\n",
    "\n",
    "    def __init__(self, \n",
    "                 in_feats: int,\n",
    "                 out_feats: int,\n",
    "                 K: int, \n",
    "                 lambda1: float = None,\n",
    "                 lambda2: float = None,\n",
    "                 L2: bool = True,\n",
    "                 dropout: float = 0.,\n",
    "                 cached: bool = False):\n",
    "\n",
    "        super(FMP, self).__init__(in_feats, out_feats)\n",
    "\n",
    "        self.K = K\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.L2 = L2\n",
    "        self.dropout = dropout\n",
    "        self.cached = cached\n",
    "        self._cached_sen = None  ## sensitive matrix\n",
    "        self.propa = GraphConv(in_feats, in_feats, weight=False, bias=False, activation=None)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self._cached_sen = None\n",
    "\n",
    "    def forward(self, x: Tensor, \n",
    "                g, \n",
    "                idx_sens_train,\n",
    "                edge_weight: OptTensor = None, \n",
    "                sens=None) -> Tensor:\n",
    "        \n",
    "        if g.device != features.device:\n",
    "            g = g.to(features.device)\n",
    "\n",
    "        if self.K <= 0: return x\n",
    "\n",
    "        cache = self._cached_sen\n",
    "        if cache is None:\n",
    "            sen_mat = get_sen(sens=sens, idx_sens_train=idx_sens_train)               ## compute sensitive matrix\n",
    "\n",
    "            if self.cached:\n",
    "                self._cached_sen = sen_mat\n",
    "                self.init_z = torch.zeros((sen_mat.size()[0], x.size()[-1]))\n",
    "        else:\n",
    "            sen_mat = self._cached_sen # N,\n",
    "\n",
    "        hh = x\n",
    "        x = self.emp_forward(g, x=x, hh=hh, sen=sen_mat, K=self.K)\n",
    "        return x\n",
    "\n",
    "    def emp_forward(self, g, x, hh, K, sen):\n",
    "        lambda1 = self.lambda1\n",
    "        lambda2 = self.lambda2\n",
    "\n",
    "        gamma = 1/(1+lambda2)\n",
    "        beta = 1/(2*gamma)\n",
    "\n",
    "        for _ in range(K):\n",
    "\n",
    "            if lambda2 > 0:\n",
    "                y = gamma * hh + (1-gamma) * self.propa(g, feat=x)\n",
    "            else:\n",
    "                y = gamma * hh + (1-gamma) * x # y = x - gamma * (x - hh)\n",
    "\n",
    "            if lambda1 > 0:\n",
    "                z = sen @ F.softmax(y, dim=1) / (gamma * sen @ sen.t())\n",
    "                \n",
    "                x_bar0 = sen.t() @ z\n",
    "                x_bar1 = F.softmax(x_bar0, dim=1) ## node * feature\n",
    "\n",
    "                correct = x_bar0 * x_bar1 \n",
    "\n",
    "                coeff = torch.sum(x_bar0 * x_bar1, 1, keepdim=True)\n",
    "                correct = correct - coeff * x_bar1\n",
    "\n",
    "                x_bar = y - gamma * correct\n",
    "                z_bar  = z + beta * (sen @ F.softmax(x_bar, dim=1))\n",
    "                \n",
    "                if self.L2:\n",
    "                    z  = self.L2_projection(z_bar, lambda_=lambda1, beta=beta)\n",
    "                else:\n",
    "                    z  = self.L1_projection(z_bar, lambda_=lambda1)\n",
    "                \n",
    "                x_bar0 = sen.t() @ z\n",
    "                x_bar1 = F.softmax(x_bar0, dim=1) ## node * feature\n",
    "                \n",
    "                correct = x_bar0 * x_bar1 \n",
    "                coeff = torch.sum(x_bar0 * x_bar1, 1, keepdim=True)\n",
    "                correct = correct - coeff * x_bar1\n",
    "                x = y - gamma * correct\n",
    "            else:\n",
    "                x = y # z=0\n",
    "\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "    def L1_projection(self, x: Tensor, lambda_):\n",
    "        return torch.clamp(x, min=-lambda_, max=lambda_)\n",
    "    \n",
    "    def L2_projection(self, x: Tensor, lambda_, beta):\n",
    "        coeff = (2*lambda_) / (2*lambda_ + beta)\n",
    "        return coeff * x\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_weight: Tensor) -> Tensor:\n",
    "        return edge_weight.view(-1, 1) * x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(K={}, lambda1={}, lambda2={}, L2={})'.format(\n",
    "            self.__class__.__name__, self.K, self.lambda1, self.lambda2, self.L2)\n",
    "\n",
    "class FMP_Q_bf(torch.nn.Module):\n",
    "    def __init__(self, input_size, size, prop, num_layer):\n",
    "        super(FMP_Q, self).__init__()\n",
    "\n",
    "        self.hidden = nn.ModuleList()\n",
    "        for _ in range(num_layer - 2):\n",
    "            self.hidden.append(nn.Linear(size, size))\n",
    "\n",
    "        self.first = nn.Linear(input_size, size)\n",
    "        self.last = nn.Linear(size, 1) \n",
    "\n",
    "        self.prop = prop\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.first.reset_parameters()\n",
    "        self.last.reset_parameters()\n",
    "        self.prop.reset_parameters()\n",
    "        for layer in self.hidden:\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    def forward(self, features, g, sens, idx_sens_train):\n",
    "        x = features\n",
    "        \n",
    "        out = F.relu(self.first(x))\n",
    "        for layer in self.hidden:\n",
    "            out = F.relu(layer(out))\n",
    "        \n",
    "        x = self.last(out).squeeze(-1) \n",
    "        x = self.prop(x, sens=sens, g=g, idx_sens_train=idx_sens_train)\n",
    "        return x\n",
    "\n",
    "# FMP는 group-average correction 수준에 머물러 있어서, distribution-aware 메시지 패싱으로 발전시키려면 몇 가지 방향에서 개선\n",
    "# 각 그룹(예: 남/여, 지역0/1)의 평균값만 계산해서 예측이 한쪽 그룹으로 치우치지 않게 고쳐줌. 근데, 평균값만 보면 그 그룹 안에서 어떤 일이 벌어지는지 모름.\n",
    "class FMP(torch.nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, K, lambda1, lambda2, dropout=0.0, cached=False, L2=True):\n",
    "        super(FMP, self).__init__()\n",
    "        self.K = K   # \n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.L2 = L2\n",
    "        self.dropout = dropout\n",
    "        self.cached = cached\n",
    "        self._cached_sen = None\n",
    "        # self.propa = GCNConv(in_feats, in_feats, add_self_loops=False, normalize=True)\n",
    "        \n",
    "        self.propa = GCNConv(out_feats, out_feats, add_self_loops=False, normalize=True)  # 그래프 컨볼루션 레이어: self-loop 미포함, 메세지 전파시 degree로 normalization\n",
    "\n",
    "    def forward(self, x, edge_index, idx_sens_train, sens):\n",
    "        if self.K <= 0:\n",
    "            return x\n",
    "\n",
    "        if self._cached_sen is None:\n",
    "            sen_mat = get_sen(sens=sens, idx_sens_train=idx_sens_train)  # 민감속성 차이 계산\n",
    "            if self.cached:\n",
    "                self._cached_sen = sen_mat\n",
    "        else:\n",
    "            sen_mat = self._cached_sen\n",
    "\n",
    "        hh = x  # 원래 입력 저장: residual 연결용\n",
    "\n",
    "        for _ in range(self.K): # 메세지 패싱 K 반복\n",
    "            if self.lambda2 > 0:\n",
    "                y = (1 / (1 + self.lambda2)) * hh + (self.lambda2 / (1 + self.lambda2)) * self.propa(x, edge_index)  # propagated 정보 + 원래 입력 비율 섞기\n",
    "            else:\n",
    "                y = hh   # 원래 입력\n",
    "\n",
    "            if self.lambda1 > 0:   # 민감속성 보정 사용 여부\n",
    "                # softmax 출력으로 민감속성별 representation 계산 \n",
    "                # sen_mat: 민감속성 정보를 나타낸 행렬 \n",
    "                # 그룹별 평균 softmax 특징 계산 \n",
    "                z = sen_mat @ F.softmax(y, dim=1) / (sen_mat @ sen_mat.t() + 1e-8)\n",
    "                # 그룹 평균 다시 노드로 확산\n",
    "                x_bar0 = sen_mat.t() @ z\n",
    "                x_bar1 = F.softmax(x_bar0, dim=1)\n",
    "                \n",
    "                # 그룹간 차이 계산\n",
    "                # 각 노드의 그룹 평균과 자기 softmax 값 차이 계산 → global mean 빼서 보정값 추출\n",
    "                correct = x_bar0 * x_bar1\n",
    "                coeff = torch.sum(x_bar0 * x_bar1, dim=1, keepdim=True)\n",
    "                correct = correct - coeff * x_bar1\n",
    "\n",
    "                # 민감속성 보정 적용: 불공정성 줄임\n",
    "                x = y - (1 / (1 + self.lambda2)) * correct\n",
    "            else:\n",
    "                x = y\n",
    "\n",
    "            # dropout으로 overffiting 방지: 네트워크가 너무 특정 노드에 과적합되지 않도록 일부 뉴런을 무작위로 꺼주는 정규화 작업\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        return x\n",
    "\n",
    "class FMP_Q(torch.nn.Module):\n",
    "    def __init__(self, input_size, size, prop, num_layer):\n",
    "        super().__init__()\n",
    "        self.first = nn.Linear(input_size, size)\n",
    "        self.hidden = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layer - 2)])\n",
    "        self.last = nn.Linear(size, 1)\n",
    "        self.prop = prop\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        sens = data.sensitive_attr\n",
    "\n",
    "        out = F.relu(self.first(x))\n",
    "        for layer in self.hidden:\n",
    "            out = F.relu(layer(out))\n",
    "\n",
    "        # x = self.last(x).squeeze(-1)\n",
    "        # x = self.prop(x, edge_index=edge_index, sens=sens, idx_sens_train=data.idx_sens_train)\n",
    "\n",
    "        out = self.prop(out, edge_index=edge_index, sens=sens, idx_sens_train=data.idx_sens_train)\n",
    "\n",
    "        x = self.last(out).squeeze(-1)  # shape: [N]\n",
    "        return x\n",
    "\n",
    "def fmp_model(data, num_layers=5, lambda1=3, lambda2=3, L2=True, num_hidden=64, num_gnn_layer=2):    \n",
    "    prop = FMP(\n",
    "        # in_feats=data.num_features,\n",
    "        # out_feats=data.num_features,\n",
    "        in_feats=num_hidden,  \n",
    "        out_feats=num_hidden,\n",
    "        K=num_layers,    # 메세지 패싱 반복 횟수\n",
    "        lambda1=lambda1, # 민감속성 보정 정도\n",
    "        lambda2=lambda2, # 메세지 패싱과 residual(원래 입력) mixing 비율: oversmoothing 방지\n",
    "        L2=L2,           # L2 norm 프로젝션 여부\n",
    "        cached=False\n",
    "    )\n",
    "\n",
    "    model = FMP_Q(\n",
    "        input_size=data.num_features,\n",
    "        size=num_hidden,\n",
    "        prop=prop,\n",
    "        num_layer=num_gnn_layer\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# 기본 모델\n",
    "def quantile_loss(y_true, y_pred, tau=0.9):\n",
    "    error = y_true - y_pred\n",
    "    return torch.mean(torch.max(tau * error, (tau - 1) * error))\n",
    "\n",
    "class MYPlainGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)  # 단일 회귀 출력\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        out = self.fc(x).squeeze(-1)  # [N] 형태\n",
    "        return out\n",
    "\n",
    "class MYFairGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, lambda_fair=1.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.lambda_fair = lambda_fair\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        preds = self.fc(x).squeeze(-1)  # [N]\n",
    "        return preds\n",
    "\n",
    "    def fairness_loss(self, preds, sensitive):\n",
    "        group0 = preds[sensitive == 0]\n",
    "        group1 = preds[sensitive == 1]\n",
    "        if len(group0) == 0 or len(group1) == 0:\n",
    "            return torch.tensor(0.0, device=preds.device)\n",
    "        return torch.abs(group0.mean() - group1.mean())\n",
    "\n",
    "    def total_loss(self, preds, y_true, sensitive, tau=0.9):\n",
    "        # 기본 quantile loss\n",
    "        error = y_true - preds\n",
    "        quantile_loss = torch.mean(torch.max(tau * error, (tau - 1) * error))\n",
    "\n",
    "        # 공정성 정규화\n",
    "        fair_loss = self.fairness_loss(preds, sensitive)\n",
    "\n",
    "        return quantile_loss + self.lambda_fair * fair_loss\n",
    "    \n",
    "\n",
    "# Our\n",
    "class DAFGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, device, lambda_fair=1.0, gamma=1.0, alpha=1.0, use_projection=True, use_fairness=True, use_lambda=True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.lambda_fair = lambda_fair\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.use_projection = use_projection\n",
    "        self.use_fairness = use_fairness\n",
    "        self.use_lambda = use_lambda\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, x, edge_index, return_hidden=False):\n",
    "        h = F.relu(self.conv1(x, edge_index))\n",
    "        h = F.relu(self.conv2(h, edge_index))\n",
    "        \n",
    "        preds = self.fc(h).squeeze(-1)\n",
    "\n",
    "        if return_hidden:\n",
    "            return preds, h\n",
    "        else:\n",
    "            return preds\n",
    "        \n",
    "    def distribution_aware_fairness_loss(self, preds, sensitive, var_gap=True):\n",
    "        g0 = preds[sensitive == 0]\n",
    "        g1 = preds[sensitive == 1]\n",
    "\n",
    "        if len(g0) == 0 or len(g1) == 0:\n",
    "            return torch.tensor(0.0, device=preds.device)\n",
    "\n",
    "        mean_gap = torch.abs(g0.mean() - g1.mean())\n",
    "        gap = mean_gap\n",
    "        \n",
    "        if var_gap:\n",
    "            var_gap = torch.abs(g0.var(unbiased=False) - g1.var(unbiased=False))\n",
    "            gap = mean_gap + self.alpha * var_gap\n",
    "        return gap\n",
    "\n",
    "    def compute_bias_score(self, hidden, sensitive):\n",
    "        g0 = hidden[sensitive == 0]\n",
    "        g1 = hidden[sensitive == 1]\n",
    "        bias = torch.norm(g0.mean(dim=0) - g1.mean(dim=0), p=2)\n",
    "        return bias\n",
    "\n",
    "    def adaptive_projection(self, h, bias_score):\n",
    "        h_norm = h.norm(p=2, dim=1, keepdim=True) + 1e-6\n",
    "        adaptive_lambda = self.lambda_fair * (1 + self.gamma * bias_score)\n",
    "        scale = torch.min(torch.ones_like(h_norm), adaptive_lambda.unsqueeze(-1) / h_norm)\n",
    "        return h * scale\n",
    "\n",
    "    def hierarchical_lambda(self, epoch, max_lambda=1.0, beta=0.05):\n",
    "        device = next(self.parameters()).device  # 모델 파라미터의 디바이스 확인\n",
    "        return max_lambda * (1 - torch.exp(-beta * torch.tensor(epoch, dtype=torch.float32, device=device)))\n",
    "\n",
    "    def total_loss(self, preds, hidden, sensitive, y_true, epoch, tau=0.9):\n",
    "        \n",
    "        loss_qr = quantile_loss(y_true, preds, tau)\n",
    "        \n",
    "        # Projection\n",
    "        if self.use_projection:\n",
    "            bias_score = self.compute_bias_score(hidden, sensitive)\n",
    "            hidden = self.adaptive_projection(hidden, bias_score)\n",
    "        \n",
    "        # Fairness\n",
    "        if self.use_fairness:\n",
    "            fair_loss = self.distribution_aware_fairness_loss(preds, sensitive)  \n",
    "        else:\n",
    "            fair_loss =  torch.tensor(0.0, device=preds.device)\n",
    "\n",
    "        # if self.use_lambda:\n",
    "        #     lambda_fair = self.hierarchical_lambda(epoch, max_lambda=self.lambda_fair)\n",
    "        # else:\n",
    "        #     lambda_fair = self.lambda_fair\n",
    "\n",
    "        return loss_qr + lambda_fair * fair_loss\n",
    "    \n",
    "def get_sen_da(sens, idx_sens_train):\n",
    "    num_nodes = sens.size(0)\n",
    "    group_0_mask = (sens == 0)\n",
    "    group_1_mask = (sens == 1)\n",
    "\n",
    "    # idx_sens_train에서 그룹별 마스크\n",
    "    train_group_0_mask = group_0_mask.clone()\n",
    "    train_group_1_mask = group_1_mask.clone()\n",
    "    train_group_0_mask[~idx_sens_train] = False\n",
    "    train_group_1_mask[~idx_sens_train] = False\n",
    "\n",
    "    group_0 = group_0_mask.float()\n",
    "    group_1 = group_1_mask.float()\n",
    "\n",
    "    sum_group_0 = train_group_0_mask.float().sum()\n",
    "    sum_group_1 = train_group_1_mask.float().sum()\n",
    "    if sum_group_0 == 0 or sum_group_1 == 0:\n",
    "        raise ValueError(f\"get_sen_da: One of the groups has no samples in idx_sens_train! group_0 sum: {sum_group_0}, group_1 sum: {sum_group_1}\")\n",
    "\n",
    "    group_0[idx_sens_train] /= sum_group_0\n",
    "    group_1[idx_sens_train] /= sum_group_1\n",
    "\n",
    "    sen_mat = torch.stack([group_0, group_1], dim=0)\n",
    "    return sen_mat\n",
    "\n",
    "class DAFMP(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, K, dropout=0.0, cached=False):\n",
    "        super(DAFMP, self).__init__()\n",
    "        self.K = K\n",
    "        self.dropout = dropout\n",
    "        self.cached = cached\n",
    "        self._cached_sen = None\n",
    "        self.propa = GATConv(in_feats, out_feats, heads=1)\n",
    "\n",
    "    def forward(self, x, edge_index, idx_sens_train, sens, tau, mean_baseline=1.0, var_baseline=1.0, range_baseline=1.0):\n",
    "        if self.K <= 0:\n",
    "            return x, None, None, None\n",
    "\n",
    "        if self._cached_sen is None:\n",
    "            sen_mat = get_sen_da(sens=sens, idx_sens_train=idx_sens_train)\n",
    "            if self.cached:\n",
    "                self._cached_sen = sen_mat\n",
    "        else:\n",
    "            sen_mat = self._cached_sen\n",
    "\n",
    "        for _ in range(self.K):\n",
    "            y = self.propa(x, edge_index)\n",
    "\n",
    "            group_counts = sen_mat.sum(dim=1, keepdim=True)\n",
    "            z_mean = sen_mat @ y / (sen_mat @ torch.ones_like(y))\n",
    "            z_var = (sen_mat @ (y ** 2)) / (group_counts + 1e-8) - z_mean ** 2\n",
    "\n",
    "            group0_mask = sen_mat[0].bool()\n",
    "            group1_mask = sen_mat[1].bool()\n",
    "            y_group0 = y[group0_mask]\n",
    "            y_group1 = y[group1_mask]\n",
    "\n",
    "            if y_group0.size(0) == 0 or y_group1.size(0) == 0:\n",
    "                raise ValueError(\"One of the groups has no samples! Check your sens and idx_sens_train.\")\n",
    "\n",
    "            z_min0, _ = torch.min(y_group0, dim=0, keepdim=True)\n",
    "            z_max0, _ = torch.max(y_group0, dim=0, keepdim=True)\n",
    "            z_min1, _ = torch.min(y_group1, dim=0, keepdim=True)\n",
    "            z_max1, _ = torch.max(y_group1, dim=0, keepdim=True)\n",
    "\n",
    "            z_range0 = z_max0 - z_min0\n",
    "            z_range1 = z_max1 - z_min1\n",
    "\n",
    "            mean_gap = torch.abs(z_mean[0] - z_mean[1])\n",
    "            var_gap = torch.abs(z_var[0] - z_var[1])\n",
    "            range_gap = torch.abs(z_range0 - z_range1)\n",
    "\n",
    "            mean_gap_norm = mean_gap / (mean_baseline + 1e-8)\n",
    "            var_gap_norm = var_gap / (var_baseline + 1e-8)\n",
    "            range_gap_norm = range_gap / (range_baseline + 1e-8)\n",
    "\n",
    "            tau_scalar = tau.mean() if isinstance(tau, torch.Tensor) and tau.numel() > 1 else tau\n",
    "\n",
    "            correction = tau_scalar * (mean_gap_norm + var_gap_norm + range_gap_norm)\n",
    "\n",
    "            x = y - correction\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        return x, mean_gap, var_gap, range_gap\n",
    "\n",
    "class DAFGNN_2(nn.Module):\n",
    "    def __init__(self, input_size, size, prop, num_layer):\n",
    "        super().__init__()\n",
    "        self.first = nn.Linear(input_size, size)\n",
    "        self.hidden = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layer - 2)])\n",
    "        self.last = nn.Linear(size, 1)\n",
    "        self.prop = prop\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        sens = data.sensitive_attr\n",
    "        tau = data.quantile_tau\n",
    "\n",
    "        out = F.relu(self.first(x))\n",
    "        for layer in self.hidden:\n",
    "            out = F.relu(layer(out))\n",
    "\n",
    "        # DAFMP 호출 후 tuple 언팩\n",
    "        out, mean_gap, var_gap, range_gap = self.prop(\n",
    "            out, edge_index=edge_index, sens=sens, idx_sens_train=data.idx_sens_train, tau=tau\n",
    "        )\n",
    "\n",
    "        x = self.last(out).squeeze(-1)\n",
    "        return x, mean_gap, var_gap, range_gap\n",
    "\n",
    "def daf_model(data, num_layers=5, num_hidden=64, num_gnn_layer=2):\n",
    "    prop = DAFMP(\n",
    "        in_feats=num_hidden,\n",
    "        out_feats=num_hidden,\n",
    "        K=num_layers,\n",
    "        cached=False\n",
    "    )\n",
    "\n",
    "    model = DAFGNN_2(\n",
    "        input_size=data.num_features,\n",
    "        size=num_hidden,\n",
    "        prop=prop,\n",
    "        num_layer=num_gnn_layer\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def daf_loss(y_true, y_pred, tau, mean_gap, var_gap, range_gap, lambda_fair, mean_baseline=1.0, var_baseline=1.0, range_baseline=1.0):\n",
    "    diff = y_true - y_pred\n",
    "    qr_loss = torch.mean(torch.max(tau * diff, (tau - 1) * diff))\n",
    "\n",
    "    mean_gap_norm = mean_gap / (mean_baseline + 1e-8)\n",
    "    var_gap_norm = var_gap / (var_baseline + 1e-8)\n",
    "    range_gap_norm = range_gap / (range_baseline + 1e-8)\n",
    "\n",
    "    # Reduce to scalar\n",
    "    fairness_loss = lambda_fair * (mean_gap_norm.mean() + var_gap_norm.mean() + range_gap_norm.mean())\n",
    "\n",
    "    return qr_loss + fairness_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f2136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928f3f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b980eebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정\n",
    "tau=0.9\n",
    "lambda_fair=1.0\n",
    "hidden_dim=64\n",
    "n_epochs=500\n",
    "lr=0.01\n",
    "weight=5e-4\n",
    "seed = 1127\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7c1967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드\n",
    "\n",
    "## 데이터셋 선택\n",
    "dataset = 'region_job' # Pokec_z \n",
    "# dataset = 'region_job_2' # Pokec_n\n",
    "# dataset = 'nba' # NBA\n",
    "# dataset = 'german' # German\n",
    "\n",
    "# completion_percentage: 프로필을 얼마나 채웠는가 (%). 자발적 행동의 간접 지표.\n",
    "pokec_predict_attr = 'completion_percentage'\n",
    "pokec_sens_attr = 'region' # gender\n",
    "pokec_label_number, pokec_sens_number, pokec_test_idx, pokec_path = 500, 200, False, \"./dataset/pokec\"\n",
    "\n",
    "# PIE (Player Impact Estimate): 통합 퍼포먼스 지표, 퍼포먼스에 대한 차별 여부 확인 가능\n",
    "# MPG (Minutes Per Game): 출전 시간 → 팀의 코치 결정, 제도적 편향 가능성\n",
    "nba_predict_attr = 'PIE'\n",
    "nba_sens_attr = 'country'  # AGE, palyer_height, player_weight\n",
    "nba_label_number, nba_sens_number, nba_test_idx, nba_path = 100, 50, True, \"./dataset/NBA\"\n",
    "\n",
    "# German\n",
    "german_predict_attr = 'LoanAmount' # LoanAmount(대출 금액), LoanRateAsPercentOfIncome(소득대비상환비율), YearsAtCurrentHome(거주연수)\n",
    "german_sens_attr = 'Gender' # Gender, ForeignWorker, Single(독신여부), HasTelephone(전화기보유여부), OwnsHouse(주택소유여부), Unemployed(실직상태여부) Age(고령자로 나눠서 가능) 등등등..\n",
    "german_path = './dataset/NIFTY'\n",
    "\n",
    "if dataset == 'german':\n",
    "    df, col, group = pd.read_csv(f'{german_path}/{dataset}.csv'), german_predict_attr, german_sens_attr\n",
    "elif dataset == 'nba':\n",
    "    df, col, group = pd.read_csv(f'{nba_path}/{dataset}.csv'), nba_predict_attr, nba_sens_attr\n",
    "else:\n",
    "    df, col, group = pd.read_csv(f'{pokec_path}/{dataset}.csv'), pokec_predict_attr, pokec_sens_attr\n",
    "\n",
    "if dataset == 'nba':\n",
    "    dn = 'NBA'\n",
    "elif dataset == 'german':\n",
    "    dn = 'German'\n",
    "else:\n",
    "    dn = 'Pokec_z' if dataset == 'region_job' else 'Pokec_n'\n",
    "\n",
    "\n",
    "if dataset == 'nba':\n",
    "    predict_attr, sens_attr, label_number, sens_number, test_idx, path = nba_predict_attr, nba_sens_attr, nba_label_number, nba_sens_number, nba_test_idx, nba_path\n",
    "elif dataset == 'german':\n",
    "    predict_attr, sens_attr, path = german_predict_attr, german_sens_attr, german_path\n",
    "else:\n",
    "    predict_attr, sens_attr, label_number, sens_number, test_idx, path = pokec_predict_attr, pokec_sens_attr, pokec_label_number, pokec_sens_number, pokec_test_idx, pokec_path\n",
    "\n",
    "adj, features, labels, idx_train, idx_val, idx_test, sens, idx_sens_train = load_dataset_unified(dataset, sens_attr, predict_attr, path, label_number, sens_number, test_idx, seed=seed)\n",
    "\n",
    "print(\"sens 0 count:\", torch.sum(sens == 0).item())\n",
    "print(\"sens 1 count:\", torch.sum(sens == 1).item())\n",
    "\n",
    "if dataset == 'nba':\n",
    "    features = feature_norm(features)\n",
    "\n",
    "if isinstance(adj, torch.Tensor):  # torch.sparse_coo_tensor\n",
    "    src, dst = adj._indices()\n",
    "    g = dgl.graph((src, dst))\n",
    "    edge_index = adj._indices()\n",
    "else:  # scipy sparse matrix\n",
    "    g = dgl.from_scipy(adj)\n",
    "    adj_coo = adj.tocoo()\n",
    "    edge_index = torch.tensor([adj_coo.row, adj_coo.col], dtype=torch.long)\n",
    "\n",
    "data = Data(x=features, edge_index=edge_index, y=labels.float(), sensitive_attr=sens, quantile_tau=torch.full((features.shape[0],), tau))\n",
    "data.idx_train = idx_train\n",
    "data.idx_val = idx_val\n",
    "data.idx_test = idx_test\n",
    "data.idx_sens_train = idx_sens_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df1340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델\n",
    "in_dim = data.x.size(1)\n",
    "data = data.to(device)\n",
    "data.sensitive_attr = data.sensitive_attr.to(device)\n",
    "\n",
    "fgnn_model = MYFairGNN(in_dim, hidden_dim, lambda_fair).to(device)\n",
    "bgnn_model = MYPlainGNN(in_dim, hidden_dim).to(device)\n",
    "fairgnn_model = FairGNN_Q(in_dim, hidden_dim, tau).to(device)\n",
    "Fmp_model = fmp_model(data).to(device)\n",
    "\n",
    "dafgnn_model = DAFGNN(in_dim, hidden_dim, device, lambda_fair).to(device)\n",
    "dafgnn2_model = daf_model(data).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c36ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "optimizer = Adam(dafgnn_model.parameters(), lr=lr, weight_decay=weight)\n",
    "\n",
    "print('-' * 100)\n",
    "n_epochs = int(n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    dafgnn_model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    preds, hidden = dafgnn_model(data.x, data.edge_index, return_hidden=True)\n",
    "    loss = dafgnn_model.total_loss(preds[data.idx_train], hidden[data.idx_train], data.sensitive_attr[data.idx_train], data.y[data.idx_train], epoch, tau)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0 or epoch == 1:\n",
    "        print(f\"[Epoch {epoch}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# 모델 평가\n",
    "dafgnn_model.eval()\n",
    "preds, _ = dafgnn_model(data.x, data.edge_index, return_hidden=True)\n",
    "\n",
    "# Tensor → numpy\n",
    "y_pred = preds[idx_eval].detach().cpu().numpy().flatten()\n",
    "y_true = data.y[idx_eval].detach().cpu().numpy().flatten()\n",
    "sensitive_attr = data.sensitive_attr[idx_eval].detach().cpu().numpy().flatten()\n",
    "\n",
    "# 기본 오류 계산\n",
    "error = y_true - y_pred\n",
    "abs_error = np.abs(error)\n",
    "sq_error = error ** 2\n",
    "\n",
    "# MAPE 관련 계산 시 y_true가 너무 작으면 제거\n",
    "mask = np.abs(y_true) > 1e-4\n",
    "mape_safe = abs_error[mask] / (np.abs(y_true[mask]) + 1e-8)\n",
    "mape = np.mean(mape_safe)\n",
    "mape_var = np.var(mape_safe)\n",
    "\n",
    "# QR Loss\n",
    "error_torch = torch.tensor(error)\n",
    "qr_loss = torch.mean(torch.max(tau * error_torch, (tau - 1) * error_torch)).item()\n",
    "\n",
    "# 정확도\n",
    "mae = np.mean(abs_error)\n",
    "rmse = np.sqrt(np.mean(sq_error))\n",
    "mae_var = np.var(abs_error)\n",
    "r2 = 1 - np.sum(sq_error) / np.sum((y_true - y_true.mean()) ** 2)\n",
    "\n",
    "# 그룹별\n",
    "df = pd.DataFrame({\n",
    "    'y_true': y_true,\n",
    "    'y_pred': y_pred,\n",
    "    'group': sensitive_attr\n",
    "})\n",
    "\n",
    "group_metrics = {}\n",
    "for group_id in np.unique(sensitive_attr):\n",
    "    group = df[df['group'] == group_id]\n",
    "    g_m = np.mean(group['y_pred'])\n",
    "    g_err = np.abs(group['y_true'] - group['y_pred'])\n",
    "    g_sq = (group['y_true'] - group['y_pred']) ** 2\n",
    "\n",
    "    group_metrics[f'mean_group_{int(group_id)}'] = g_m\n",
    "    group_metrics[f'mae_group_{int(group_id)}'] = g_err.mean()\n",
    "    group_metrics[f'rmse_group_{int(group_id)}'] = np.sqrt(g_sq.mean())\n",
    "\n",
    "mean_gap = abs(group_metrics.get('mean_group_0', 0) - group_metrics.get('mean_group_1', 0))\n",
    "mae_gap = abs(group_metrics.get('mae_group_0', 0) - group_metrics.get('mae_group_1', 0))\n",
    "rmse_gap = abs(group_metrics.get('rmse_group_0', 0) - group_metrics.get('rmse_group_1', 0))\n",
    "\n",
    "# 민감속성과 오차의 상관관계\n",
    "corr_error = np.corrcoef(abs_error, sensitive_attr)[0, 1]    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8686953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "optimizer = Adam(dafgnn2_model.parameters(), lr=lr, weight_decay=weight)\n",
    "\n",
    "print('-' * 100)\n",
    "n_epochs = int(n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    dafgnn2_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    preds, mean_gap, var_gap, range_gap = dafgnn2_model(data)\n",
    "    loss = daf_loss(data.y[data.idx_train], preds[data.idx_train], tau, mean_gap, var_gap, range_gap, lambda_fair)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0 or epoch == 1:\n",
    "        print(f\"[Epoch {epoch}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# 모델 평가\n",
    "dafgnn2_model.eval()\n",
    "preds_tuple = dafgnn2_model(data)\n",
    "if isinstance(preds_tuple, tuple):\n",
    "    preds = preds_tuple[0]\n",
    "else:\n",
    "    preds = preds_tuple\n",
    "\n",
    "# Tensor → numpy\n",
    "y_pred = preds[idx_eval].detach().cpu().numpy().flatten()\n",
    "y_true = data.y[idx_eval].detach().cpu().numpy().flatten()\n",
    "sensitive_attr = data.sensitive_attr[idx_eval].detach().cpu().numpy().flatten()\n",
    "\n",
    "# 기본 오류 계산\n",
    "error = y_true - y_pred\n",
    "abs_error = np.abs(error)\n",
    "sq_error = error ** 2\n",
    "\n",
    "# ⚠️ MAPE 관련 계산 시 y_true가 너무 작으면 제거\n",
    "mask = np.abs(y_true) > 1e-4\n",
    "mape_safe = abs_error[mask] / (np.abs(y_true[mask]) + 1e-8)\n",
    "mape = np.mean(mape_safe)\n",
    "mape_var = np.var(mape_safe)\n",
    "\n",
    "# QR Loss\n",
    "error_torch = torch.tensor(error)\n",
    "qr_loss = torch.mean(torch.max(tau * error_torch, (tau - 1) * error_torch)).item()\n",
    "\n",
    "# 정확도\n",
    "mae = np.mean(abs_error)\n",
    "rmse = np.sqrt(np.mean(sq_error))\n",
    "mae_var = np.var(abs_error)\n",
    "r2 = 1 - np.sum(sq_error) / np.sum((y_true - y_true.mean()) ** 2)\n",
    "\n",
    "# 그룹별\n",
    "df = pd.DataFrame({\n",
    "    'y_true': y_true,\n",
    "    'y_pred': y_pred,\n",
    "    'group': sensitive_attr\n",
    "})\n",
    "\n",
    "group_metrics = {}\n",
    "for group_id in np.unique(sensitive_attr):\n",
    "    group = df[df['group'] == group_id]\n",
    "    g_m = np.mean(group['y_pred'])\n",
    "    g_err = np.abs(group['y_true'] - group['y_pred'])\n",
    "    g_sq = (group['y_true'] - group['y_pred']) ** 2\n",
    "\n",
    "    group_metrics[f'mean_group_{int(group_id)}'] = g_m\n",
    "    group_metrics[f'mae_group_{int(group_id)}'] = g_err.mean()\n",
    "    group_metrics[f'rmse_group_{int(group_id)}'] = np.sqrt(g_sq.mean())\n",
    "\n",
    "mean_gap = abs(group_metrics.get('mean_group_0', 0) - group_metrics.get('mean_group_1', 0))\n",
    "mae_gap = abs(group_metrics.get('mae_group_0', 0) - group_metrics.get('mae_group_1', 0))\n",
    "rmse_gap = abs(group_metrics.get('rmse_group_0', 0) - group_metrics.get('rmse_group_1', 0))\n",
    "\n",
    "# 민감속성과 오차의 상관관계\n",
    "corr_error = np.corrcoef(abs_error, sensitive_attr)[0, 1]    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0222822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "optimizer = Adam(fairgnn_model.parameters(), lr=lr, weight_decay=weight)\n",
    "\n",
    "print('-' * 100)\n",
    "n_epochs = int(n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    fairgnn_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    fairgnn_model.optimize(data, tau)\n",
    "    loss = fairgnn_model.G_loss \n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0 or epoch == 1:\n",
    "        print(f\"[Epoch {epoch}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# 모델 평가\n",
    "fairgnn_model.eval()\n",
    "preds, _ = fairgnn_model(data)\n",
    "\n",
    "# Tensor → numpy\n",
    "y_pred = preds[idx_eval].detach().cpu().numpy().flatten()\n",
    "y_true = data.y[idx_eval].detach().cpu().numpy().flatten()\n",
    "sensitive_attr = data.sensitive_attr[idx_eval].detach().cpu().numpy().flatten()\n",
    "\n",
    "# 기본 오류 계산\n",
    "error = y_true - y_pred\n",
    "abs_error = np.abs(error)\n",
    "sq_error = error ** 2\n",
    "\n",
    "# MAPE 관련 계산 시 y_true가 너무 작으면 제거\n",
    "mask = np.abs(y_true) > 1e-4\n",
    "mape_safe = abs_error[mask] / (np.abs(y_true[mask]) + 1e-8)\n",
    "mape = np.mean(mape_safe)\n",
    "mape_var = np.var(mape_safe)\n",
    "\n",
    "# QR Loss\n",
    "error_torch = torch.tensor(error)\n",
    "qr_loss = torch.mean(torch.max(tau * error_torch, (tau - 1) * error_torch)).item()\n",
    "\n",
    "# 정확도\n",
    "mae = np.mean(abs_error)\n",
    "rmse = np.sqrt(np.mean(sq_error))\n",
    "mae_var = np.var(abs_error)\n",
    "r2 = 1 - np.sum(sq_error) / np.sum((y_true - y_true.mean()) ** 2)\n",
    "\n",
    "# 그룹별\n",
    "df = pd.DataFrame({\n",
    "    'y_true': y_true,\n",
    "    'y_pred': y_pred,\n",
    "    'group': sensitive_attr\n",
    "})\n",
    "\n",
    "group_metrics = {}\n",
    "for group_id in np.unique(sensitive_attr):\n",
    "    group = df[df['group'] == group_id]\n",
    "    g_m = np.mean(group['y_pred'])\n",
    "    g_err = np.abs(group['y_true'] - group['y_pred'])\n",
    "    g_sq = (group['y_true'] - group['y_pred']) ** 2\n",
    "\n",
    "    group_metrics[f'mean_group_{int(group_id)}'] = g_m\n",
    "    group_metrics[f'mae_group_{int(group_id)}'] = g_err.mean()\n",
    "    group_metrics[f'rmse_group_{int(group_id)}'] = np.sqrt(g_sq.mean())\n",
    "\n",
    "mean_gap = abs(group_metrics.get('mean_group_0', 0) - group_metrics.get('mean_group_1', 0))\n",
    "mae_gap = abs(group_metrics.get('mae_group_0', 0) - group_metrics.get('mae_group_1', 0))\n",
    "rmse_gap = abs(group_metrics.get('rmse_group_0', 0) - group_metrics.get('rmse_group_1', 0))\n",
    "\n",
    "# 민감속성과 오차의 상관관계\n",
    "corr_error = np.corrcoef(abs_error, sensitive_attr)[0, 1]    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2563b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "optimizer = Adam(Fmp_model.parameters(), lr=lr, weight_decay=weight)\n",
    "\n",
    "print('-' * 100)\n",
    "n_epochs = int(n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    Fmp_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    preds = Fmp_model(data)\n",
    "    loss = quantile_loss(data.y[data.idx_train], preds[data.idx_train], tau)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0 or epoch == 1:\n",
    "        print(f\"[Epoch {epoch}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# 모델 평가\n",
    "Fmp_model.eval()\n",
    "preds = Fmp_model(data)\n",
    "\n",
    "# Tensor → numpy\n",
    "y_pred = preds[idx_eval].detach().cpu().numpy().flatten()\n",
    "y_true = data.y[idx_eval].detach().cpu().numpy().flatten()\n",
    "sensitive_attr = data.sensitive_attr[idx_eval].detach().cpu().numpy().flatten()\n",
    "\n",
    "# 기본 오류 계산\n",
    "error = y_true - y_pred\n",
    "abs_error = np.abs(error)\n",
    "sq_error = error ** 2\n",
    "\n",
    "# MAPE 관련 계산 시 y_true가 너무 작으면 제거\n",
    "mask = np.abs(y_true) > 1e-4\n",
    "mape_safe = abs_error[mask] / (np.abs(y_true[mask]) + 1e-8)\n",
    "mape = np.mean(mape_safe)\n",
    "mape_var = np.var(mape_safe)\n",
    "\n",
    "# QR Loss\n",
    "error_torch = torch.tensor(error)\n",
    "qr_loss = torch.mean(torch.max(tau * error_torch, (tau - 1) * error_torch)).item()\n",
    "\n",
    "# 정확도\n",
    "mae = np.mean(abs_error)\n",
    "rmse = np.sqrt(np.mean(sq_error))\n",
    "mae_var = np.var(abs_error)\n",
    "r2 = 1 - np.sum(sq_error) / np.sum((y_true - y_true.mean()) ** 2)\n",
    "\n",
    "# 그룹별\n",
    "df = pd.DataFrame({\n",
    "    'y_true': y_true,\n",
    "    'y_pred': y_pred,\n",
    "    'group': sensitive_attr\n",
    "})\n",
    "\n",
    "group_metrics = {}\n",
    "for group_id in np.unique(sensitive_attr):\n",
    "    group = df[df['group'] == group_id]\n",
    "    g_m = np.mean(group['y_pred'])\n",
    "    g_err = np.abs(group['y_true'] - group['y_pred'])\n",
    "    g_sq = (group['y_true'] - group['y_pred']) ** 2\n",
    "\n",
    "    group_metrics[f'mean_group_{int(group_id)}'] = g_m\n",
    "    group_metrics[f'mae_group_{int(group_id)}'] = g_err.mean()\n",
    "    group_metrics[f'rmse_group_{int(group_id)}'] = np.sqrt(g_sq.mean())\n",
    "\n",
    "mean_gap = abs(group_metrics.get('mean_group_0', 0) - group_metrics.get('mean_group_1', 0))\n",
    "mae_gap = abs(group_metrics.get('mae_group_0', 0) - group_metrics.get('mae_group_1', 0))\n",
    "rmse_gap = abs(group_metrics.get('rmse_group_0', 0) - group_metrics.get('rmse_group_1', 0))\n",
    "\n",
    "# 민감속성과 오차의 상관관계\n",
    "corr_error = np.corrcoef(abs_error, sensitive_attr)[0, 1]    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8927a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "optimizer = Adam(fgnn_model.parameters(), lr=lr, weight_decay=weight)\n",
    "\n",
    "print('-' * 100)\n",
    "n_epochs = int(n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    fgnn_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    preds = fgnn_model(data.x, data.edge_index)\n",
    "    loss = fgnn_model.total_loss(preds[data.idx_train], data.y[data.idx_train], data.sensitive_attr[data.idx_train], tau)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0 or epoch == 1:\n",
    "        print(f\"[Epoch {epoch}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# 모델 평가\n",
    "fgnn_model.eval()\n",
    "preds = fgnn_model(data.x, data.edge_index)\n",
    "\n",
    "# Tensor → numpy\n",
    "y_pred = preds[idx_eval].detach().cpu().numpy().flatten()\n",
    "y_true = data.y[idx_eval].detach().cpu().numpy().flatten()\n",
    "sensitive_attr = data.sensitive_attr[idx_eval].detach().cpu().numpy().flatten()\n",
    "\n",
    "# 기본 오류 계산\n",
    "error = y_true - y_pred\n",
    "abs_error = np.abs(error)\n",
    "sq_error = error ** 2\n",
    "\n",
    "# MAPE 관련 계산 시 y_true가 너무 작으면 제거\n",
    "mask = np.abs(y_true) > 1e-4\n",
    "mape_safe = abs_error[mask] / (np.abs(y_true[mask]) + 1e-8)\n",
    "mape = np.mean(mape_safe)\n",
    "mape_var = np.var(mape_safe)\n",
    "\n",
    "# QR Loss\n",
    "error_torch = torch.tensor(error)\n",
    "qr_loss = torch.mean(torch.max(tau * error_torch, (tau - 1) * error_torch)).item()\n",
    "\n",
    "# 정확도\n",
    "mae = np.mean(abs_error)\n",
    "rmse = np.sqrt(np.mean(sq_error))\n",
    "mae_var = np.var(abs_error)\n",
    "r2 = 1 - np.sum(sq_error) / np.sum((y_true - y_true.mean()) ** 2)\n",
    "\n",
    "# 그룹별\n",
    "df = pd.DataFrame({\n",
    "    'y_true': y_true,\n",
    "    'y_pred': y_pred,\n",
    "    'group': sensitive_attr\n",
    "})\n",
    "\n",
    "group_metrics = {}\n",
    "for group_id in np.unique(sensitive_attr):\n",
    "    group = df[df['group'] == group_id]\n",
    "    g_m = np.mean(group['y_pred'])\n",
    "    g_err = np.abs(group['y_true'] - group['y_pred'])\n",
    "    g_sq = (group['y_true'] - group['y_pred']) ** 2\n",
    "\n",
    "    group_metrics[f'mean_group_{int(group_id)}'] = g_m\n",
    "    group_metrics[f'mae_group_{int(group_id)}'] = g_err.mean()\n",
    "    group_metrics[f'rmse_group_{int(group_id)}'] = np.sqrt(g_sq.mean())\n",
    "\n",
    "mean_gap = abs(group_metrics.get('mean_group_0', 0) - group_metrics.get('mean_group_1', 0))\n",
    "mae_gap = abs(group_metrics.get('mae_group_0', 0) - group_metrics.get('mae_group_1', 0))\n",
    "rmse_gap = abs(group_metrics.get('rmse_group_0', 0) - group_metrics.get('rmse_group_1', 0))\n",
    "\n",
    "# 민감속성과 오차의 상관관계\n",
    "corr_error = np.corrcoef(abs_error, sensitive_attr)[0, 1]    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f763d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "optimizer = Adam(bgnn_model.parameters(), lr=lr, weight_decay=weight)\n",
    "\n",
    "print('-' * 100)\n",
    "n_epochs = int(n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    bgnn_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    preds = bgnn_model(data.x, data.edge_index)\n",
    "    loss = quantile_loss(data.y[data.idx_train], preds[data.idx_train], tau)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0 or epoch == 1:\n",
    "        print(f\"[Epoch {epoch}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# 모델 평가\n",
    "bgnn_model.eval()\n",
    "preds = bgnn_model(data.x, data.edge_index)\n",
    "\n",
    "# Tensor → numpy\n",
    "y_pred = preds[idx_eval].detach().cpu().numpy().flatten()\n",
    "y_true = data.y[idx_eval].detach().cpu().numpy().flatten()\n",
    "sensitive_attr = data.sensitive_attr[idx_eval].detach().cpu().numpy().flatten()\n",
    "\n",
    "# 기본 오류 계산\n",
    "error = y_true - y_pred\n",
    "abs_error = np.abs(error)\n",
    "sq_error = error ** 2\n",
    "\n",
    "# MAPE 관련 계산 시 y_true가 너무 작으면 제거\n",
    "mask = np.abs(y_true) > 1e-4\n",
    "mape_safe = abs_error[mask] / (np.abs(y_true[mask]) + 1e-8)\n",
    "mape = np.mean(mape_safe)\n",
    "mape_var = np.var(mape_safe)\n",
    "\n",
    "# QR Loss\n",
    "error_torch = torch.tensor(error)\n",
    "qr_loss = torch.mean(torch.max(tau * error_torch, (tau - 1) * error_torch)).item()\n",
    "\n",
    "# 정확도\n",
    "mae = np.mean(abs_error)\n",
    "rmse = np.sqrt(np.mean(sq_error))\n",
    "mae_var = np.var(abs_error)\n",
    "r2 = 1 - np.sum(sq_error) / np.sum((y_true - y_true.mean()) ** 2)\n",
    "\n",
    "# 그룹별\n",
    "df = pd.DataFrame({\n",
    "    'y_true': y_true,\n",
    "    'y_pred': y_pred,\n",
    "    'group': sensitive_attr\n",
    "})\n",
    "\n",
    "group_metrics = {}\n",
    "for group_id in np.unique(sensitive_attr):\n",
    "    group = df[df['group'] == group_id]\n",
    "    g_m = np.mean(group['y_pred'])\n",
    "    g_err = np.abs(group['y_true'] - group['y_pred'])\n",
    "    g_sq = (group['y_true'] - group['y_pred']) ** 2\n",
    "\n",
    "    group_metrics[f'mean_group_{int(group_id)}'] = g_m\n",
    "    group_metrics[f'mae_group_{int(group_id)}'] = g_err.mean()\n",
    "    group_metrics[f'rmse_group_{int(group_id)}'] = np.sqrt(g_sq.mean())\n",
    "\n",
    "mean_gap = abs(group_metrics.get('mean_group_0', 0) - group_metrics.get('mean_group_1', 0))\n",
    "mae_gap = abs(group_metrics.get('mae_group_0', 0) - group_metrics.get('mae_group_1', 0))\n",
    "rmse_gap = abs(group_metrics.get('rmse_group_0', 0) - group_metrics.get('rmse_group_1', 0))\n",
    "\n",
    "# 민감속성과 오차의 상관관계\n",
    "corr_error = np.corrcoef(abs_error, sensitive_attr)[0, 1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c08444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55695933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
