{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d510bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import dgl\n",
    "import torch\n",
    "import torch_sparse\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "# from dgl.nn.pytorch import GraphConv\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, f1_score, average_precision_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import wasserstein_distance, pearsonr, spearmanr\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.typing import Adj, OptTensor\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "\n",
    "# import warnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bb45ca",
   "metadata": {},
   "source": [
    "## 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444bdfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_Body(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, dropout):\n",
    "        super(GCN_Body, self).__init__()\n",
    "        self.gc1 = GCNConv(nfeat, nhid)\n",
    "        self.gc2 = GCNConv(nhid, nhid)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.gc1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.gc2(x, edge_index)\n",
    "        return x  \n",
    "    \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.body = GCN_Body(nfeat,nhid,dropout)\n",
    "        self.fc = nn.Linear(nhid,nclass)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.body(x, edge_index)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 나중에 수정 필요함\n",
    "class GAT_body(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 in_dim,\n",
    "                 num_hidden,\n",
    "                 heads,\n",
    "                 feat_drop,\n",
    "                 attn_drop,\n",
    "                 negative_slope,\n",
    "                 residual):\n",
    "        super(GAT_body, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        self.activation = F.elu\n",
    "        # input projection (no residual)\n",
    "        self.gat_layers.append(GATConv(\n",
    "            in_dim, num_hidden, heads[0],\n",
    "            feat_drop, attn_drop, negative_slope, False, self.activation))\n",
    "        \n",
    "        # hidden layers\n",
    "        for l in range(1, num_layers):\n",
    "            # due to multi-head, the in_dim = num_hidden * num_heads\n",
    "            self.gat_layers.append(GATConv(\n",
    "                num_hidden * heads[l-1], num_hidden, heads[l],\n",
    "                feat_drop, attn_drop, negative_slope, residual, self.activation))\n",
    "        \n",
    "        # output projection\n",
    "        self.gat_layers.append(GATConv(\n",
    "            num_hidden * heads[-2], num_hidden, heads[-1],\n",
    "            feat_drop, attn_drop, negative_slope, residual, None))\n",
    "    \n",
    "    def forward(self, g, inputs):\n",
    "        h = inputs\n",
    "        for l in range(self.num_layers):\n",
    "            h = self.gat_layers[l](g, h).flatten(1)\n",
    "        \n",
    "        # output projection\n",
    "        logits = self.gat_layers[-1](g, h).mean(1)\n",
    "\n",
    "        return logits\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 in_dim,\n",
    "                 num_hidden,\n",
    "                 num_classes,\n",
    "                 heads,\n",
    "                 feat_drop,\n",
    "                 attn_drop,\n",
    "                 negative_slope,\n",
    "                 residual):\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        self.body = GAT_body(num_layers, in_dim, num_hidden, heads, feat_drop, attn_drop, negative_slope, residual)\n",
    "        self.fc = nn.Linear(num_hidden,num_classes)\n",
    "    \n",
    "    def forward(self, g, inputs):\n",
    "        logits = self.body(g,inputs)\n",
    "        logits = self.fc(logits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "def get_model(model, nfeat,\n",
    "              num_hidden=64, dropout=0.5, num_heads=1, num_layers=1, num_out_heads=1, attn_drop=0.0, negative_slope=0.2, residual=False):\n",
    "    if model == \"GCN\":\n",
    "        model = GCN_Body(nfeat, num_hidden, dropout)\n",
    "    elif model == \"GAT\":\n",
    "        heads = ([num_heads] * num_layers) + [num_out_heads]\n",
    "        model = GAT_body(num_layers, nfeat, num_hidden, heads, dropout, attn_drop, negative_slope, residual)\n",
    "    else:\n",
    "        raise ValueError(\"Model not implemented\")\n",
    "    return model\n",
    "   \n",
    "def fair_metric(output, labels, sens, idx):\n",
    "    val_y = labels[idx].cpu().numpy()\n",
    "    idx_s0 = sens.cpu().numpy()[idx.cpu().numpy()]==0\n",
    "    idx_s1 = sens.cpu().numpy()[idx.cpu().numpy()]>0\n",
    "\n",
    "    idx_s0_y1 = np.bitwise_and(idx_s0,val_y>0)\n",
    "    idx_s1_y1 = np.bitwise_and(idx_s1,val_y>0)\n",
    "\n",
    "    pred_y = (output[idx].squeeze()>0.5).type_as(labels).cpu().numpy()\n",
    "\n",
    "    parity = abs(sum(pred_y[idx_s0])/sum(idx_s0)-sum(pred_y[idx_s1])/sum(idx_s1))\n",
    "    equality = abs(sum(pred_y[idx_s0_y1])/sum(idx_s0_y1)-sum(pred_y[idx_s1_y1])/sum(idx_s1_y1))\n",
    "\n",
    "    return parity, equality\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    output = output.squeeze()\n",
    "    preds = (output>0).type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    \n",
    "    return correct / len(labels)\n",
    "\n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Unsupported value encountered.')\n",
    "\n",
    "def feature_norm(features):\n",
    "    min_values = features.min(axis=0)[0]\n",
    "    max_values = features.max(axis=0)[0]\n",
    "\n",
    "    return 2*(features - min_values).div(max_values-min_values) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5908a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fair_metric_regression(output, labels, sens):\n",
    "    y_g0 = output[sens == 0]\n",
    "    y_g1 = output[sens == 1]\n",
    "    \n",
    "    # 그룹별 MSE 차이\n",
    "    mse_g0 = mean_squared_error(labels[sens == 0].cpu().numpy(), y_g0.cpu().numpy()) if len(y_g0) > 0 else 0.0\n",
    "    mse_g1 = mean_squared_error(labels[sens == 1].cpu().numpy(), y_g1.cpu().numpy()) if len(y_g1) > 0 else 0.0\n",
    "    mse_diff = abs(mse_g0 - mse_g1)\n",
    "    \n",
    "    # Wasserstein 거리: 1D 배열로 변환\n",
    "    w_dist = wasserstein_distance(\n",
    "        y_g0.cpu().numpy().flatten(), \n",
    "        y_g1.cpu().numpy().flatten()\n",
    "    ) if len(y_g0) > 0 and len(y_g1) > 0 else 0.0\n",
    "    \n",
    "    return mse_diff, w_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8db720a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sen(sens, idx_sens_train):\n",
    "#     sens_zeros = torch.zeros_like(sens)\n",
    "#     sens_1 = sens \n",
    "#     sens_0 = (1 - sens) \n",
    "    \n",
    "#     sens_1[idx_sens_train] = sens_1[idx_sens_train] / len(sens_1[idx_sens_train])\n",
    "#     sens_0[idx_sens_train] = sens_0[idx_sens_train] / len(sens_0[idx_sens_train])\n",
    "\n",
    "#     sens_zeros[idx_sens_train] = sens_1[idx_sens_train] - sens_0[idx_sens_train]\n",
    "#     sen_mat = torch.unsqueeze(sens_zeros, dim=0)\n",
    "#     return sen_mat\n",
    "\n",
    "def get_sen(sens, idx_sens_train):\n",
    "    num_classes = 2  # binary sensitive attribute assumed\n",
    "    one_hot = F.one_hot(sens.long(), num_classes=num_classes).float()  # (N, 2)\n",
    "\n",
    "    # training 노드에 대해서만 정규화\n",
    "    group_sums = one_hot[idx_sens_train].sum(dim=0, keepdim=True)  # (1, 2)\n",
    "    group_sums[group_sums == 0] = 1  # 0으로 나누는 것 방지\n",
    "\n",
    "    one_hot[idx_sens_train] = one_hot[idx_sens_train] / group_sums  # group-normalized\n",
    "\n",
    "    return one_hot  # shape: (N, 2)\n",
    "\n",
    "# def check_sen(edge_index, sen):\n",
    "#     nnz = edge_index.nnz()\n",
    "#     deg = torch.eye(edge_index.sizes()[0]).cuda()\n",
    "#     adj = edge_index.to_dense()\n",
    "#     lap = (sen.t() @ sen).to_dense()\n",
    "#     lap2 = deg - adj\n",
    "#     diff = torch.sum(torch.abs(lap2-lap)) / nnz\n",
    "#     assert diff < 0.000001, f'error: {diff} need to make sure L=B^TB'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c19493",
   "metadata": {},
   "source": [
    "## 데이터셋 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "63d798f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드 함수\n",
    "def feature_norm(features):\n",
    "    min_values = features.min(axis=0)[0]\n",
    "    max_values = features.max(axis=0)[0]\n",
    "    return 2*(features - min_values).div(max_values-min_values) - 1\n",
    "\n",
    "def load_dataset(dataset,sens_attr,predict_attr, path=\"./dataset/pokec/\", \n",
    "               label_number=1000, sens_number=500, test_idx=False, seed=1127):\n",
    "    \"\"\"Load data\"\"\"\n",
    "    print('Loading {} dataset from {}'.format(dataset,path))\n",
    "\n",
    "    idx_features_labels = pd.read_csv(os.path.join(path,\"{}.csv\".format(dataset)))\n",
    "    header = list(idx_features_labels.columns)\n",
    "    header.remove(\"user_id\")\n",
    "\n",
    "    header.remove(sens_attr)\n",
    "    header.remove(predict_attr)\n",
    "\n",
    "    features = sp.csr_matrix(idx_features_labels[header], dtype=np.float32)\n",
    "    labels = idx_features_labels[predict_attr].values\n",
    "    \n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[\"user_id\"], dtype=int)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "\n",
    "    # edges_unordered = np.genfromtxt(os.path.join(path,\"{}_relationship.txt\".format(dataset)), dtype=int)\n",
    "    edges_unordered = np.genfromtxt(os.path.join(path, f\"{dataset}_relationship.txt\"), dtype=np.int64)\n",
    "\n",
    "    # edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "    #                  dtype=int).reshape(edges_unordered.shape)\n",
    "    mapped_edges = []\n",
    "    dropped = 0\n",
    "    for u, v in edges_unordered:\n",
    "        u_mapped = idx_map.get(u)\n",
    "        v_mapped = idx_map.get(v)\n",
    "        if u_mapped is not None and v_mapped is not None:\n",
    "            mapped_edges.append((u_mapped, v_mapped))\n",
    "        else:\n",
    "            dropped += 1\n",
    "\n",
    "    print(f\"[INFO] 유효하지 않은 user_id로 인해 제거된 edge 수: {dropped}\")\n",
    "    edges = np.array(mapped_edges, dtype=int)\n",
    "\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "    \n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    # features = normalize(features)\n",
    "    adj = adj + sp.eye(adj.shape[0])\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(labels)\n",
    "    # adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    random.seed(seed)\n",
    "    label_idx = np.where(labels>=0)[0]\n",
    "    random.shuffle(label_idx)\n",
    "\n",
    "    idx_train = label_idx[:min(int(0.5 * len(label_idx)),label_number)]\n",
    "    idx_val = label_idx[int(0.5 * len(label_idx)):int(0.75 * len(label_idx))]\n",
    "    if test_idx:\n",
    "        idx_test = label_idx[label_number:]\n",
    "        idx_val = idx_test\n",
    "    else:\n",
    "        idx_test = label_idx[int(0.75 * len(label_idx)):]\n",
    "\n",
    "    sens = idx_features_labels[sens_attr].values\n",
    "\n",
    "    sens_idx = set(np.where(sens >= 0)[0])\n",
    "    idx_test = np.asarray(list(sens_idx & set(idx_test)))\n",
    "    sens = torch.FloatTensor(sens)\n",
    "    idx_sens_train = list(sens_idx - set(idx_val) - set(idx_test))\n",
    "    random.seed(seed)\n",
    "    random.shuffle(idx_sens_train)\n",
    "    idx_sens_train = torch.LongTensor(idx_sens_train[:sens_number])\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    \n",
    "    # random.shuffle(sens_idx)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test, sens,idx_sens_train\n",
    "\n",
    "def is_symmetric(m):\n",
    "    '''\n",
    "    Judge whether the matrix is symmetric or not.\n",
    "    :param m: Adjacency matrix(Array)\n",
    "    '''\n",
    "    res = np.int64(np.triu(m).T == np.tril(m))\n",
    "    # if np.where(res==0)[0] != []:\n",
    "    if np.where(res == 0)[0].size > 0:\n",
    "        raise ValueError(\"The matrix is not symmetric!\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "def symetric_normalize(m, half: bool):\n",
    "    '''\n",
    "    Symmetrically normalization for adjacency matrix\n",
    "    :param m: (Array) Adjacency matrix\n",
    "    :param half: (bool) whether m is triu or full\n",
    "    :return: (Array) An symmetric adjacency matrix\n",
    "    '''\n",
    "    if not half:\n",
    "        is_symmetric(m)\n",
    "    else:\n",
    "        m = m + m.T - np.diag(np.diagonal(m))\n",
    "\n",
    "    hat_m = m + np.eye(m.shape[0])\n",
    "    D = np.sum(hat_m, axis=1)\n",
    "    D = np.diag(D)\n",
    "\n",
    "    # D = np.power(D, -0.5)\n",
    "    with np.errstate(divide='ignore'):\n",
    "        D = np.power(D, -0.5)\n",
    "        D[np.isinf(D)] = 0\n",
    "\n",
    "    D[np.isinf(D)] = 0\n",
    "    sn_m = np.matmul(np.matmul(D, hat_m), D)\n",
    "    return sn_m\n",
    "\n",
    "def sp2sptensor(m):\n",
    "    # sparse_m = sp.coo_matrix(m).astype(np.float)\n",
    "    sparse_m = sp.coo_matrix(m).astype(np.float64)\n",
    "    # indices = torch.from_numpy(np.vstack((sparse_m.row, sparse_m.col)).astype(int))\n",
    "    indices = torch.from_numpy(np.vstack((sparse_m.row, sparse_m.col))).long()\n",
    "    values = torch.from_numpy(sparse_m.data)\n",
    "    shape = torch.Size(sparse_m.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "class German:\n",
    "    def __init__(self, path):\n",
    "        if not os.path.exists(path):\n",
    "            raise ValueError(\"Data path doesn't exist!\")\n",
    "        else:\n",
    "            self.data_path = path\n",
    "        self.raw_data = self._node_process()\n",
    "        self.A_tensor, self.A = self._edge_process()\n",
    "        self.senIdx, self.sen_vals, self.trainIdxtensor, self.valIdxTensor, self.testIdxTensor, self.features, self.labels = self._split_data()\n",
    "\n",
    "    def _node_process(self):\n",
    "        filenames = os.listdir(self.data_path)\n",
    "\n",
    "        for file in filenames:\n",
    "            if os.path.splitext(file)[1] != '.csv':\n",
    "                continue\n",
    "            else:\n",
    "                df_data = pd.read_csv(os.path.join(self.data_path, file))\n",
    "\n",
    "                # modify str feature\n",
    "                # df_data['GoodCustomer'].replace(-1, 0, inplace=True)\n",
    "                df_data['GoodCustomer'] = df_data['GoodCustomer'].replace(-1, 0).astype(int)\n",
    "\n",
    "                # df_data['Gender'].replace('Male', 1, inplace=True)\n",
    "                # df_data['Gender'].replace('Female', 0, inplace=True)\n",
    "                gender_map = {'Female': 0, 'Male': 1}\n",
    "                df_data['Gender'] = df_data['Gender'].map(gender_map).astype(int)\n",
    "\n",
    "                purposeList = list(df_data['PurposeOfLoan'])\n",
    "                random.shuffle(purposeList)\n",
    "                purposeDict = {}\n",
    "                index = 0\n",
    "                for pur in purposeList:\n",
    "                    if purposeDict.get(pur, None) is None:\n",
    "                        purposeDict[pur] = index\n",
    "                        index += 1\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                for key in purposeDict.keys():\n",
    "                    # df_data['PurposeOfLoan'].replace(key, purposeDict[key], inplace=True)\n",
    "                    df_data['PurposeOfLoan'] = df_data['PurposeOfLoan'].map(purposeDict).fillna(-1).astype(int)\n",
    "                    \n",
    "                return df_data\n",
    "\n",
    "    def _edge_process(self):\n",
    "        filenames = os.listdir(self.data_path)\n",
    "\n",
    "        for file in filenames:\n",
    "            if os.path.splitext(file)[1] != '.txt':\n",
    "                continue\n",
    "            else:\n",
    "                edges = np.loadtxt(os.path.join(self.data_path, file)).astype(int)\n",
    "\n",
    "                # Adjacency\n",
    "                num_dim = len(self.raw_data)\n",
    "                A = np.zeros((num_dim, num_dim))\n",
    "                for i in range(len(edges)):\n",
    "                    A[edges[i][0]][edges[i][1]] = 1\n",
    "                    A[edges[i][1]][edges[i][0]] = 1\n",
    "                sym_norm_A = symetric_normalize(A, half=False)\n",
    "                syn_norm_A_tensor = sp2sptensor(sym_norm_A)\n",
    "                return syn_norm_A_tensor, A\n",
    "\n",
    "    def _split_data(self):\n",
    "        pos_data = self.raw_data[self.raw_data['GoodCustomer']==1]\n",
    "        pos_index = list(pos_data.index)\n",
    "        neg_data = self.raw_data[self.raw_data['GoodCustomer']==0]\n",
    "        neg_index = list(neg_data.index)\n",
    "\n",
    "        # shuffle the index\n",
    "        random.seed(20)\n",
    "        random.shuffle(pos_index)\n",
    "        random.shuffle(neg_index)\n",
    "\n",
    "        # split the data\n",
    "        train_pos_idx = pos_index[:int(0.5*len(pos_index))]\n",
    "        train_neg_idx = neg_index[:int(0.5*len(neg_index))]\n",
    "        val_pos_idx = pos_index[int(0.5*len(pos_index)): int(0.75*len(pos_index))]\n",
    "        val_neg_idx = neg_index[int(0.5*len(neg_index)): int(0.75*len(neg_index))]\n",
    "        test_pos_idx = pos_index[int(0.75*len(pos_index)):]\n",
    "        test_neg_idx = neg_index[int(0.75*len(neg_index)):]\n",
    "\n",
    "        trainIdx = train_pos_idx + train_neg_idx\n",
    "        random.shuffle(trainIdx)\n",
    "        valIdx = val_pos_idx + val_neg_idx\n",
    "        random.shuffle(valIdx)\n",
    "        testIdx = test_pos_idx + test_neg_idx\n",
    "        random.shuffle(testIdx)\n",
    "\n",
    "        assert len(trainIdx)+len(valIdx)+len(testIdx) == len(self.raw_data), \"Missing data or leaking data!\"\n",
    "\n",
    "        feature_cols = list(self.raw_data.columns)\n",
    "        feature_cols.remove('GoodCustomer')\n",
    "        sen_idx = feature_cols.index('Gender')\n",
    "        sen_vals = self.raw_data['Gender'].values.astype(int)\n",
    "        feature_data = self.raw_data[feature_cols]\n",
    "        labels = self.raw_data['GoodCustomer']\n",
    "\n",
    "        # transform to tensor\n",
    "        trainIdxTensor = torch.LongTensor(trainIdx)\n",
    "        valIdxTensor = torch.LongTensor(valIdx)\n",
    "        testIdxTensor = torch.LongTensor(testIdx)\n",
    "        featuredata = torch.FloatTensor(np.array(feature_data))\n",
    "        labels = torch.LongTensor(np.array(labels))\n",
    "\n",
    "        return sen_idx, sen_vals, trainIdxTensor, valIdxTensor, testIdxTensor, featuredata, labels\n",
    "\n",
    "    def get_index(self):\n",
    "        return [self.trainIdxtensor, self.valIdxTensor, self.testIdxTensor]\n",
    "\n",
    "    def get_raw_data(self):\n",
    "        return [self.features, self.A_tensor, self.labels]\n",
    "\n",
    "    def generate_counterfactual_perturbation(self, data):\n",
    "        '''\n",
    "        Generate counterfactual data by flipping sensitive attribute.\n",
    "        :param data: Tensor\n",
    "        :return: counterfactual data\n",
    "        '''\n",
    "        feature_data = copy.deepcopy(data)\n",
    "        feature_data[:, self.senIdx] = 1 - feature_data[:, self.senIdx]\n",
    "        return feature_data\n",
    "\n",
    "    def generate_node_perturbation(self, prob: float, sen: bool = False):\n",
    "        '''\n",
    "        Perturbing node attributes, except for sensitive attributes.\n",
    "        :param prob: portion of perturbed data\n",
    "        :return: perturbed data\n",
    "        '''\n",
    "        feature_data = copy.deepcopy(self.features)\n",
    "        r = np.random.binomial(n=1, p=prob, size=feature_data.numpy().shape)\n",
    "        for i in range(len(feature_data)):\n",
    "            r[i][self.senIdx] = 0\n",
    "        noise = np.multiply(r, np.random.normal(0., 1., r.shape))\n",
    "        noise_tensor = torch.FloatTensor(noise)\n",
    "        x_hat = feature_data + noise_tensor\n",
    "\n",
    "        if sen:\n",
    "            x_hat = self.generate_counterfactual_perturbation(x_hat)\n",
    "        return x_hat\n",
    "\n",
    "    def generate_struc_perturbation(self, drop_prob: float, tensor: bool = True):\n",
    "        A = copy.deepcopy(self.A)\n",
    "        half_A = np.triu(A)\n",
    "        row, col = np.nonzero(half_A)\n",
    "        idx_perturb = np.random.binomial(n=1, p=1-drop_prob, size=row.shape)\n",
    "        broken_edges = np.where(idx_perturb==0)[0]\n",
    "        for idx in broken_edges:\n",
    "            half_A[row[idx]][col[idx]] = 0\n",
    "        new_A = symetric_normalize(half_A, half=True)\n",
    "        if tensor:\n",
    "            new_A = sp2sptensor(new_A)\n",
    "        return new_A\n",
    "\n",
    "def load_dataset_unified(dataset, sens_attr, predict_attr, path=\"./dataset/\", \n",
    "                         label_number=1000, sens_number=500, test_idx=False, seed=1127):\n",
    "    if dataset.lower() != 'german':\n",
    "        full_path = path\n",
    "        # full_path = os.path.join(path, \"pokec\")\n",
    "        return load_dataset(dataset, sens_attr, predict_attr, full_path, label_number, sens_number, test_idx, seed)\n",
    "\n",
    "    elif dataset.lower() == 'german':\n",
    "        full_path = path\n",
    "        # full_path = os.path.join(path, \"german\")\n",
    "        german_data = German(full_path)\n",
    "\n",
    "        adj = german_data.A_tensor\n",
    "        features = german_data.features\n",
    "        labels = german_data.labels\n",
    "        sens = torch.FloatTensor(german_data.sen_vals)\n",
    "        idx_train, idx_val, idx_test = german_data.get_index()\n",
    "\n",
    "        # 민감 속성 학습용 인덱스 (val/test 제외)\n",
    "        all_idx = set(range(len(sens)))\n",
    "        sens_idx = set(np.where(sens.numpy() >= 0)[0])\n",
    "        idx_sens_train = list(sens_idx - set(idx_val.numpy()) - set(idx_test.numpy()))\n",
    "        random.seed(seed)\n",
    "        random.shuffle(idx_sens_train)\n",
    "        idx_sens_train = torch.LongTensor(idx_sens_train[:sens_number])\n",
    "\n",
    "        return adj, features, labels, idx_train, idx_val, idx_test, sens, idx_sens_train\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset: {dataset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a90b6217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 설정\n",
    "# completion_percentage: 프로필을 얼마나 채웠는가 (%). 자발적 행동의 간접 지표.\n",
    "pokec_predict_attr = 'completion_percentage'\n",
    "pokec_sens_attr = 'region' # gender\n",
    "pokec_label_number, pokec_sens_number, pokec_test_idx, pokec_path = 500, 200, False, \"./dataset/pokec\"\n",
    "\n",
    "# PIE (Player Impact Estimate): 통합 퍼포먼스 지표, 퍼포먼스에 대한 차별 여부 확인 가능\n",
    "# MPG (Minutes Per Game): 출전 시간 → 팀의 코치 결정, 제도적 편향 가능성\n",
    "nba_predict_attr = 'PIE'\n",
    "nba_sens_attr = 'country'  # AGE, palyer_height, player_weight\n",
    "nba_label_number, nba_sens_number, nba_test_idx, nba_path = 100, 50, True, \"./dataset/NBA\"\n",
    "\n",
    "# German\n",
    "german_predict_attr = 'LoanAmount' # LoanAmount(대출 금액), LoanRateAsPercentOfIncome(소득대비상환비율), YearsAtCurrentHome(거주연수)\n",
    "german_sens_attr = 'Gender' # Gender, ForeignWorker, Single(독신여부), HasTelephone(전화기보유여부), OwnsHouse(주택소유여부), Unemployed(실직상태여부) Age(고령자로 나눠서 가능) 등등등..\n",
    "german_path = './dataset/NIFTY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e3f88",
   "metadata": {},
   "source": [
    "## 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "fe31d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 선택\n",
    "# dataset = 'region_job' # Pokec_z \n",
    "# dataset = 'region_job_2' # Pokec_n\n",
    "# dataset = 'nba' # NBA\n",
    "dataset = 'german' # German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "1fdcf880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sens 0 count: 310\n",
      "sens 1 count: 690\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 로드\n",
    "seed=42\n",
    "tau=0.5\n",
    "\n",
    "if dataset == 'german':\n",
    "    df, col, group = pd.read_csv(f'{german_path}/{dataset}.csv'), german_predict_attr, german_sens_attr\n",
    "elif dataset == 'nba':\n",
    "    df, col, group = pd.read_csv(f'{nba_path}/{dataset}.csv'), nba_predict_attr, nba_sens_attr\n",
    "else:\n",
    "    df, col, group = pd.read_csv(f'{pokec_path}/{dataset}.csv'), pokec_predict_attr, pokec_sens_attr\n",
    "\n",
    "if dataset == 'nba':\n",
    "    dn = 'NBA'\n",
    "elif dataset == 'german':\n",
    "    dn = 'German'\n",
    "else:\n",
    "    dn = 'Pokec_z' if dataset == 'region_job' else 'Pokec_n'\n",
    "\n",
    "if dataset == 'nba':\n",
    "    predict_attr, sens_attr, label_number, sens_number, test_idx, path = nba_predict_attr, nba_sens_attr, nba_label_number, nba_sens_number, nba_test_idx, nba_path\n",
    "elif dataset == 'german':\n",
    "    predict_attr, sens_attr, path = german_predict_attr, german_sens_attr, german_path\n",
    "else:\n",
    "    predict_attr, sens_attr, label_number, sens_number, test_idx, path = pokec_predict_attr, pokec_sens_attr, pokec_label_number, pokec_sens_number, pokec_test_idx, pokec_path\n",
    "\n",
    "adj, features, labels, idx_train, idx_val, idx_test, sens, idx_sens_train = load_dataset_unified(dataset, sens_attr, predict_attr, path, label_number, sens_number, test_idx, seed=seed)\n",
    "\n",
    "print(\"sens 0 count:\", torch.sum(sens == 0).item())\n",
    "print(\"sens 1 count:\", torch.sum(sens == 1).item())\n",
    "\n",
    "if dataset == 'nba':\n",
    "    features = feature_norm(features)\n",
    "\n",
    "if isinstance(adj, torch.Tensor):  # torch.sparse_coo_tensor\n",
    "    src, dst = adj._indices()\n",
    "    # g = dgl.graph((src, dst))\n",
    "    edge_index = adj._indices()\n",
    "else:  # scipy sparse matrix\n",
    "    # g = dgl.from_scipy(adj)\n",
    "    adj_coo = adj.tocoo()\n",
    "    edge_index = torch.tensor([adj_coo.row, adj_coo.col], dtype=torch.long)\n",
    "\n",
    "data = Data(x=features, edge_index=edge_index, y=labels.float(), sensitive_attr=sens, quantile_tau=torch.full((features.shape[0],), tau))\n",
    "# data.idx_train = idx_train\n",
    "# data.idx_val = idx_val\n",
    "# data.idx_test = idx_test\n",
    "# data.idx_sens_train = idx_sens_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "48c252cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Dataset Summary ==========\n",
      "Total nodes: 1000\n",
      "Total edges: 44484\n",
      "\n",
      "--- Sensitive Attribute Distribution ---\n",
      "Group 0: 310 (31.00%)\n",
      "Group 1: 690 (69.00%)\n",
      "\n",
      "--- Label Distribution ---\n",
      "Overall: mean=0.7000, std=0.4583\n",
      "Group 0: mean=0.6484, std=0.4775\n",
      "Group 1: mean=0.7232, std=0.4474\n",
      "\n",
      "--- Correlation (Sensitive vs Label) ---\n",
      "Pearson:  0.0755\n",
      "Spearman: 0.0755\n",
      "\n",
      "--- Graph Homophily ---\n",
      "Sensitive attribute homophily: 0.8092\n",
      "Label homophily: 0.5963\n",
      "\n",
      "--- Node Degree (per group) ---\n",
      "Group 0: mean=41.52, std=24.36\n",
      "Group 1: mean=45.81, std=27.32\n",
      "\n",
      "--- Neighborhood Composition ---\n",
      "Average same-group neighbor ratio: 0.8149\n"
     ]
    }
   ],
   "source": [
    "compute_graph_fairness_stats(data, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4883afb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28743eb4",
   "metadata": {},
   "source": [
    "## FairGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8038c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FairGNN\n",
    "# GNN 백본: 기본 GNN을 사용하여 노드 임베딩을 생성\n",
    "# 민감 속성 추정기: 노드 임베딩에서 민감한 속성을 예측하는 모듈. 이는 민감한 속성 정보가 제한적인 경우에도 공정성을 보장하기 위해 사용\n",
    "# 적대적 학습: 민감 속성 추정기를 속이는 방향으로 GNN을 학습시켜, 임베딩에서 민감한 속성 정보를 제거\n",
    "# 손실함수: 노드 분류 손실 + 적대적 손실 + 민감 속성 추정기의 예측 분산 줄이는 정규화 항\n",
    "\n",
    "# 장점\n",
    "# 민감 속성 정보가 부족한 상황에서도 효과적\n",
    "# 적대적 학습으로 임베딩 수준에서 공정성을 보장\n",
    "\n",
    "# 한계\n",
    "# 적대적 학습은 훈련이 불안정할 수 있음\n",
    "# 주로 그룹 공정성에 초점 맞춰, 개별 공정성은 제한적\n",
    "# 그래프 구조 자체의 편향을 직접적으로 해결하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7c6e50c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairGNN(nn.Module):\n",
    "    def __init__(self, nfeat,\n",
    "                 hidden_dim=64, model='GCN', dropout=0.5, hidden=128, lr=0.001, weight_decay=1e-5, alpha=4, beta=0.01):\n",
    "        super(FairGNN,self).__init__()\n",
    "\n",
    "        nfeat = nfeat\n",
    "        nhid = hidden_dim\n",
    "        dropout = dropout\n",
    "        \n",
    "        # 추가\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "        self.estimator = GCN(nfeat, hidden, 1, dropout) # 민감 속성 추정기\n",
    "        self.GNN = get_model(model, nfeat) \n",
    "        self.classifier = nn.Linear(nhid, 1) # 회귀 출력\n",
    "        self.adv = nn.Linear(nhid, 1)  # 적대적 민감 속성 예측기\n",
    "\n",
    "        G_params = list(self.GNN.parameters()) + list(self.classifier.parameters()) + list(self.estimator.parameters())\n",
    "        self.optimizer_G = torch.optim.Adam(G_params, lr=lr, weight_decay=weight_decay)\n",
    "        self.optimizer_A = torch.optim.Adam(self.adv.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        # self.args = args\n",
    "        # self.criterion = nn.BCEWithLogitsLoss()\n",
    "        # 회귀 손실 함수\n",
    "        self.criterion = nn.MSELoss() \n",
    "        self.G_loss = 0\n",
    "        self.A_loss = 0\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        s = self.estimator(x, edge_index) # 민감 속성 추정\n",
    "        z = self.GNN(x, edge_index) # 노드 임베딩\n",
    "        y = self.classifier(z) # 회귀 예측\n",
    "        return y,s\n",
    "    \n",
    "    def optimize(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        labels = data.y\n",
    "        idx_train = data.idx_train\n",
    "        sens = data.sensitive_attr\n",
    "        idx_sens_train = data.idx_sens_train\n",
    "           \n",
    "        self.train()\n",
    "\n",
    "        ### update E, G\n",
    "        self.adv.requires_grad_(False)\n",
    "        self.optimizer_G.zero_grad()\n",
    "\n",
    "        s = self.estimator(x, edge_index)\n",
    "        h = self.GNN(x, edge_index)\n",
    "        y = self.classifier(h)\n",
    "        s_g = self.adv(h)\n",
    "\n",
    "        # s_score = torch.sigmoid(s.detach())\n",
    "        # s_score = (s_score > 0.5).float()\n",
    "        s_score = s.detach() # 회귀용\n",
    "        s_score[idx_sens_train]=sens[idx_sens_train].unsqueeze(1).float()\n",
    "        # y_score = torch.sigmoid(y)\n",
    "        y_score = y # 연속값 사용\n",
    "        \n",
    "        self.cov =  torch.abs(torch.mean((s_score - torch.mean(s_score)) * \n",
    "                                         (y_score - torch.mean(y_score))))\n",
    "\n",
    "        self.cls_loss = self.criterion(y[idx_train], labels[idx_train].unsqueeze(1).float())\n",
    "        self.adv_loss = self.criterion(s_g, s_score)                \n",
    "        \n",
    "        self.G_loss = self.cls_loss  + self.alpha * self.cov - self.beta * self.adv_loss\n",
    "        self.G_loss.backward()\n",
    "        self.optimizer_G.step()\n",
    "\n",
    "        ## update Adv\n",
    "        self.adv.requires_grad_(True)\n",
    "        self.optimizer_A.zero_grad()\n",
    "        s_g = self.adv(h.detach())\n",
    "        \n",
    "        self.A_loss = self.criterion(s_g, s_score)\n",
    "        self.A_loss.backward()\n",
    "        self.optimizer_A.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "6d2cefd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Sen Training settings\n",
    "# no_cuda=False\n",
    "fastmode=False\n",
    "# seed=42\n",
    "lr=0.001\n",
    "weight_decay=1e-5\n",
    "hidden=128\n",
    "dropout=0.5\n",
    "# sens_number=200\n",
    "# dataset='pokec_z'\n",
    "\n",
    "# cuda = not no_cuda and torch.cuda.is_available()\n",
    "cuda =False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "sens[sens>0]=1\n",
    "if sens_attr:\n",
    "    sens[sens>0]=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "98f27ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0] Test set results: mse_test=17371.0527 mse_val=15279.7129 mse_diff_val=4118.8760, w_dist_val=15.7599\n",
      "Epoch [10] Test set results: mse_test=1221.9738 mse_val=1078.3721 mse_diff_val=329.2822, w_dist_val=4.1202\n",
      "The best MSE of estimator: 1221.9738\n",
      "Best test results: MSE=1221.9738,  MSE_diff=378.0270, W_dist=4.1462\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.7732s\n"
     ]
    }
   ],
   "source": [
    "# Sen Train model\n",
    "epochs = 10\n",
    "\n",
    "# 데이터셋 \n",
    "data = Data(x=features, edge_index=edge_index, y=labels.float(), sensitive_attr=sens, quantile_tau=torch.full((features.shape[0],), tau))\n",
    "data.idx_train = idx_train\n",
    "data.idx_val = idx_val\n",
    "data.idx_test = idx_test\n",
    "data.idx_sens_train = idx_sens_train\n",
    "\n",
    "# Model and optimizer\n",
    "model = GCN(nfeat=data.x.shape[1], nhid=hidden, nclass=1, dropout=dropout)\n",
    "optimizer = Adam(model.parameters(),lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "best_mse = float('inf')\n",
    "best_result = {}\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data.x, data.edge_index)\n",
    "    \n",
    "    # loss_train = F.binary_cross_entropy_with_logits(output[idx_sens_train], sens[idx_sens_train].unsqueeze(1).float())\n",
    "    # acc_train = accuracy(output[idx_sens_train], sens[idx_sens_train])\n",
    "    loss_train = F.mse_loss(output[data.idx_train], data.sensitive_attr[data.idx_train].unsqueeze(1).float())\n",
    "    mse_train = mean_squared_error(\n",
    "        data.sensitive_attr[data.idx_train].cpu().numpy(),\n",
    "        output[data.idx_train].detach().cpu().numpy()\n",
    "    )\n",
    "    \n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not fastmode:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(data.x, data.edge_index)\n",
    "        \n",
    "            mse_val = mean_squared_error(\n",
    "                data.sensitive_attr[data.idx_val].cpu().numpy(),\n",
    "                output[data.idx_val].cpu().numpy()\n",
    "            )\n",
    "\n",
    "            mse_diff_val, w_dist_val = fair_metric_regression(\n",
    "                output[data.idx_val],\n",
    "                data.sensitive_attr[data.idx_val],\n",
    "                data.sensitive_attr[data.idx_val]\n",
    "            )\n",
    "\n",
    "            mse_test = mean_squared_error(\n",
    "                data.sensitive_attr[data.idx_test].cpu().numpy(),\n",
    "                output[data.idx_test].cpu().numpy()\n",
    "            )\n",
    "\n",
    "            mse_diff_test, w_dist_test = fair_metric_regression(\n",
    "                output[data.idx_test],\n",
    "                data.sensitive_attr[data.idx_test],\n",
    "                data.sensitive_attr[data.idx_test]\n",
    "            )\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch [{epoch}] Test set results:\",\n",
    "              f\"mse_test={mse_test:.4f}\",\n",
    "              f\"mse_val={mse_val:.4f}\",\n",
    "              f\"mse_diff_val={mse_diff_val:.4f}, w_dist_val={w_dist_val:.4f}\")\n",
    "\n",
    "        if mse_val < best_mse:\n",
    "            best_mse = mse_val\n",
    "            best_result = {\n",
    "                'mse': mse_test,\n",
    "                'mse_diff': mse_diff_test,\n",
    "                'w_dist': w_dist_test\n",
    "            }\n",
    "            torch.save(model.state_dict(), f\"./checkpoint/GCN_sens_{dataset}_ns_{sens_number}\")\n",
    "\n",
    "print(f\"The best MSE of estimator: {best_result['mse']:.4f}\")\n",
    "print(f\"Best test results: MSE={best_result['mse']:.4f}, \"\n",
    "      f\" MSE_diff={best_result['mse_diff']:.4f}, \"\n",
    "      f\"W_dist={best_result['w_dist']:.4f}\")\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(f\"Total time elapsed: {time.time() - t_total:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "9fd95d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "\n",
    "# fastmode = False\n",
    "# dataset = 'pokec_z'\n",
    "# epochs = 2000\n",
    "# lr = 0.001\n",
    "# weight_decay = 1e-5\n",
    "# hidden = 128\n",
    "# dropout = 0.5\n",
    "# alpha = 4\n",
    "# beta = 0.01\n",
    "# model = \"GAT\"\n",
    "# residual = False\n",
    "# attn-drop = 0.0\n",
    "# negative-slope = 0.2\n",
    "# num-hidden = 64\n",
    "# num-heads = 1\n",
    "# num-out-heads = 1\n",
    "# num-layers = 1\n",
    "# seed = 42\n",
    "# acc = 0.688\n",
    "# roc = 0.745\n",
    "\n",
    "cuda = False\n",
    "# if cuda:\n",
    "#     model.cuda()\n",
    "#     features = features.cuda()\n",
    "#     labels = labels.cuda()\n",
    "#     idx_train = idx_train.cuda()\n",
    "#     idx_val = idx_val.cuda()\n",
    "#     idx_test = idx_test.cuda()\n",
    "#     sens = sens.cuda()\n",
    "#     idx_sens_train = idx_sens_train.cuda()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "labels[labels>1]=1\n",
    "if sens_attr:\n",
    "    sens[sens>0]=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "d8bd2787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FairGNN] Run 1, Epoch 000 | Val MSE: 99948.1846, MAE: 297.4223, R2: -475942.7362, MSE_diff: 25893.8047, W_dist: 40.9356 | Test MSE: 114023.0452, MAE: 317.7929, R2: -542965.8819, MSE_diff: 30153.0156, W_dist: 40.8191\n",
      "[FairGNN] Run 1, Epoch 010 | Val MSE: 25889.3274, MAE: 151.6999, R2: -123281.5113, MSE_diff: 6575.9961, W_dist: 20.4217 | Test MSE: 29408.1577, MAE: 161.7857, R2: -140037.8462, MSE_diff: 7712.0957, W_dist: 20.5262\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "epochs = 20\n",
    "running_times=1\n",
    "RUNNING_TIME = running_times\n",
    "\n",
    "# 데이터셋 \n",
    "data = Data(x=features, edge_index=edge_index, y=labels.float(), sensitive_attr=sens, quantile_tau=torch.full((features.shape[0],), tau))\n",
    "data.idx_train = idx_train\n",
    "data.idx_val = idx_val\n",
    "data.idx_test = idx_test\n",
    "data.idx_sens_train = idx_sens_train\n",
    "\n",
    "performances = []\n",
    "fairnesss = []\n",
    "\n",
    "for run in range(RUNNING_TIME):\n",
    "    model = FairGNN(nfeat=data.x.shape[1])\n",
    "    model = model.to(device)\n",
    "\n",
    "    try:\n",
    "        model.estimator.load_state_dict(torch.load(\n",
    "            f\"./checkpoint/GCN_sens_{dataset}_ns_{sens_number}\",\n",
    "            map_location=torch.device(device)\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        print(f\"Checkpoint load failed: {e}\")\n",
    "\n",
    "    best_mse = float('inf')\n",
    "    best_result = {}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        model.optimize(data)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output, s = model(data)\n",
    "\n",
    "            y_true = data.y\n",
    "            idx_train = data.idx_train\n",
    "            idx_val = data.idx_val\n",
    "            idx_test = data.idx_test\n",
    "\n",
    "            # Validation\n",
    "            mse_val = mean_squared_error(y_true[idx_val].cpu(), output[idx_val].cpu())\n",
    "            mae_val = mean_absolute_error(y_true[idx_val].cpu(), output[idx_val].cpu())\n",
    "            r2_val = r2_score(y_true[idx_val].cpu(), output[idx_val].cpu())\n",
    "            mse_diff_val, w_dist_val = fair_metric_regression(output[idx_val], y_true[idx_val], data.sensitive_attr[idx_val])\n",
    "\n",
    "            # Test\n",
    "            mse_test = mean_squared_error(y_true[idx_test].cpu(), output[idx_test].cpu())\n",
    "            mae_test = mean_absolute_error(y_true[idx_test].cpu(), output[idx_test].cpu())\n",
    "            r2_test = r2_score(y_true[idx_test].cpu(), output[idx_test].cpu())\n",
    "            mse_diff_test, w_dist_test = fair_metric_regression(output[idx_test], y_true[idx_test], data.sensitive_attr[idx_test])\n",
    "\n",
    "        if mse_val < best_mse:\n",
    "            best_mse = mse_val\n",
    "            best_result = {\n",
    "                'mse': mse_test,\n",
    "                'mae': mae_test,\n",
    "                'r2': r2_test,\n",
    "                'mse_diff': mse_diff_test,\n",
    "                'w_dist': w_dist_test\n",
    "            }\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"[FairGNN] Run {run+1}, Epoch {epoch:03d} | \"\n",
    "                  f\"Val MSE: {mse_val:.4f}, MAE: {mae_val:.4f}, R2: {r2_val:.4f}, \"\n",
    "                  f\"MSE_diff: {mse_diff_val:.4f}, W_dist: {w_dist_val:.4f} | \"\n",
    "                  f\"Test MSE: {mse_test:.4f}, MAE: {mae_test:.4f}, R2: {r2_test:.4f}, \"\n",
    "                  f\"MSE_diff: {mse_diff_test:.4f}, W_dist: {w_dist_test:.4f}\")\n",
    "\n",
    "    performances.append([best_result['mse'], best_result['mae'], best_result['r2']])\n",
    "    fairnesss.append([best_result['mse_diff'], best_result['w_dist']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "a88b1eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistical Results:\n",
      "Performance Mean: MSE=54.6223, MAE=7.1493, R2=-259.1064\n",
      "Performance Std:  MSE=0.0, MAE=0.0, R2=0.0\n",
      "Fairness Mean:    MSE_diff=12.3621, W_dist=0.7216\n",
      "Fairness Std:     MSE_diff=0.0, W_dist=0.0\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "performance_mean = np.around(np.mean(performances, axis=0), 4)\n",
    "performance_std = np.around(np.std(performances, axis=0), 4)\n",
    "fairness_mean = np.around(np.mean(fairnesss, axis=0), 4)\n",
    "fairness_std = np.around(np.std(fairnesss, axis=0), 4)\n",
    "\n",
    "print(\"\\nStatistical Results:\")\n",
    "print(f\"Performance Mean: MSE={performance_mean[0]}, MAE={performance_mean[1]}, R2={performance_mean[2]}\")\n",
    "print(f\"Performance Std:  MSE={performance_std[0]}, MAE={performance_std[1]}, R2={performance_std[2]}\")\n",
    "print(f\"Fairness Mean:    MSE_diff={fairness_mean[0]}, W_dist={fairness_mean[1]}\")\n",
    "print(f\"Fairness Std:     MSE_diff={fairness_std[0]}, W_dist={fairness_std[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e691df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee7ea7ae",
   "metadata": {},
   "source": [
    "## FMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b1906e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FMP\n",
    "# GNN의 이웃 집계가 편향을 증폭시킬 수 있다는 점을 해결하려는 접근법\n",
    "# 목표: 메세지 패싱 과정에서 민감한 속성의 영향을 최소화하여 공정성을 보장하고, 동시에 분류 정확도를 유지\n",
    "\n",
    "# 이웃 집계: 표준 GNN처럼 이웃 노드의 정보를 집계하여 노드 임베딩을 생성\n",
    "# 편향 완화 단계: 민감한 속성에 따른 노드 표현의 차이를 줄이기 위해, 그룹별 노드 표현 중심을 명시적으로 가깝게 만듦\n",
    "# 통합 최적화 프레임워크: 분류 손실 + 그룹별 표현 중심의 공정성 손실\n",
    "\n",
    "# GNN 아키텍처 자체를 수정하여 공정성을 보장\n",
    "# 데이터 전처리 없이 메세지 패싱 과정에서 편향을 완화\n",
    "\n",
    "# 장점\n",
    "# 메세지 패싱 과정에서 직접 편향을 완화하므로 그래프 구조의 영향을 고려\n",
    "# 데이터 전처리 없이 모델 아키텍처로 공정성을 보장\n",
    "\n",
    "# 한계\n",
    "# 주로 그룹 공정성에 초점, 개별 공정성 미흡\n",
    "# 고차 이웃의 복잡한 상호작용을 완전히 해결하지 못할 수 있음\n",
    "# 계산 비용이 증가할 가능성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4f969681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMP(torch.nn.Module):\n",
    "    _cached_sen = Optional[SparseTensor]\n",
    "    def __init__(self, in_feats, out_feats, K, lambda1, lambda2, dropout=0.0, cached=False, L2=True):\n",
    "        super(FMP, self).__init__()\n",
    "        self.K = K\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.L2 = L2\n",
    "        self.dropout = dropout\n",
    "        self.cached = cached\n",
    "        self._cached_sen = None  ## sensitive matrix\n",
    "\n",
    "        # self.propa = GraphConv(in_feats, in_feats, weight=False, bias=False, activation=None)\n",
    "        self.propa = GCNConv(out_feats, out_feats, add_self_loops=False, normalize=True)  # 그래프 컨볼루션 레이어: self-loop 미포함, 메세지 전파시 degree로 normalization\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self._cached_sen = None\n",
    "\n",
    "    def forward(self, x: Tensor, \n",
    "                edge_index, \n",
    "                idx_sens_train,\n",
    "                edge_weight: OptTensor = None, \n",
    "                sens=None) -> Tensor:\n",
    "\n",
    "        if self.K <= 0: \n",
    "            return x\n",
    "\n",
    "        cache = self._cached_sen\n",
    "        if cache is None:\n",
    "            sen_mat = get_sen(sens, idx_sens_train)               ## compute sensitive matrix\n",
    "            if self.cached:\n",
    "                self._cached_sen = sen_mat\n",
    "                self.init_z = torch.zeros((sen_mat.size()[0], x.size()[-1])).cuda()\n",
    "        else:\n",
    "            sen_mat = self._cached_sen # N,\n",
    "\n",
    "        hh = x\n",
    "        x = self.emp_forward(x, edge_index, hh, K=self.K, sen=sen_mat)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def emp_forward(self, x, edge_index, hh, K, sen):\n",
    "        lambda1 = self.lambda1\n",
    "        lambda2 = self.lambda2\n",
    "\n",
    "        gamma = 1/(1+lambda2)\n",
    "        beta = 1/(2*gamma)\n",
    "\n",
    "        for _ in range(K):\n",
    "            if lambda2 > 0:\n",
    "                y = gamma * hh + (1-gamma) * self.propa(x, edge_index)\n",
    "            else:\n",
    "                y = gamma * hh + (1-gamma) * x\n",
    "\n",
    "            if lambda1 > 0:\n",
    "                # z = sen @ F.softmax(y, dim=1) / (gamma * sen @ sen.t())\n",
    "                y_soft = F.softmax(y, dim=1)  # (N, F)\n",
    "                # Group-wise 평균 표현 벡터\n",
    "                z = sen.T @ y_soft / gamma  # (C, F)\n",
    "                \n",
    "                # x_bar0 = sen.t() @ z\n",
    "                x_bar0 = sen @ z\n",
    "                x_bar1 = F.softmax(x_bar0, dim=1) ## node * feature\n",
    "\n",
    "                correct = x_bar0 * x_bar1 \n",
    "                coeff = torch.sum(correct, dim=1, keepdim=True)\n",
    "                correct = correct - coeff * x_bar1\n",
    "\n",
    "                x_bar = y - gamma * correct\n",
    "                # z_bar  = z + beta * (sen @ F.softmax(x_bar, dim=1))\n",
    "                z_bar = z + beta * (sen.T @ F.softmax(x_bar, dim=1)) \n",
    "                \n",
    "                if self.L2:\n",
    "                    z  = self.L2_projection(z_bar, lambda_=lambda1, beta=beta)\n",
    "                else:\n",
    "                    z  = self.L1_projection(z_bar, lambda_=lambda1)\n",
    "                \n",
    "                # x_bar0 = sen.t() @ z\n",
    "                x_bar0 = sen @ z \n",
    "                x_bar1 = F.softmax(x_bar0, dim=1) ## node * feature\n",
    "                \n",
    "                correct = x_bar0 * x_bar1 \n",
    "                coeff = torch.sum(correct, 1, keepdim=True)\n",
    "                correct = correct - coeff * x_bar1\n",
    "\n",
    "                x = y - gamma * correct\n",
    "            else:\n",
    "                x = y # z=0\n",
    "\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def L1_projection(self, x: Tensor, lambda_):\n",
    "        # component-wise projection onto the l∞ ball of radius λ1.\n",
    "        return torch.clamp(x, min=-lambda_, max=lambda_)\n",
    "    \n",
    "    def L2_projection(self, x: Tensor, lambda_, beta):\n",
    "        # projection on the l2 ball of radius λ1.\n",
    "        coeff = (2*lambda_) / (2*lambda_ + beta)\n",
    "        return coeff * x\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_weight: Tensor) -> Tensor:\n",
    "        return edge_weight.view(-1, 1) * x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(K={}, lambda1={}, lambda2={}, L2={})'.format(\n",
    "            self.__class__.__name__, self.K, self.lambda1, self.lambda2, self.L2)\n",
    "        \n",
    "class FMPGNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, size, num_classes, num_layer, prop):\n",
    "        super(FMPGNN, self).__init__()\n",
    "        \n",
    "        self.hidden = nn.ModuleList()\n",
    "        for _ in range(num_layer-2):\n",
    "            self.hidden.append(nn.Linear(size, size))\n",
    "\n",
    "        self.first = nn.Linear(input_size, size)\n",
    "        self.last = nn.Linear(size, num_classes)\n",
    "        self.prop = prop\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "        self.prop.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        sens = data.sensitive_attr\n",
    "        idx_sens_train = data.idx_sens_train\n",
    "        \n",
    "        out = F.relu(self.first(x))\n",
    "        for layer in self.hidden:\n",
    "            out = F.relu(layer(out))\n",
    "        \n",
    "        x = self.last(out)\n",
    "        if sens is None:\n",
    "            raise ValueError(\"data.sensitive_attr is None\")\n",
    "        x = self.prop(x, edge_index, idx_sens_train, sens=sens)  # 인자 순서 수정\n",
    "        \n",
    "        return x\n",
    "\n",
    "def fmp_model(data,\n",
    "              num_layers=5, lambda1=3, lambda2=3, L2=True, num_hidden=64, num_gnn_layer=2): \n",
    "\n",
    "    Model = FMPGNN\n",
    "\n",
    "    prop =  FMP(in_feats=data.num_features,\n",
    "                # out_feats=data.num_features,\n",
    "                out_feats=1,\n",
    "                K=num_layers, \n",
    "                lambda1=lambda1,\n",
    "                lambda2=lambda2,\n",
    "                L2=L2,\n",
    "                cached=False)\n",
    "\n",
    "    model = Model(input_size=data.num_features, \n",
    "                  size=num_hidden, \n",
    "                #   num_classes=data.num_classes, \n",
    "                  num_classes=1,  # 회귀용\n",
    "                  num_layer=num_gnn_layer, \n",
    "                  prop=prop)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "b7eef560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "\n",
    "# gpu=0\n",
    "# seed=42\n",
    "# prefix='FGNN'\n",
    "lr=0.001\n",
    "weight_decay=1e-5\n",
    "dropout=0.5\n",
    "# dataset='pokec_n'\n",
    "# num-hidden=64\n",
    "# num-heads=4\n",
    "# num-out-heads=1\n",
    "# num-layers=5\n",
    "residual=False\n",
    "# in-drop=0.5\n",
    "# edge-drop=0.5\n",
    "# attn-drop=0.5\n",
    "# negative-slope=0.2\n",
    "bias=False\n",
    "# acc=0.5\n",
    "# roc=0.5\n",
    "lambda1=3\n",
    "lambda2=3\n",
    "num_gnn_layer=2\n",
    "L2=True\n",
    "alpha=0.1\n",
    "n_classes = 1  # 2->1\n",
    "\n",
    "# device = torch.device('cuda:{}'.format(gpu))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "labels[labels>1]=1\n",
    "if sens_attr:\n",
    "    sens[sens>0]=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "0b28ff30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FMPGNN] Run 1, Epoch 000 | Val MSE: 17.9255, MAE: 4.0726, R2: -84.3594, MSE_diff: 3.1483, W_dist: 0.3448 | Test MSE: 19.8317, MAE: 4.2656, R2: -93.4366, MSE_diff: 3.7160, W_dist: 0.3323\n",
      "[FMPGNN] Run 1, Epoch 010 | Val MSE: 39.4980, MAE: 6.0410, R2: -187.0859, MSE_diff: 7.1191, W_dist: 0.5260 | Test MSE: 43.6457, MAE: 6.3284, R2: -206.8368, MSE_diff: 8.2417, W_dist: 0.5192\n",
      "[FMPGNN] Run 1, Epoch 020 | Val MSE: 89.2510, MAE: 9.0619, R2: -424.0049, MSE_diff: 16.4954, W_dist: 0.8181 | Test MSE: 98.6997, MAE: 9.5016, R2: -468.9988, MSE_diff: 18.7555, W_dist: 0.8170\n",
      "[FMPGNN] Run 1, Epoch 030 | Val MSE: 1.6021, MAE: 1.1605, R2: -6.6289, MSE_diff: 0.2043, W_dist: 0.0661 | Test MSE: 1.7104, MAE: 1.2026, R2: -7.1448, MSE_diff: 0.2849, W_dist: 0.0615\n",
      "[FMPGNN] Run 1, Epoch 040 | Val MSE: 6.3791, MAE: 2.2706, R2: -29.3769, MSE_diff: 1.3815, W_dist: 0.3025 | Test MSE: 6.9678, MAE: 2.4009, R2: -32.1800, MSE_diff: 1.3366, W_dist: 0.2991\n",
      "[FMPGNN] Run 1, Epoch 050 | Val MSE: 4.7996, MAE: 1.9424, R2: -21.8550, MSE_diff: 1.0513, W_dist: 0.2672 | Test MSE: 5.2220, MAE: 2.0559, R2: -23.8666, MSE_diff: 0.9977, W_dist: 0.2670\n",
      "[FMPGNN] Run 1, Epoch 060 | Val MSE: 1.7766, MAE: 1.0926, R2: -7.4600, MSE_diff: 0.4030, W_dist: 0.1815 | Test MSE: 1.8847, MAE: 1.1597, R2: -7.9749, MSE_diff: 0.3464, W_dist: 0.1870\n",
      "[FMPGNN] Run 1, Epoch 070 | Val MSE: 0.7593, MAE: 0.6452, R2: -2.6156, MSE_diff: 0.1684, W_dist: 0.1341 | Test MSE: 0.7638, MAE: 0.6728, R2: -2.6372, MSE_diff: 0.1246, W_dist: 0.1446\n",
      "[FMPGNN] Run 1, Epoch 080 | Val MSE: 0.4827, MAE: 0.5023, R2: -1.2986, MSE_diff: 0.0970, W_dist: 0.1132 | Test MSE: 0.4601, MAE: 0.5057, R2: -1.1912, MSE_diff: 0.0630, W_dist: 0.1251\n",
      "[FMPGNN] Run 1, Epoch 090 | Val MSE: 0.3874, MAE: 0.4557, R2: -0.8447, MSE_diff: 0.0689, W_dist: 0.1024 | Test MSE: 0.3561, MAE: 0.4541, R2: -0.6956, MSE_diff: 0.0411, W_dist: 0.1146\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "epochs=100\n",
    "running_times=1\n",
    "RUNNING_TIME = running_times\n",
    "\n",
    "# 데이터셋 \n",
    "data = Data(x=features, edge_index=edge_index, y=labels.float(), sensitive_attr=sens, quantile_tau=torch.full((features.shape[0],), tau))\n",
    "data.idx_train = idx_train\n",
    "data.idx_val = idx_val\n",
    "data.idx_test = idx_test\n",
    "data.idx_sens_train = idx_sens_train\n",
    "\n",
    "performances = []\n",
    "fairnesss = []\n",
    "\n",
    "for run in range(RUNNING_TIME):\n",
    "    model = fmp_model(data).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    best_mse = float('inf')\n",
    "    best_result = {}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        output = model(data)\n",
    "        loss = criterion(output[data.idx_train], data.y[data.idx_train].unsqueeze(1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "\n",
    "            # Validation\n",
    "            mse_val = mean_squared_error(data.y[data.idx_val].cpu(), output[data.idx_val].cpu())\n",
    "            mae_val = mean_absolute_error(data.y[data.idx_val].cpu(), output[data.idx_val].cpu())\n",
    "            r2_val = r2_score(data.y[data.idx_val].cpu(), output[data.idx_val].cpu())\n",
    "            mse_diff_val, w_dist_val = fair_metric_regression(output[data.idx_val], data.y[data.idx_val], data.sensitive_attr[data.idx_val])\n",
    "\n",
    "            # Test\n",
    "            mse_test = mean_squared_error(data.y[data.idx_test].cpu(), output[data.idx_test].cpu())\n",
    "            mae_test = mean_absolute_error(data.y[data.idx_test].cpu(), output[data.idx_test].cpu())\n",
    "            r2_test = r2_score(data.y[data.idx_test].cpu(), output[data.idx_test].cpu())\n",
    "            mse_diff_test, w_dist_test = fair_metric_regression(output[data.idx_test], data.y[data.idx_test], data.sensitive_attr[data.idx_test])\n",
    "\n",
    "        if mse_val < best_mse:\n",
    "            best_mse = mse_val\n",
    "            best_result = {\n",
    "                'mse': mse_test,\n",
    "                'mae': mae_test,\n",
    "                'r2': r2_test,\n",
    "                'mse_diff': mse_diff_test,\n",
    "                'w_dist': w_dist_test\n",
    "            }\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"[FMPGNN] Run {run+1}, Epoch {epoch:03d} | \"\n",
    "                  f\"Val MSE: {mse_val:.4f}, MAE: {mae_val:.4f}, R2: {r2_val:.4f}, \"\n",
    "                  f\"MSE_diff: {mse_diff_val:.4f}, W_dist: {w_dist_val:.4f} | \"\n",
    "                  f\"Test MSE: {mse_test:.4f}, MAE: {mae_test:.4f}, R2: {r2_test:.4f}, \"\n",
    "                  f\"MSE_diff: {mse_diff_test:.4f}, W_dist: {w_dist_test:.4f}\")\n",
    "\n",
    "    performances.append([best_result['mse'], best_result['mae'], best_result['r2']])\n",
    "    fairnesss.append([best_result['mse_diff'], best_result['w_dist']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "774c1a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2748657999383524, 0.4369139624238014, -0.30888476161120204]]\n",
      "[[0.02228367328643799, 0.0959251651002902]]\n"
     ]
    }
   ],
   "source": [
    "print(performances)\n",
    "print(fairnesss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "63778839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistical Results:\n",
      "Performance Mean: MSE=0.2749, MAE=0.4369, R2=-0.3089\n",
      "Performance Std:  MSE=0.0, MAE=0.0, R2=0.0\n",
      "Fairness Mean:    MSE_diff=0.0223, W_dist=0.0959\n",
      "Fairness Std:     MSE_diff=0.0, W_dist=0.0\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "performance_mean = np.around(np.mean(performances, axis=0), 4)\n",
    "performance_std = np.around(np.std(performances, axis=0), 4)\n",
    "fairness_mean = np.around(np.mean(fairnesss, axis=0), 4)\n",
    "fairness_std = np.around(np.std(fairnesss, axis=0), 4)\n",
    "\n",
    "print(\"\\nStatistical Results:\")\n",
    "print(f\"Performance Mean: MSE={performance_mean[0]}, MAE={performance_mean[1]}, R2={performance_mean[2]}\")\n",
    "print(f\"Performance Std:  MSE={performance_std[0]}, MAE={performance_std[1]}, R2={performance_std[2]}\")\n",
    "print(f\"Fairness Mean:    MSE_diff={fairness_mean[0]}, W_dist={fairness_mean[1]}\")\n",
    "print(f\"Fairness Std:     MSE_diff={fairness_std[0]}, W_dist={fairness_std[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4276e70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6498d46",
   "metadata": {},
   "source": [
    "## Ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "efb514fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        row, col = edge_index\n",
    "        deg = torch.bincount(row, minlength=x.size(0)).float().clamp(min=1)\n",
    "        norm = 1.0 / deg[row].sqrt() / deg[col].sqrt()\n",
    "        out = torch.zeros_like(x)\n",
    "        out.index_add_(0, row, x[col] * norm.unsqueeze(1))\n",
    "        return self.linear(out)\n",
    "\n",
    "class FairRegGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=64):\n",
    "        super(FairRegGNN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        preds = self.fc(x).squeeze(-1)  # [N]\n",
    "        return preds\n",
    "\n",
    "def correlation_loss(y_pred, sensitive_attr):\n",
    "    y_centered = y_pred - y_pred.mean()\n",
    "    a_centered = sensitive_attr - sensitive_attr.mean()\n",
    "    corr = torch.sum(y_centered * a_centered) / (\n",
    "        torch.sqrt(torch.sum(y_centered ** 2)) * torch.sqrt(torch.sum(a_centered ** 2)) + 1e-8)\n",
    "    return torch.abs(corr)\n",
    "\n",
    "def wasserstein_loss(y_pred, sensitive_attr):\n",
    "    from scipy.stats import wasserstein_distance\n",
    "    group_0 = y_pred[sensitive_attr == 0].detach().cpu()\n",
    "    group_1 = y_pred[sensitive_attr == 1].detach().cpu()\n",
    "    return torch.tensor(wasserstein_distance(group_0, group_1))\n",
    "\n",
    "def mean_gap_loss(y_pred, sensitive_attr):\n",
    "    group_0 = y_pred[sensitive_attr == 0]\n",
    "    group_1 = y_pred[sensitive_attr == 1]\n",
    "    return torch.abs(group_0.mean() - group_1.mean())\n",
    "\n",
    "def mask_same_group_edges(edge_index, sensitive_attr):\n",
    "    src, dst = edge_index\n",
    "    mask = sensitive_attr[src] != sensitive_attr[dst]\n",
    "    return edge_index[:, mask]\n",
    "\n",
    "def auto_params_from_analysis(\n",
    "    n_nodes, mean_gap, std_gap, corr_sa_y, homophily_sensitive\n",
    "):\n",
    "    params = {}\n",
    "\n",
    "    # hidden_dim, lr 조정\n",
    "    if n_nodes < 500:\n",
    "        params['hidden_dim'] = 32\n",
    "        params['lr'] = 0.01\n",
    "    elif n_nodes < 5000:\n",
    "        params['hidden_dim'] = 64\n",
    "        params['lr'] = 0.005\n",
    "    else:\n",
    "        params['hidden_dim'] = 64\n",
    "        params['lr'] = 0.003\n",
    "\n",
    "    # λ₁: 평균 차이 클수록 penalty 높임\n",
    "    params['lambda1'] = min(10.0, 10 * mean_gap) if mean_gap > 0.05 else 1.0\n",
    "\n",
    "    # λ₂: 상관 높을수록 penalty 높임\n",
    "    params['lambda2'] = 10 * corr_sa_y if corr_sa_y > 0.05 else 1.0\n",
    "\n",
    "    # λ₃: 분포 차이 클수록 penalty 높임\n",
    "    params['lambda3'] = min(10.0, 5 * std_gap) if std_gap > 0.05 else 0.5\n",
    "\n",
    "    # edge masking 여부\n",
    "    params['use_edge_masking'] = homophily_sensitive > 0.8\n",
    "\n",
    "    return params\n",
    "\n",
    "def compute_homophily(attr_tensor, edge_index):\n",
    "    attr = attr_tensor.cpu().numpy()\n",
    "    src, dst = edge_index[0].cpu().numpy(), edge_index[1].cpu().numpy()\n",
    "    return np.mean(attr[src] == attr[dst])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2089a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "epochs=100\n",
    "running_times=1\n",
    "RUNNING_TIME = running_times\n",
    "# hidden_dim=64\n",
    "# lambda1=1.0\n",
    "# lambda2=1.0\n",
    "# lambda3=1.0\n",
    "\n",
    "# 데이터셋 \n",
    "data = Data(x=features, edge_index=edge_index, y=labels.float(), sensitive_attr=sens, quantile_tau=torch.full((features.shape[0],), tau))\n",
    "data.idx_train = idx_train\n",
    "data.idx_val = idx_val\n",
    "data.idx_test = idx_test\n",
    "data.idx_sens_train = idx_sens_train\n",
    "\n",
    "# 데이터 분석 → 파라미터 자동 설정\n",
    "params = auto_params_from_analysis(\n",
    "    n_nodes = data.x.shape[0],\n",
    "    mean_gap = torch.abs(data.y[data.sensitive_attr == 0].mean() - data.y[data.sensitive_attr == 1].mean()).item(),\n",
    "    std_gap = torch.abs(data.y[data.sensitive_attr == 0].std() - data.y[data.sensitive_attr == 1].std()).item(),\n",
    "    corr_sa_y = torch.corrcoef(torch.stack([data.y, data.sensitive_attr.float()]))[0,1].item(),\n",
    "    homophily_sensitive = compute_homophily(data.sensitive_attr, data.edge_index)  # 별도 함수\n",
    ")\n",
    "\n",
    "lr=params['lr']\n",
    "hidden_dim=params['hidden_dim']\n",
    "lambda1=params['lambda1']\n",
    "lambda2=params['lambda2']\n",
    "lambda3=params['lambda3']\n",
    "    \n",
    "performances, fairnesss = [], []\n",
    "\n",
    "for run in range(running_times):\n",
    "    model = FairRegGNN(data.x.size(1), hidden_dim).to(data.x.device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_mse = float('inf')\n",
    "    best_result = {}\n",
    "\n",
    "    masked_edge_index = mask_same_group_edges(data.edge_index, data.sensitive_attr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        output = model(data.x, masked_edge_index)\n",
    "\n",
    "        mse = F.mse_loss(output[data.idx_train], data.y[data.idx_train])\n",
    "        mean_gap = mean_gap_loss(output[data.idx_train], data.sensitive_attr[data.idx_train])\n",
    "        corr = correlation_loss(output[data.idx_train], data.sensitive_attr[data.idx_train])\n",
    "        w_loss = wasserstein_loss(output[data.idx_train], data.sensitive_attr[data.idx_train]).to(data.x.device)\n",
    "\n",
    "        loss = mse + lambda1 * mean_gap + lambda2 * corr + lambda3 * w_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(data.x, masked_edge_index)\n",
    "\n",
    "            mse_val = mean_squared_error(data.y[data.idx_val].cpu(), output[data.idx_val].cpu())\n",
    "            mae_val = mean_absolute_error(data.y[data.idx_val].cpu(), output[data.idx_val].cpu())\n",
    "            r2_val = r2_score(data.y[data.idx_val].cpu(), output[data.idx_val].cpu())\n",
    "            mse_diff_val, w_dist_val = fair_metric_regression(output[data.idx_val], data.y[data.idx_val], data.sensitive_attr[data.idx_val])\n",
    "\n",
    "            mse_test = mean_squared_error(data.y[data.idx_test].cpu(), output[data.idx_test].cpu())\n",
    "            mae_test = mean_absolute_error(data.y[data.idx_test].cpu(), output[data.idx_test].cpu())\n",
    "            r2_test = r2_score(data.y[data.idx_test].cpu(), output[data.idx_test].cpu())\n",
    "            mse_diff_test, w_dist_test = fair_metric_regression(output[data.idx_test], data.y[data.idx_test], data.sensitive_attr[data.idx_test])\n",
    "\n",
    "        if mse_val < best_mse:\n",
    "            best_mse = mse_val\n",
    "            best_result = {\n",
    "                'mse': mse_test,\n",
    "                'mae': mae_test,\n",
    "                'r2': r2_test,\n",
    "                'mse_diff': mse_diff_test,\n",
    "                'w_dist': w_dist_test\n",
    "            }\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"[FairRegGNN] Run {run+1}, Epoch {epoch:03d} | \"\n",
    "                    f\"Val MSE: {mse_val:.4f}, MAE: {mae_val:.4f}, R2: {r2_val:.4f} | \"\n",
    "                    f\"Test MSE: {mse_test:.4f}, MAE: {mae_test:.4f}, R2: {r2_test:.4f}\")\n",
    "\n",
    "    performances.append([best_result['mse'], best_result['mae'], best_result['r2']])\n",
    "    fairnesss.append([best_result['mse_diff'], best_result['w_dist']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d72ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(performances)\n",
    "print(fairnesss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47548940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "performance_mean = np.around(np.mean(performances, axis=0), 4)\n",
    "performance_std = np.around(np.std(performances, axis=0), 4)\n",
    "fairness_mean = np.around(np.mean(fairnesss, axis=0), 4)\n",
    "fairness_std = np.around(np.std(fairnesss, axis=0), 4)\n",
    "\n",
    "print(\"\\nStatistical Results:\")\n",
    "print(f\"Performance Mean: MSE={performance_mean[0]}, MAE={performance_mean[1]}, R2={performance_mean[2]}\")\n",
    "print(f\"Performance Std:  MSE={performance_std[0]}, MAE={performance_std[1]}, R2={performance_std[2]}\")\n",
    "print(f\"Fairness Mean:    MSE_diff={fairness_mean[0]}, W_dist={fairness_mean[1]}\")\n",
    "print(f\"Fairness Std:     MSE_diff={fairness_std[0]}, W_dist={fairness_std[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92e507e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "d597dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MYFairGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, lambda_fair=1.0, lambda_wass=1.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.lambda_fair = lambda_fair\n",
    "        self.lambda_wass = lambda_wass\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        preds = self.fc(x).squeeze(-1)  # [N]\n",
    "        return preds\n",
    "\n",
    "    def fairness_loss(self, preds, sensitive):\n",
    "        group0 = preds[sensitive == 0]\n",
    "        group1 = preds[sensitive == 1]\n",
    "        if group0.numel() == 0 or group1.numel() == 0:\n",
    "            return torch.tensor(0.0, device=preds.device)\n",
    "        return torch.abs(group0.mean() - group1.mean())\n",
    "\n",
    "    def wasserstein_loss(self, preds, sensitive):\n",
    "        from scipy.stats import wasserstein_distance\n",
    "        group0 = preds[sensitive == 0].detach().cpu()\n",
    "        group1 = preds[sensitive == 1].detach().cpu()\n",
    "        if group0.numel() == 0 or group1.numel() == 0:\n",
    "            return torch.tensor(0.0, device=preds.device)\n",
    "        return torch.tensor(wasserstein_distance(group0, group1), device=preds.device)\n",
    "\n",
    "    def weighted_mse_loss(preds, y_true, sensitive):\n",
    "        weights = torch.ones_like(y_true)\n",
    "        weights[sensitive == 1] = 2.0  # G1 더 중요하게\n",
    "        return ((preds - y_true) ** 2 * weights).mean()\n",
    "\n",
    "    def correlation_loss(preds, sensitive):\n",
    "        preds = preds - preds.mean()\n",
    "        sensitive = sensitive.float() - sensitive.float().mean()\n",
    "        return torch.sum(preds * sensitive) / (\n",
    "            torch.sqrt(torch.sum(preds ** 2)) * torch.sqrt(torch.sum(sensitive ** 2)) + 1e-8\n",
    "        )\n",
    "\n",
    "    def total_loss(self, preds, y_true, sensitive):\n",
    "        # mse_loss = weighted_mse_loss(preds, y_true, sensitive)\n",
    "        mse_loss = nn.MSELoss()(preds, y_true)\n",
    "        fair_loss = self.fairness_loss(preds, sensitive)\n",
    "        wass_loss = self.wasserstein_loss(preds, sensitive)\n",
    "        corr_loss = correlation_loss(preds, sensitive)\n",
    "\n",
    "        return mse_loss + self.lambda_fair * fair_loss + self.lambda_wass * wass_loss + 1.0 * corr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "b0077f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FairRegGNN] Run 1, Epoch 000 | Val MSE: 530.0490, MAE: 20.4089, R2: -2523.0430 | Test MSE: 578.0283, MAE: 21.4182, R2: -2751.5158\n",
      "[FairRegGNN] Run 1, Epoch 010 | Val MSE: 763.0615, MAE: 24.3512, R2: -3632.6262 | Test MSE: 834.8447, MAE: 25.5940, R2: -3974.4512\n",
      "[FairRegGNN] Run 1, Epoch 020 | Val MSE: 27.3614, MAE: 4.4246, R2: -129.2925 | Test MSE: 29.0300, MAE: 4.6557, R2: -137.2380\n",
      "[FairRegGNN] Run 1, Epoch 030 | Val MSE: 64.9731, MAE: 6.9154, R2: -308.3958 | Test MSE: 69.6175, MAE: 7.2739, R2: -330.5119\n",
      "[FairRegGNN] Run 1, Epoch 040 | Val MSE: 16.2321, MAE: 3.6316, R2: -76.2956 | Test MSE: 18.1407, MAE: 3.8135, R2: -85.3842\n",
      "[FairRegGNN] Run 1, Epoch 050 | Val MSE: 9.5962, MAE: 2.8033, R2: -44.6963 | Test MSE: 10.7863, MAE: 2.9428, R2: -50.3632\n",
      "[FairRegGNN] Run 1, Epoch 060 | Val MSE: 0.5182, MAE: 0.5183, R2: -1.4677 | Test MSE: 0.4278, MAE: 0.4958, R2: -1.0369\n",
      "[FairRegGNN] Run 1, Epoch 070 | Val MSE: 1.7039, MAE: 0.9361, R2: -7.1139 | Test MSE: 1.6202, MAE: 0.9762, R2: -6.7150\n",
      "[FairRegGNN] Run 1, Epoch 080 | Val MSE: 0.8850, MAE: 0.6486, R2: -3.2143 | Test MSE: 0.7865, MAE: 0.6512, R2: -2.7450\n",
      "[FairRegGNN] Run 1, Epoch 090 | Val MSE: 0.4489, MAE: 0.4951, R2: -1.1374 | Test MSE: 0.3662, MAE: 0.4692, R2: -0.7438\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "epochs=100\n",
    "running_times=1\n",
    "RUNNING_TIME = running_times\n",
    "hidden_dim=64 \n",
    "\n",
    "# 데이터셋 \n",
    "data = Data(x=features, edge_index=edge_index, y=labels.float(), sensitive_attr=sens, quantile_tau=torch.full((features.shape[0],), tau))\n",
    "data.idx_train = idx_train\n",
    "data.idx_val = idx_val\n",
    "data.idx_test = idx_test\n",
    "data.idx_sens_train = idx_sens_train\n",
    "\n",
    "performances, fairnesss = [], []\n",
    "\n",
    "for run in range(running_times):\n",
    "    model = MYFairGNN(data.x.size(1), hidden_dim).to(data.x.device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_mse = float('inf')\n",
    "    best_result = {}\n",
    "\n",
    "    masked_edge_index = mask_same_group_edges(data.edge_index, data.sensitive_attr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        output = model(data.x, masked_edge_index)\n",
    "        \n",
    "        loss = model.total_loss(output[data.idx_train], data.y[data.idx_train], data.sensitive_attr[data.idx_train])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(data.x, masked_edge_index)\n",
    "\n",
    "            mse_val = mean_squared_error(data.y[data.idx_val].cpu(), output[data.idx_val].cpu())\n",
    "            mae_val = mean_absolute_error(data.y[data.idx_val].cpu(), output[data.idx_val].cpu())\n",
    "            r2_val = r2_score(data.y[data.idx_val].cpu(), output[data.idx_val].cpu())\n",
    "            mse_diff_val, w_dist_val = fair_metric_regression(output[data.idx_val], data.y[data.idx_val], data.sensitive_attr[data.idx_val])\n",
    "\n",
    "            mse_test = mean_squared_error(data.y[data.idx_test].cpu(), output[data.idx_test].cpu())\n",
    "            mae_test = mean_absolute_error(data.y[data.idx_test].cpu(), output[data.idx_test].cpu())\n",
    "            r2_test = r2_score(data.y[data.idx_test].cpu(), output[data.idx_test].cpu())\n",
    "            mse_diff_test, w_dist_test = fair_metric_regression(output[data.idx_test], data.y[data.idx_test], data.sensitive_attr[data.idx_test])\n",
    "\n",
    "        if mse_val < best_mse:\n",
    "            best_mse = mse_val\n",
    "            best_result = {\n",
    "                'mse': mse_test,\n",
    "                'mae': mae_test,\n",
    "                'r2': r2_test,\n",
    "                'mse_diff': mse_diff_test,\n",
    "                'w_dist': w_dist_test\n",
    "            }\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"[FairRegGNN] Run {run+1}, Epoch {epoch:03d} | \"\n",
    "                    f\"Val MSE: {mse_val:.4f}, MAE: {mae_val:.4f}, R2: {r2_val:.4f} | \"\n",
    "                    f\"Test MSE: {mse_test:.4f}, MAE: {mae_test:.4f}, R2: {r2_test:.4f}\")\n",
    "\n",
    "    performances.append([best_result['mse'], best_result['mae'], best_result['r2']])\n",
    "    fairnesss.append([best_result['mse_diff'], best_result['w_dist']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "39977e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28515780115754924, 0.4782689298316836, -0.3578942912264249]]\n",
      "[[0.04715731739997864, 0.07459782689181028]]\n"
     ]
    }
   ],
   "source": [
    "print(performances)\n",
    "print(fairnesss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "f18eaec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistical Results:\n",
      "Performance Mean: MSE=0.2852, MAE=0.4783, R2=-0.3579\n",
      "Performance Std:  MSE=0.0, MAE=0.0, R2=0.0\n",
      "Fairness Mean:    MSE_diff=0.0472, W_dist=0.0746\n",
      "Fairness Std:     MSE_diff=0.0, W_dist=0.0\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "performance_mean = np.around(np.mean(performances, axis=0), 4)\n",
    "performance_std = np.around(np.std(performances, axis=0), 4)\n",
    "fairness_mean = np.around(np.mean(fairnesss, axis=0), 4)\n",
    "fairness_std = np.around(np.std(fairnesss, axis=0), 4)\n",
    "\n",
    "print(\"\\nStatistical Results:\")\n",
    "print(f\"Performance Mean: MSE={performance_mean[0]}, MAE={performance_mean[1]}, R2={performance_mean[2]}\")\n",
    "print(f\"Performance Std:  MSE={performance_std[0]}, MAE={performance_std[1]}, R2={performance_std[2]}\")\n",
    "print(f\"Fairness Mean:    MSE_diff={fairness_mean[0]}, W_dist={fairness_mean[1]}\")\n",
    "print(f\"Fairness Std:     MSE_diff={fairness_std[0]}, W_dist={fairness_std[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3e6d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "193f1f39",
   "metadata": {},
   "source": [
    "## 모델 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48a8b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 모델\n",
    "def quantile_loss(y_true, y_pred, tau=0.9):\n",
    "    error = y_true - y_pred\n",
    "    return torch.mean(torch.max(tau * error, (tau - 1) * error))\n",
    "\n",
    "class MYPlainGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)  # 단일 회귀 출력\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        out = self.fc(x).squeeze(-1)  # [N] 형태\n",
    "        return out\n",
    "\n",
    "class MYFairGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, lambda_fair=1.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.lambda_fair = lambda_fair\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        preds = self.fc(x).squeeze(-1)  # [N]\n",
    "        return preds\n",
    "\n",
    "    def fairness_loss(self, preds, sensitive):\n",
    "        group0 = preds[sensitive == 0]\n",
    "        group1 = preds[sensitive == 1]\n",
    "        if len(group0) == 0 or len(group1) == 0:\n",
    "            return torch.tensor(0.0, device=preds.device)\n",
    "        return torch.abs(group0.mean() - group1.mean())\n",
    "\n",
    "    def total_loss(self, preds, y_true, sensitive, tau=0.9):\n",
    "        # 기본 quantile loss\n",
    "        error = y_true - preds\n",
    "        quantile_loss = torch.mean(torch.max(tau * error, (tau - 1) * error))\n",
    "\n",
    "        # 공정성 정규화\n",
    "        fair_loss = self.fairness_loss(preds, sensitive)\n",
    "\n",
    "        return quantile_loss + self.lambda_fair * fair_loss\n",
    "    \n",
    "\n",
    "# Our\n",
    "class DAFGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, device, lambda_fair=1.0, gamma=1.0, alpha=1.0, use_projection=True, use_fairness=True, use_lambda=True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.lambda_fair = lambda_fair\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.use_projection = use_projection\n",
    "        self.use_fairness = use_fairness\n",
    "        self.use_lambda = use_lambda\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, x, edge_index, return_hidden=False):\n",
    "        h = F.relu(self.conv1(x, edge_index))\n",
    "        h = F.relu(self.conv2(h, edge_index))\n",
    "        \n",
    "        preds = self.fc(h).squeeze(-1)\n",
    "\n",
    "        if return_hidden:\n",
    "            return preds, h\n",
    "        else:\n",
    "            return preds\n",
    "        \n",
    "    def distribution_aware_fairness_loss(self, preds, sensitive, var_gap=True):\n",
    "        g0 = preds[sensitive == 0]\n",
    "        g1 = preds[sensitive == 1]\n",
    "\n",
    "        if len(g0) == 0 or len(g1) == 0:\n",
    "            return torch.tensor(0.0, device=preds.device)\n",
    "\n",
    "        mean_gap = torch.abs(g0.mean() - g1.mean())\n",
    "        gap = mean_gap\n",
    "        \n",
    "        if var_gap:\n",
    "            var_gap = torch.abs(g0.var(unbiased=False) - g1.var(unbiased=False))\n",
    "            gap = mean_gap + self.alpha * var_gap\n",
    "        return gap\n",
    "\n",
    "    def compute_bias_score(self, hidden, sensitive):\n",
    "        g0 = hidden[sensitive == 0]\n",
    "        g1 = hidden[sensitive == 1]\n",
    "        bias = torch.norm(g0.mean(dim=0) - g1.mean(dim=0), p=2)\n",
    "        return bias\n",
    "\n",
    "    def adaptive_projection(self, h, bias_score):\n",
    "        h_norm = h.norm(p=2, dim=1, keepdim=True) + 1e-6\n",
    "        adaptive_lambda = self.lambda_fair * (1 + self.gamma * bias_score)\n",
    "        scale = torch.min(torch.ones_like(h_norm), adaptive_lambda.unsqueeze(-1) / h_norm)\n",
    "        return h * scale\n",
    "\n",
    "    def hierarchical_lambda(self, epoch, max_lambda=1.0, beta=0.05):\n",
    "        device = next(self.parameters()).device  # 모델 파라미터의 디바이스 확인\n",
    "        return max_lambda * (1 - torch.exp(-beta * torch.tensor(epoch, dtype=torch.float32, device=device)))\n",
    "\n",
    "    def total_loss(self, preds, hidden, sensitive, y_true, epoch, tau=0.9):\n",
    "        \n",
    "        loss_qr = quantile_loss(y_true, preds, tau)\n",
    "        \n",
    "        # Projection\n",
    "        if self.use_projection:\n",
    "            bias_score = self.compute_bias_score(hidden, sensitive)\n",
    "            hidden = self.adaptive_projection(hidden, bias_score)\n",
    "        \n",
    "        # Fairness\n",
    "        if self.use_fairness:\n",
    "            fair_loss = self.distribution_aware_fairness_loss(preds, sensitive)  \n",
    "        else:\n",
    "            fair_loss =  torch.tensor(0.0, device=preds.device)\n",
    "\n",
    "        # if self.use_lambda:\n",
    "        #     lambda_fair = self.hierarchical_lambda(epoch, max_lambda=self.lambda_fair)\n",
    "        # else:\n",
    "        #     lambda_fair = self.lambda_fair\n",
    "\n",
    "        return loss_qr + lambda_fair * fair_loss\n",
    "    \n",
    "def get_sen_da(sens, idx_sens_train):\n",
    "    num_nodes = sens.size(0)\n",
    "    group_0_mask = (sens == 0)\n",
    "    group_1_mask = (sens == 1)\n",
    "\n",
    "    # idx_sens_train에서 그룹별 마스크\n",
    "    train_group_0_mask = group_0_mask.clone()\n",
    "    train_group_1_mask = group_1_mask.clone()\n",
    "    train_group_0_mask[~idx_sens_train] = False\n",
    "    train_group_1_mask[~idx_sens_train] = False\n",
    "\n",
    "    group_0 = group_0_mask.float()\n",
    "    group_1 = group_1_mask.float()\n",
    "\n",
    "    sum_group_0 = train_group_0_mask.float().sum()\n",
    "    sum_group_1 = train_group_1_mask.float().sum()\n",
    "    if sum_group_0 == 0 or sum_group_1 == 0:\n",
    "        raise ValueError(f\"get_sen_da: One of the groups has no samples in idx_sens_train! group_0 sum: {sum_group_0}, group_1 sum: {sum_group_1}\")\n",
    "\n",
    "    group_0[idx_sens_train] /= sum_group_0\n",
    "    group_1[idx_sens_train] /= sum_group_1\n",
    "\n",
    "    sen_mat = torch.stack([group_0, group_1], dim=0)\n",
    "    return sen_mat\n",
    "\n",
    "class DAFMP(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, K, dropout=0.0, cached=False):\n",
    "        super(DAFMP, self).__init__()\n",
    "        self.K = K\n",
    "        self.dropout = dropout\n",
    "        self.cached = cached\n",
    "        self._cached_sen = None\n",
    "        self.propa = GATConv(in_feats, out_feats, heads=1)\n",
    "\n",
    "    def forward(self, x, edge_index, idx_sens_train, sens, tau, mean_baseline=1.0, var_baseline=1.0, range_baseline=1.0):\n",
    "        if self.K <= 0:\n",
    "            return x, None, None, None\n",
    "\n",
    "        if self._cached_sen is None:\n",
    "            sen_mat = get_sen_da(sens=sens, idx_sens_train=idx_sens_train)\n",
    "            if self.cached:\n",
    "                self._cached_sen = sen_mat\n",
    "        else:\n",
    "            sen_mat = self._cached_sen\n",
    "\n",
    "        for _ in range(self.K):\n",
    "            y = self.propa(x, edge_index)\n",
    "\n",
    "            group_counts = sen_mat.sum(dim=1, keepdim=True)\n",
    "            z_mean = sen_mat @ y / (sen_mat @ torch.ones_like(y))\n",
    "            z_var = (sen_mat @ (y ** 2)) / (group_counts + 1e-8) - z_mean ** 2\n",
    "\n",
    "            group0_mask = sen_mat[0].bool()\n",
    "            group1_mask = sen_mat[1].bool()\n",
    "            y_group0 = y[group0_mask]\n",
    "            y_group1 = y[group1_mask]\n",
    "\n",
    "            if y_group0.size(0) == 0 or y_group1.size(0) == 0:\n",
    "                raise ValueError(\"One of the groups has no samples! Check your sens and idx_sens_train.\")\n",
    "\n",
    "            z_min0, _ = torch.min(y_group0, dim=0, keepdim=True)\n",
    "            z_max0, _ = torch.max(y_group0, dim=0, keepdim=True)\n",
    "            z_min1, _ = torch.min(y_group1, dim=0, keepdim=True)\n",
    "            z_max1, _ = torch.max(y_group1, dim=0, keepdim=True)\n",
    "\n",
    "            z_range0 = z_max0 - z_min0\n",
    "            z_range1 = z_max1 - z_min1\n",
    "\n",
    "            mean_gap = torch.abs(z_mean[0] - z_mean[1])\n",
    "            var_gap = torch.abs(z_var[0] - z_var[1])\n",
    "            range_gap = torch.abs(z_range0 - z_range1)\n",
    "\n",
    "            mean_gap_norm = mean_gap / (mean_baseline + 1e-8)\n",
    "            var_gap_norm = var_gap / (var_baseline + 1e-8)\n",
    "            range_gap_norm = range_gap / (range_baseline + 1e-8)\n",
    "\n",
    "            tau_scalar = tau.mean() if isinstance(tau, torch.Tensor) and tau.numel() > 1 else tau\n",
    "\n",
    "            correction = tau_scalar * (mean_gap_norm + var_gap_norm + range_gap_norm)\n",
    "\n",
    "            x = y - correction\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        return x, mean_gap, var_gap, range_gap\n",
    "\n",
    "class DAFGNN_2(nn.Module):\n",
    "    def __init__(self, input_size, size, prop, num_layer):\n",
    "        super().__init__()\n",
    "        self.first = nn.Linear(input_size, size)\n",
    "        self.hidden = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layer - 2)])\n",
    "        self.last = nn.Linear(size, 1)\n",
    "        self.prop = prop\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        sens = data.sensitive_attr\n",
    "        tau = data.quantile_tau\n",
    "\n",
    "        out = F.relu(self.first(x))\n",
    "        for layer in self.hidden:\n",
    "            out = F.relu(layer(out))\n",
    "\n",
    "        # DAFMP 호출 후 tuple 언팩\n",
    "        out, mean_gap, var_gap, range_gap = self.prop(\n",
    "            out, edge_index=edge_index, sens=sens, idx_sens_train=data.idx_sens_train, tau=tau\n",
    "        )\n",
    "\n",
    "        x = self.last(out).squeeze(-1)\n",
    "        return x, mean_gap, var_gap, range_gap\n",
    "\n",
    "def daf_model(data, num_layers=5, num_hidden=64, num_gnn_layer=2):\n",
    "    prop = DAFMP(\n",
    "        in_feats=num_hidden,\n",
    "        out_feats=num_hidden,\n",
    "        K=num_layers,\n",
    "        cached=False\n",
    "    )\n",
    "\n",
    "    model = DAFGNN_2(\n",
    "        input_size=data.num_features,\n",
    "        size=num_hidden,\n",
    "        prop=prop,\n",
    "        num_layer=num_gnn_layer\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def daf_loss(y_true, y_pred, tau, mean_gap, var_gap, range_gap, lambda_fair, mean_baseline=1.0, var_baseline=1.0, range_baseline=1.0):\n",
    "    diff = y_true - y_pred\n",
    "    qr_loss = torch.mean(torch.max(tau * diff, (tau - 1) * diff))\n",
    "\n",
    "    mean_gap_norm = mean_gap / (mean_baseline + 1e-8)\n",
    "    var_gap_norm = var_gap / (var_baseline + 1e-8)\n",
    "    range_gap_norm = range_gap / (range_baseline + 1e-8)\n",
    "\n",
    "    # Reduce to scalar\n",
    "    fairness_loss = lambda_fair * (mean_gap_norm.mean() + var_gap_norm.mean() + range_gap_norm.mean())\n",
    "\n",
    "    return qr_loss + fairness_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f2136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FairRegGNN\n",
    "\n",
    "# GNN 백본\n",
    "# 표준 GNN 아키텍처를 사용하여 노드 임베딩 생성\n",
    "# 회구 출력층: 노드 임베딩을 연속값으로 매핑\n",
    "\n",
    "# 공정성 매커니즘\n",
    "# FairGNN에서 영감: 적대적 학습을 사용하여 민감 속성 정보를 임베딩에서 제거\n",
    "# FMP에서 영감: 메세지 패싱 과정에서 그룹별 예측값 중심을 정규화\n",
    "# 새로운 요소: 일반 머신러닝 FairRegression 프레임워크를 통합하여 예측 오차 균등성과 예측값 분포 균등성을 동시에 최적화\n",
    "# 그래프 구조 정규화: FairDrop과 유사하게, 민감 속성 간 homophily를 줄이기 위해 엣지 드롭아웃 또는 가중치 조정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928f3f93",
   "metadata": {},
   "source": [
    "## 데이터 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4e9a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 분석: Pokec_z, Pokec_n, NBA, German 데이터셋을 분석하여 민감 속성(sensitive attribute)과 예측 속성(predict attribute)의 분포, 그래프 구조 특성(homophily 등)을 파악하고, 공정성 문제를 식별.\n",
    "# 데이터셋의 특성과 공정성 문제를 이해하기 위해 다음 분석을 수행\n",
    "\n",
    "# 데이터셋 분석 결과를 새로운 GNN 기반 공정성 모델 설계에 효과적으로 연결\n",
    "# 분석에서 도출된 통찰을 모델의 공정성 메커니즘, 손실 함수, 그래프 구조 처리 방식 등에 반영\n",
    "# 질문에서 제공된 데이터셋(Pokec_z, Pokec_n, NBA, German)을 분석하여 FairGNN 및 FMP와 차별화된 새로운 회귀 공정성 모델(FairRegGNN)을 설계\n",
    "# FairRegGNN 모델 설계에 필요한 하이퍼파라미터(lambda1, lambda2, lambda3, group_weights)를 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "41a68214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: German\n",
      "Total nodes: 1000\n",
      "Total edges: 44484\n",
      "Sensitive Attribute Distribution:\n",
      " - Group 0 (A=0): 310 (31.00%)\n",
      " - Group 1 (A=1): 690 (69.00%)\n",
      "Predict Attribute Summary:\n",
      " - Overall: mean=0.7000, std=0.4583\n",
      " - Group 0: mean=0.6484, std=0.4775\n",
      " - Group 1: mean=0.7232, std=0.4474\n",
      "Correlation (Sensitive vs Label):\n",
      " - Pearson:  0.0755\n",
      " - Spearman: 0.0755\n",
      "Graph Homophily:\n",
      " - Sensitive attribute homophily: 0.8092\n",
      " - Label homophily: 0.5963\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset: {dn}\")\n",
    "print(f\"Total nodes: {features.shape[0]}\")\n",
    "print(f\"Total edges: {edge_index.shape[1]}\")\n",
    "\n",
    "### 1. 민감 속성 분포\n",
    "sens_np = sens.cpu().numpy()\n",
    "sens0_cnt = np.sum(sens_np == 0)\n",
    "sens1_cnt = np.sum(sens_np == 1)\n",
    "total = len(sens_np)\n",
    "print(f\"Sensitive Attribute Distribution:\")\n",
    "print(f\" - Group 0 (A=0): {sens0_cnt} ({sens0_cnt / total:.2%})\")\n",
    "print(f\" - Group 1 (A=1): {sens1_cnt} ({sens1_cnt / total:.2%})\")\n",
    "\n",
    "### 2. 예측 대상 분포\n",
    "labels_np = labels.cpu().numpy()\n",
    "print(f\"Predict Attribute Summary:\")\n",
    "print(f\" - Overall: mean={labels_np.mean():.4f}, std={labels_np.std():.4f}\")\n",
    "print(f\" - Group 0: mean={labels_np[sens_np==0].mean():.4f}, std={labels_np[sens_np==0].std():.4f}\")\n",
    "print(f\" - Group 1: mean={labels_np[sens_np==1].mean():.4f}, std={labels_np[sens_np==1].std():.4f}\")\n",
    "\n",
    "# plt.figure(figsize=(6, 3))\n",
    "# sns.kdeplot(labels_np[sens_np == 0], label='Group 0')\n",
    "# sns.kdeplot(labels_np[sens_np == 1], label='Group 1')\n",
    "# plt.title(f\"{dn} Dataset: Distribution by Sensitive Group\")\n",
    "# plt.xlabel(\"Label Value\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "### 3. 상관계수\n",
    "pear_corr, _ = pearsonr(sens_np, labels_np)\n",
    "spear_corr, _ = spearmanr(sens_np, labels_np)\n",
    "print(f\"Correlation (Sensitive vs Label):\")\n",
    "print(f\" - Pearson:  {pear_corr:.4f}\")\n",
    "print(f\" - Spearman: {spear_corr:.4f}\")\n",
    "\n",
    "### 4. Homophily 분석\n",
    "def compute_homophily(attr_tensor, edge_index):\n",
    "    attr = attr_tensor.cpu().numpy()\n",
    "    src, dst = edge_index[0].cpu().numpy(), edge_index[1].cpu().numpy()\n",
    "    matches = (attr[src] == attr[dst])\n",
    "    return np.mean(matches)\n",
    "\n",
    "sens_homophily = compute_homophily(sens, edge_index)\n",
    "label_homophily = compute_homophily(labels, edge_index)\n",
    "\n",
    "print(f\"Graph Homophily:\")\n",
    "print(f\" - Sensitive attribute homophily: {sens_homophily:.4f}\")\n",
    "print(f\" - Label homophily: {label_homophily:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ecb3c880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_graph_fairness_stats(data, edge_index):\n",
    "    sens = data.sensitive_attr.cpu().numpy()\n",
    "    labels = data.y.cpu().numpy()\n",
    "    src, dst = edge_index[0].cpu().numpy(), edge_index[1].cpu().numpy()\n",
    "    num_nodes = len(sens)\n",
    "\n",
    "    print(\"========== Dataset Summary ==========\")\n",
    "    print(f\"Total nodes: {num_nodes}\")\n",
    "    print(f\"Total edges: {edge_index.shape[1]}\")\n",
    "\n",
    "    # 1. 민감 속성 분포\n",
    "    g0, g1 = np.sum(sens == 0), np.sum(sens == 1)\n",
    "    print(\"\\n--- Sensitive Attribute Distribution ---\")\n",
    "    print(f\"Group 0: {g0} ({g0/num_nodes:.2%})\")\n",
    "    print(f\"Group 1: {g1} ({g1/num_nodes:.2%})\")\n",
    "\n",
    "    # 2. 예측값 분포\n",
    "    print(\"\\n--- Label Distribution ---\")\n",
    "    print(f\"Overall: mean={labels.mean():.4f}, std={labels.std():.4f}\")\n",
    "    print(f\"Group 0: mean={labels[sens==0].mean():.4f}, std={labels[sens==0].std():.4f}\")\n",
    "    print(f\"Group 1: mean={labels[sens==1].mean():.4f}, std={labels[sens==1].std():.4f}\")\n",
    "\n",
    "    # 3. 상관관계\n",
    "    pear, _ = pearsonr(sens, labels)\n",
    "    spear, _ = spearmanr(sens, labels)\n",
    "    print(\"\\n--- Correlation (Sensitive vs Label) ---\")\n",
    "    print(f\"Pearson:  {pear:.4f}\")\n",
    "    print(f\"Spearman: {spear:.4f}\")\n",
    "\n",
    "    # 4. Homophily\n",
    "    def homophily(attr):\n",
    "        return np.mean(attr[src] == attr[dst])\n",
    "    sens_hom = homophily(sens)\n",
    "    label_hom = homophily(labels.round())  # 연속형 label인 경우 이진화\n",
    "    print(\"\\n--- Graph Homophily ---\")\n",
    "    print(f\"Sensitive attribute homophily: {sens_hom:.4f}\")\n",
    "    print(f\"Label homophily: {label_hom:.4f}\")\n",
    "\n",
    "    # 5. Group별 node degree\n",
    "    degree = np.bincount(src, minlength=num_nodes)\n",
    "    deg_g0 = degree[sens == 0]\n",
    "    deg_g1 = degree[sens == 1]\n",
    "    print(\"\\n--- Node Degree (per group) ---\")\n",
    "    print(f\"Group 0: mean={deg_g0.mean():.2f}, std={deg_g0.std():.2f}\")\n",
    "    print(f\"Group 1: mean={deg_g1.mean():.2f}, std={deg_g1.std():.2f}\")\n",
    "\n",
    "    # 6. 이웃 구성 동질성 (각 노드 이웃 중 같은 그룹 비율 평균)\n",
    "    same_group_ratios = []\n",
    "    for i in range(num_nodes):\n",
    "        neighbors = dst[src == i]\n",
    "        if len(neighbors) > 0:\n",
    "            same_ratio = np.mean(sens[neighbors] == sens[i])\n",
    "            same_group_ratios.append(same_ratio)\n",
    "    print(\"\\n--- Neighborhood Composition ---\")\n",
    "    print(f\"Average same-group neighbor ratio: {np.mean(same_group_ratios):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866095b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb111a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f392c3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b7b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dff0059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df1340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델\n",
    "in_dim = data.x.size(1)\n",
    "data = data.to(device)\n",
    "data.sensitive_attr = data.sensitive_attr.to(device)\n",
    "\n",
    "fgnn_model = MYFairGNN(in_dim, hidden_dim, lambda_fair).to(device)\n",
    "bgnn_model = MYPlainGNN(in_dim, hidden_dim).to(device)\n",
    "fairgnn_model = FairGNN_Q(in_dim, hidden_dim, tau).to(device)\n",
    "Fmp_model = fmp_model(data).to(device)\n",
    "\n",
    "dafgnn_model = DAFGNN(in_dim, hidden_dim, device, lambda_fair).to(device)\n",
    "dafgnn2_model = daf_model(data).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c36ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "optimizer = Adam(dafgnn_model.parameters(), lr=lr, weight_decay=weight)\n",
    "\n",
    "print('-' * 100)\n",
    "n_epochs = int(n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    dafgnn_model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    preds, hidden = dafgnn_model(data.x, data.edge_index, return_hidden=True)\n",
    "    loss = dafgnn_model.total_loss(preds[data.idx_train], hidden[data.idx_train], data.sensitive_attr[data.idx_train], data.y[data.idx_train], epoch, tau)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0 or epoch == 1:\n",
    "        print(f\"[Epoch {epoch}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# 모델 평가\n",
    "dafgnn_model.eval()\n",
    "preds, _ = dafgnn_model(data.x, data.edge_index, return_hidden=True)\n",
    "\n",
    "# Tensor → numpy\n",
    "y_pred = preds[idx_eval].detach().cpu().numpy().flatten()\n",
    "y_true = data.y[idx_eval].detach().cpu().numpy().flatten()\n",
    "sensitive_attr = data.sensitive_attr[idx_eval].detach().cpu().numpy().flatten()\n",
    "\n",
    "# 기본 오류 계산\n",
    "error = y_true - y_pred\n",
    "abs_error = np.abs(error)\n",
    "sq_error = error ** 2\n",
    "\n",
    "# MAPE 관련 계산 시 y_true가 너무 작으면 제거\n",
    "mask = np.abs(y_true) > 1e-4\n",
    "mape_safe = abs_error[mask] / (np.abs(y_true[mask]) + 1e-8)\n",
    "mape = np.mean(mape_safe)\n",
    "mape_var = np.var(mape_safe)\n",
    "\n",
    "# QR Loss\n",
    "error_torch = torch.tensor(error)\n",
    "qr_loss = torch.mean(torch.max(tau * error_torch, (tau - 1) * error_torch)).item()\n",
    "\n",
    "# 정확도\n",
    "mae = np.mean(abs_error)\n",
    "rmse = np.sqrt(np.mean(sq_error))\n",
    "mae_var = np.var(abs_error)\n",
    "r2 = 1 - np.sum(sq_error) / np.sum((y_true - y_true.mean()) ** 2)\n",
    "\n",
    "# 그룹별\n",
    "df = pd.DataFrame({\n",
    "    'y_true': y_true,\n",
    "    'y_pred': y_pred,\n",
    "    'group': sensitive_attr\n",
    "})\n",
    "\n",
    "group_metrics = {}\n",
    "for group_id in np.unique(sensitive_attr):\n",
    "    group = df[df['group'] == group_id]\n",
    "    g_m = np.mean(group['y_pred'])\n",
    "    g_err = np.abs(group['y_true'] - group['y_pred'])\n",
    "    g_sq = (group['y_true'] - group['y_pred']) ** 2\n",
    "\n",
    "    group_metrics[f'mean_group_{int(group_id)}'] = g_m\n",
    "    group_metrics[f'mae_group_{int(group_id)}'] = g_err.mean()\n",
    "    group_metrics[f'rmse_group_{int(group_id)}'] = np.sqrt(g_sq.mean())\n",
    "\n",
    "mean_gap = abs(group_metrics.get('mean_group_0', 0) - group_metrics.get('mean_group_1', 0))\n",
    "mae_gap = abs(group_metrics.get('mae_group_0', 0) - group_metrics.get('mae_group_1', 0))\n",
    "rmse_gap = abs(group_metrics.get('rmse_group_0', 0) - group_metrics.get('rmse_group_1', 0))\n",
    "\n",
    "# 민감속성과 오차의 상관관계\n",
    "corr_error = np.corrcoef(abs_error, sensitive_attr)[0, 1]    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8686953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "optimizer = Adam(dafgnn2_model.parameters(), lr=lr, weight_decay=weight)\n",
    "\n",
    "print('-' * 100)\n",
    "n_epochs = int(n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    dafgnn2_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    preds, mean_gap, var_gap, range_gap = dafgnn2_model(data)\n",
    "    loss = daf_loss(data.y[data.idx_train], preds[data.idx_train], tau, mean_gap, var_gap, range_gap, lambda_fair)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0 or epoch == 1:\n",
    "        print(f\"[Epoch {epoch}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# 모델 평가\n",
    "dafgnn2_model.eval()\n",
    "preds_tuple = dafgnn2_model(data)\n",
    "if isinstance(preds_tuple, tuple):\n",
    "    preds = preds_tuple[0]\n",
    "else:\n",
    "    preds = preds_tuple\n",
    "\n",
    "# Tensor → numpy\n",
    "y_pred = preds[idx_eval].detach().cpu().numpy().flatten()\n",
    "y_true = data.y[idx_eval].detach().cpu().numpy().flatten()\n",
    "sensitive_attr = data.sensitive_attr[idx_eval].detach().cpu().numpy().flatten()\n",
    "\n",
    "# 기본 오류 계산\n",
    "error = y_true - y_pred\n",
    "abs_error = np.abs(error)\n",
    "sq_error = error ** 2\n",
    "\n",
    "# ⚠️ MAPE 관련 계산 시 y_true가 너무 작으면 제거\n",
    "mask = np.abs(y_true) > 1e-4\n",
    "mape_safe = abs_error[mask] / (np.abs(y_true[mask]) + 1e-8)\n",
    "mape = np.mean(mape_safe)\n",
    "mape_var = np.var(mape_safe)\n",
    "\n",
    "# QR Loss\n",
    "error_torch = torch.tensor(error)\n",
    "qr_loss = torch.mean(torch.max(tau * error_torch, (tau - 1) * error_torch)).item()\n",
    "\n",
    "# 정확도\n",
    "mae = np.mean(abs_error)\n",
    "rmse = np.sqrt(np.mean(sq_error))\n",
    "mae_var = np.var(abs_error)\n",
    "r2 = 1 - np.sum(sq_error) / np.sum((y_true - y_true.mean()) ** 2)\n",
    "\n",
    "# 그룹별\n",
    "df = pd.DataFrame({\n",
    "    'y_true': y_true,\n",
    "    'y_pred': y_pred,\n",
    "    'group': sensitive_attr\n",
    "})\n",
    "\n",
    "group_metrics = {}\n",
    "for group_id in np.unique(sensitive_attr):\n",
    "    group = df[df['group'] == group_id]\n",
    "    g_m = np.mean(group['y_pred'])\n",
    "    g_err = np.abs(group['y_true'] - group['y_pred'])\n",
    "    g_sq = (group['y_true'] - group['y_pred']) ** 2\n",
    "\n",
    "    group_metrics[f'mean_group_{int(group_id)}'] = g_m\n",
    "    group_metrics[f'mae_group_{int(group_id)}'] = g_err.mean()\n",
    "    group_metrics[f'rmse_group_{int(group_id)}'] = np.sqrt(g_sq.mean())\n",
    "\n",
    "mean_gap = abs(group_metrics.get('mean_group_0', 0) - group_metrics.get('mean_group_1', 0))\n",
    "mae_gap = abs(group_metrics.get('mae_group_0', 0) - group_metrics.get('mae_group_1', 0))\n",
    "rmse_gap = abs(group_metrics.get('rmse_group_0', 0) - group_metrics.get('rmse_group_1', 0))\n",
    "\n",
    "# 민감속성과 오차의 상관관계\n",
    "corr_error = np.corrcoef(abs_error, sensitive_attr)[0, 1]    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0222822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "optimizer = Adam(fairgnn_model.parameters(), lr=lr, weight_decay=weight)\n",
    "\n",
    "print('-' * 100)\n",
    "n_epochs = int(n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    fairgnn_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    fairgnn_model.optimize(data, tau)\n",
    "    loss = fairgnn_model.G_loss \n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0 or epoch == 1:\n",
    "        print(f\"[Epoch {epoch}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# 모델 평가\n",
    "fairgnn_model.eval()\n",
    "preds, _ = fairgnn_model(data)\n",
    "\n",
    "# Tensor → numpy\n",
    "y_pred = preds[idx_eval].detach().cpu().numpy().flatten()\n",
    "y_true = data.y[idx_eval].detach().cpu().numpy().flatten()\n",
    "sensitive_attr = data.sensitive_attr[idx_eval].detach().cpu().numpy().flatten()\n",
    "\n",
    "# 기본 오류 계산\n",
    "error = y_true - y_pred\n",
    "abs_error = np.abs(error)\n",
    "sq_error = error ** 2\n",
    "\n",
    "# MAPE 관련 계산 시 y_true가 너무 작으면 제거\n",
    "mask = np.abs(y_true) > 1e-4\n",
    "mape_safe = abs_error[mask] / (np.abs(y_true[mask]) + 1e-8)\n",
    "mape = np.mean(mape_safe)\n",
    "mape_var = np.var(mape_safe)\n",
    "\n",
    "# QR Loss\n",
    "error_torch = torch.tensor(error)\n",
    "qr_loss = torch.mean(torch.max(tau * error_torch, (tau - 1) * error_torch)).item()\n",
    "\n",
    "# 정확도\n",
    "mae = np.mean(abs_error)\n",
    "rmse = np.sqrt(np.mean(sq_error))\n",
    "mae_var = np.var(abs_error)\n",
    "r2 = 1 - np.sum(sq_error) / np.sum((y_true - y_true.mean()) ** 2)\n",
    "\n",
    "# 그룹별\n",
    "df = pd.DataFrame({\n",
    "    'y_true': y_true,\n",
    "    'y_pred': y_pred,\n",
    "    'group': sensitive_attr\n",
    "})\n",
    "\n",
    "group_metrics = {}\n",
    "for group_id in np.unique(sensitive_attr):\n",
    "    group = df[df['group'] == group_id]\n",
    "    g_m = np.mean(group['y_pred'])\n",
    "    g_err = np.abs(group['y_true'] - group['y_pred'])\n",
    "    g_sq = (group['y_true'] - group['y_pred']) ** 2\n",
    "\n",
    "    group_metrics[f'mean_group_{int(group_id)}'] = g_m\n",
    "    group_metrics[f'mae_group_{int(group_id)}'] = g_err.mean()\n",
    "    group_metrics[f'rmse_group_{int(group_id)}'] = np.sqrt(g_sq.mean())\n",
    "\n",
    "mean_gap = abs(group_metrics.get('mean_group_0', 0) - group_metrics.get('mean_group_1', 0))\n",
    "mae_gap = abs(group_metrics.get('mae_group_0', 0) - group_metrics.get('mae_group_1', 0))\n",
    "rmse_gap = abs(group_metrics.get('rmse_group_0', 0) - group_metrics.get('rmse_group_1', 0))\n",
    "\n",
    "# 민감속성과 오차의 상관관계\n",
    "corr_error = np.corrcoef(abs_error, sensitive_attr)[0, 1]    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2563b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "optimizer = Adam(Fmp_model.parameters(), lr=lr, weight_decay=weight)\n",
    "\n",
    "print('-' * 100)\n",
    "n_epochs = int(n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    Fmp_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    preds = Fmp_model(data)\n",
    "    loss = quantile_loss(data.y[data.idx_train], preds[data.idx_train], tau)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0 or epoch == 1:\n",
    "        print(f\"[Epoch {epoch}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# 모델 평가\n",
    "Fmp_model.eval()\n",
    "preds = Fmp_model(data)\n",
    "\n",
    "# Tensor → numpy\n",
    "y_pred = preds[idx_eval].detach().cpu().numpy().flatten()\n",
    "y_true = data.y[idx_eval].detach().cpu().numpy().flatten()\n",
    "sensitive_attr = data.sensitive_attr[idx_eval].detach().cpu().numpy().flatten()\n",
    "\n",
    "# 기본 오류 계산\n",
    "error = y_true - y_pred\n",
    "abs_error = np.abs(error)\n",
    "sq_error = error ** 2\n",
    "\n",
    "# MAPE 관련 계산 시 y_true가 너무 작으면 제거\n",
    "mask = np.abs(y_true) > 1e-4\n",
    "mape_safe = abs_error[mask] / (np.abs(y_true[mask]) + 1e-8)\n",
    "mape = np.mean(mape_safe)\n",
    "mape_var = np.var(mape_safe)\n",
    "\n",
    "# QR Loss\n",
    "error_torch = torch.tensor(error)\n",
    "qr_loss = torch.mean(torch.max(tau * error_torch, (tau - 1) * error_torch)).item()\n",
    "\n",
    "# 정확도\n",
    "mae = np.mean(abs_error)\n",
    "rmse = np.sqrt(np.mean(sq_error))\n",
    "mae_var = np.var(abs_error)\n",
    "r2 = 1 - np.sum(sq_error) / np.sum((y_true - y_true.mean()) ** 2)\n",
    "\n",
    "# 그룹별\n",
    "df = pd.DataFrame({\n",
    "    'y_true': y_true,\n",
    "    'y_pred': y_pred,\n",
    "    'group': sensitive_attr\n",
    "})\n",
    "\n",
    "group_metrics = {}\n",
    "for group_id in np.unique(sensitive_attr):\n",
    "    group = df[df['group'] == group_id]\n",
    "    g_m = np.mean(group['y_pred'])\n",
    "    g_err = np.abs(group['y_true'] - group['y_pred'])\n",
    "    g_sq = (group['y_true'] - group['y_pred']) ** 2\n",
    "\n",
    "    group_metrics[f'mean_group_{int(group_id)}'] = g_m\n",
    "    group_metrics[f'mae_group_{int(group_id)}'] = g_err.mean()\n",
    "    group_metrics[f'rmse_group_{int(group_id)}'] = np.sqrt(g_sq.mean())\n",
    "\n",
    "mean_gap = abs(group_metrics.get('mean_group_0', 0) - group_metrics.get('mean_group_1', 0))\n",
    "mae_gap = abs(group_metrics.get('mae_group_0', 0) - group_metrics.get('mae_group_1', 0))\n",
    "rmse_gap = abs(group_metrics.get('rmse_group_0', 0) - group_metrics.get('rmse_group_1', 0))\n",
    "\n",
    "# 민감속성과 오차의 상관관계\n",
    "corr_error = np.corrcoef(abs_error, sensitive_attr)[0, 1]    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8927a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "optimizer = Adam(fgnn_model.parameters(), lr=lr, weight_decay=weight)\n",
    "\n",
    "print('-' * 100)\n",
    "n_epochs = int(n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    fgnn_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    preds = fgnn_model(data.x, data.edge_index)\n",
    "    loss = fgnn_model.total_loss(preds[data.idx_train], data.y[data.idx_train], data.sensitive_attr[data.idx_train], tau)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0 or epoch == 1:\n",
    "        print(f\"[Epoch {epoch}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# 모델 평가\n",
    "fgnn_model.eval()\n",
    "preds = fgnn_model(data.x, data.edge_index)\n",
    "\n",
    "# Tensor → numpy\n",
    "y_pred = preds[idx_eval].detach().cpu().numpy().flatten()\n",
    "y_true = data.y[idx_eval].detach().cpu().numpy().flatten()\n",
    "sensitive_attr = data.sensitive_attr[idx_eval].detach().cpu().numpy().flatten()\n",
    "\n",
    "# 기본 오류 계산\n",
    "error = y_true - y_pred\n",
    "abs_error = np.abs(error)\n",
    "sq_error = error ** 2\n",
    "\n",
    "# MAPE 관련 계산 시 y_true가 너무 작으면 제거\n",
    "mask = np.abs(y_true) > 1e-4\n",
    "mape_safe = abs_error[mask] / (np.abs(y_true[mask]) + 1e-8)\n",
    "mape = np.mean(mape_safe)\n",
    "mape_var = np.var(mape_safe)\n",
    "\n",
    "# QR Loss\n",
    "error_torch = torch.tensor(error)\n",
    "qr_loss = torch.mean(torch.max(tau * error_torch, (tau - 1) * error_torch)).item()\n",
    "\n",
    "# 정확도\n",
    "mae = np.mean(abs_error)\n",
    "rmse = np.sqrt(np.mean(sq_error))\n",
    "mae_var = np.var(abs_error)\n",
    "r2 = 1 - np.sum(sq_error) / np.sum((y_true - y_true.mean()) ** 2)\n",
    "\n",
    "# 그룹별\n",
    "df = pd.DataFrame({\n",
    "    'y_true': y_true,\n",
    "    'y_pred': y_pred,\n",
    "    'group': sensitive_attr\n",
    "})\n",
    "\n",
    "group_metrics = {}\n",
    "for group_id in np.unique(sensitive_attr):\n",
    "    group = df[df['group'] == group_id]\n",
    "    g_m = np.mean(group['y_pred'])\n",
    "    g_err = np.abs(group['y_true'] - group['y_pred'])\n",
    "    g_sq = (group['y_true'] - group['y_pred']) ** 2\n",
    "\n",
    "    group_metrics[f'mean_group_{int(group_id)}'] = g_m\n",
    "    group_metrics[f'mae_group_{int(group_id)}'] = g_err.mean()\n",
    "    group_metrics[f'rmse_group_{int(group_id)}'] = np.sqrt(g_sq.mean())\n",
    "\n",
    "mean_gap = abs(group_metrics.get('mean_group_0', 0) - group_metrics.get('mean_group_1', 0))\n",
    "mae_gap = abs(group_metrics.get('mae_group_0', 0) - group_metrics.get('mae_group_1', 0))\n",
    "rmse_gap = abs(group_metrics.get('rmse_group_0', 0) - group_metrics.get('rmse_group_1', 0))\n",
    "\n",
    "# 민감속성과 오차의 상관관계\n",
    "corr_error = np.corrcoef(abs_error, sensitive_attr)[0, 1]    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f763d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "optimizer = Adam(bgnn_model.parameters(), lr=lr, weight_decay=weight)\n",
    "\n",
    "print('-' * 100)\n",
    "n_epochs = int(n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    bgnn_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    preds = bgnn_model(data.x, data.edge_index)\n",
    "    loss = quantile_loss(data.y[data.idx_train], preds[data.idx_train], tau)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0 or epoch == 1:\n",
    "        print(f\"[Epoch {epoch}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# 모델 평가\n",
    "bgnn_model.eval()\n",
    "preds = bgnn_model(data.x, data.edge_index)\n",
    "\n",
    "# Tensor → numpy\n",
    "y_pred = preds[idx_eval].detach().cpu().numpy().flatten()\n",
    "y_true = data.y[idx_eval].detach().cpu().numpy().flatten()\n",
    "sensitive_attr = data.sensitive_attr[idx_eval].detach().cpu().numpy().flatten()\n",
    "\n",
    "# 기본 오류 계산\n",
    "error = y_true - y_pred\n",
    "abs_error = np.abs(error)\n",
    "sq_error = error ** 2\n",
    "\n",
    "# MAPE 관련 계산 시 y_true가 너무 작으면 제거\n",
    "mask = np.abs(y_true) > 1e-4\n",
    "mape_safe = abs_error[mask] / (np.abs(y_true[mask]) + 1e-8)\n",
    "mape = np.mean(mape_safe)\n",
    "mape_var = np.var(mape_safe)\n",
    "\n",
    "# QR Loss\n",
    "error_torch = torch.tensor(error)\n",
    "qr_loss = torch.mean(torch.max(tau * error_torch, (tau - 1) * error_torch)).item()\n",
    "\n",
    "# 정확도\n",
    "mae = np.mean(abs_error)\n",
    "rmse = np.sqrt(np.mean(sq_error))\n",
    "mae_var = np.var(abs_error)\n",
    "r2 = 1 - np.sum(sq_error) / np.sum((y_true - y_true.mean()) ** 2)\n",
    "\n",
    "# 그룹별\n",
    "df = pd.DataFrame({\n",
    "    'y_true': y_true,\n",
    "    'y_pred': y_pred,\n",
    "    'group': sensitive_attr\n",
    "})\n",
    "\n",
    "group_metrics = {}\n",
    "for group_id in np.unique(sensitive_attr):\n",
    "    group = df[df['group'] == group_id]\n",
    "    g_m = np.mean(group['y_pred'])\n",
    "    g_err = np.abs(group['y_true'] - group['y_pred'])\n",
    "    g_sq = (group['y_true'] - group['y_pred']) ** 2\n",
    "\n",
    "    group_metrics[f'mean_group_{int(group_id)}'] = g_m\n",
    "    group_metrics[f'mae_group_{int(group_id)}'] = g_err.mean()\n",
    "    group_metrics[f'rmse_group_{int(group_id)}'] = np.sqrt(g_sq.mean())\n",
    "\n",
    "mean_gap = abs(group_metrics.get('mean_group_0', 0) - group_metrics.get('mean_group_1', 0))\n",
    "mae_gap = abs(group_metrics.get('mae_group_0', 0) - group_metrics.get('mae_group_1', 0))\n",
    "rmse_gap = abs(group_metrics.get('rmse_group_0', 0) - group_metrics.get('rmse_group_1', 0))\n",
    "\n",
    "# 민감속성과 오차의 상관관계\n",
    "corr_error = np.corrcoef(abs_error, sensitive_attr)[0, 1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c08444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55695933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fair-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
